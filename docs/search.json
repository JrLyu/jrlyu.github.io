[
  {
    "objectID": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html",
    "href": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html",
    "title": "1 Linear Classification",
    "section": "",
    "text": "feature vector: \\(\\va x=\\mqty[x_1&x_2&\\cdots&x_d]^\\top\\in\\R^d\\). \\(\\R^d\\) is called the feature space.\nlabel: \\(y\\in\\qty{-1,+1}\\), binary.\ntraining set of labeled examples: \\[D=\\qty{\\va x^{(i)},y^{(i)}}_{i=1}^N\\]\nclassifier: \\[h:\\R^d\\to\\qty{-1,+1}.\\]\n\nGola: select the best \\(h\\) from a set of possible classifiers \\(\\mathcal{H}\\) that would (the ability to generalization).\nWe will solve this goal by a learning algorithm, typically an optimization problem \\(\\wrt D\\)."
  },
  {
    "objectID": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html#machine-learning-terminology",
    "href": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html#machine-learning-terminology",
    "title": "1 Linear Classification",
    "section": "",
    "text": "feature vector: \\(\\va x=\\mqty[x_1&x_2&\\cdots&x_d]^\\top\\in\\R^d\\). \\(\\R^d\\) is called the feature space.\nlabel: \\(y\\in\\qty{-1,+1}\\), binary.\ntraining set of labeled examples: \\[D=\\qty{\\va x^{(i)},y^{(i)}}_{i=1}^N\\]\nclassifier: \\[h:\\R^d\\to\\qty{-1,+1}.\\]\n\nGola: select the best \\(h\\) from a set of possible classifiers \\(\\mathcal{H}\\) that would (the ability to generalization).\nWe will solve this goal by a learning algorithm, typically an optimization problem \\(\\wrt D\\)."
  },
  {
    "objectID": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html#linear-classifer-through-origin",
    "href": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html#linear-classifer-through-origin",
    "title": "1 Linear Classification",
    "section": "Linear Classifer (Through Origin)",
    "text": "Linear Classifer (Through Origin)\n\n\n\n\n\n\n\nDefinition 1 (Thresholded Linear Mapping from Feature Vectors to Labels) \\[\nh(\\va x; \\va\\theta)=\\begin{cases}+1\\quad\\text{if }\\va\\theta\\cdot\\va x&gt;0,\\\\-1\\quad\\text{if }\\va\\theta\\cdot\\va x&lt;0\\end{cases},\n\\] where \\(\\va\\theta\\in\\R^d\\) is the parameter vector, and \\(\\va\\theta=\\mqty[\\theta_1,\\theta_2,\\dots,\\theta_d]^\\top\\).\nOne can also write it using the \\(\\operatorname{sign}\\) function: \\[\nh(\\va x;\\va\\theta)=\\operatorname{sign}(\\va\\theta\\cdot\\va x)=\\begin{cases}\n+1\\quad\\text{if }\\va\\theta\\cdot\\va x&gt;0,\\\\\n0\\quad\\text{if }\\va\\theta\\cdot\\va x=0,\\\\\n-1\\quad\\text{if }\\va\\theta\\cdot\\va x&lt;0.\n\\end{cases}.\n\\]\n\n\n\n\n\nRecall: dot product: \\[\n\\va\\theta\\cdot\\va x=\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_dx_d=\\sum_{j=1}^d\\theta_jx_j,\n\\] a linear combination of input features.\nIn \\(h(\\va x;\\va\\theta)\\), different \\(\\va\\theta\\)’s produce (potentially) different labelings for the same \\(\\va x\\).\n\n\nGraphical Representation\n\n\n\n\n\n\nFigure 1: Decision Boundary\n\n\n\n\nHowever, what happens on this \\(90^\\circ\\) line? We call this line the decision boundary, which separates the two classes. Recall: \\[\n\\va\\theta\\cdot\\va x=\\norm{\\va\\theta}\\cdot\\norm{\\va x}\\cdot\\cos 90^\\circ=0.\n\\]\nView the decision boundary as a hyperplane in \\(\\R^d\\).\n\nDoes the length of \\(\\va\\theta\\) matter? No.\nDoes the direction of \\(\\va\\theta\\) matter? Yes."
  },
  {
    "objectID": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html#linear-classifier-with-offset",
    "href": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html#linear-classifier-with-offset",
    "title": "1 Linear Classification",
    "section": "Linear Classifier with Offset",
    "text": "Linear Classifier with Offset\n\n\n\n\n\n\n\nDefinition 2 (Linear Classifier with Offset) \\[\nh(\\va x;\\va\\theta,b)=\\operatorname{sign}(\\va\\theta\\cdot\\va x+b),\n\\] where \\(\\va x\\in\\R^d\\), \\(\\va\\theta\\in\\R^d\\), and \\(b\\in\\R\\). \\(b\\) is called the offset or intercept.\n\n\n\n\n\nGraphical Representation\n\n\n\n\n\n\nFigure 2: Linear Classifier with Offset\n\n\n\n\nNote that the signed distance from \\(\\va\\theta\\cdot\\va x=0\\) to the hyperplane \\(\\va\\theta\\cdot\\va x+b=0\\) is given by: \\[\n\\dfrac{-b}{\\norm{\\va\\theta}}.\n\\]\n\n\nProof. \n\nPick a point on old decision boundary \\(\\va x^{(1)}\\) satisfies \\[\\va\\theta\\cdot\\va x^{(1)}=0.\\]\nPick a point on the new decision boundary \\(\\va x^{(2)}\\) satisfies \\[\\va\\theta\\cdot\\va x^{(2)}+b=0\\implies\\va\\theta\\va x^{(2)}=-b.\\]\nLet \\(\\va v=\\va x^{(2)}-\\va x^{(1)}.\\)\nNow, project \\(\\va v\\) into direction of \\(\\va\\theta\\): \\[\n\\operatorname{proj}_{\\va\\theta}\\va v=\\qty(\\va v\\cdot\\dfrac{\\va\\theta}{\\norm{\\va\\theta}})\\dfrac{\\va\\theta}{\\norm{\\va\\theta}}.\n\\] Note: \\(\\dfrac{\\va\\theta}{\\norm{\\va\\theta}}\\) is the unit vector in the direction of \\(\\va\\theta\\).\nTherefore, the signed sitance is given by: \\[\n\\va v\\cdot\\dfrac{\\va\\theta}{\\norm{\\va\\theta}}=\\dfrac{\\qty(\\va x^{(2)}-\\va x^{(1)})\\cdot\\va\\theta}{\\norm{\\va\\theta}}=\\dfrac{\\va x^{(2)}\\cdot\\va\\theta-\\va x^{(1)}\\cdot\\va\\theta}{\\norm{\\va\\theta}}=\\dfrac{-b}{\\norm{\\va\\theta}}.\n\\]"
  },
  {
    "objectID": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html#training-error",
    "href": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html#training-error",
    "title": "1 Linear Classification",
    "section": "Training Error",
    "text": "Training Error\n\nIntuition: we want \\(\\va\\theta\\) that works well on training data \\(D\\).\n\n\n\n\n\n\n\n\nRemark 1. We’ve restricted the class of possible clasassifiers to linear classifiers, reducing the chance of overfitting.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3 (Training Error and Learning Algorithms) The training error (\\(\\epsilon\\)) is the fraction of training examples for which the classifier produces wrong labels: \\[\n\\epsilon_N(\\va \\theta)=\\dfrac{1}{N}\\sum_{i=1}^N\\1\\qty{y^{(i)}\\neq h\\qty(\\va x^{(i)};\\va\\theta)},\n\\] where \\(\\1\\{\\cdot\\}\\) returns \\(1\\) if true and \\(0\\) if false.\nAn equivalent form is: \\[\n\\epsilon_N(\\va \\theta)=\\dfrac{1}{N}\\sum_{i=1}^N\\1\\{{\\color{orange}{\\underbrace{y^{(i)}\\qty(\\va\\theta\\cdot\\va x^{(i)})}_{\\substack{y^(i)\\text{ and }\\va\\theta\\cdot\\va x^{(i)}\\\\\\text{ have opposite signs}}}}}{\\color{green}{\\overbrace{\\leq 0}^{\\substack{\\text{points on the decision boundary}\\\\\\text{are considered misclassified}}}}}\\}\n\\]\n\n\n\n\n\nGoal: Find \\(\\displaystyle\\va\\theta^*=\\argmin_{\\va\\theta}\\epsilon_N\\qty(\\va\\theta)\\).\nHow:\n\nIn general, this is not easy to solve (it’s NP-hard).\nFor now, we will consider a special case: linearly separable data.\n\n\n\n\n\n\n\n\n\nDefinition 4 (Linear Separable) Training examples \\(D=\\qty{\\va x^{(i)}, y^{(i)}}_{i=1}^N\\) are linearly separable through the origin if \\(\\exists\\ \\va\\theta\\) such that \\[\ny^{(i)}\\qty(\\va\\theta\\cdot\\va x^{(i)})&gt;0\\quad\\forall\\ i=1,\\dots, N\n\\]\n\n\n\n\n\n\n\n\n\n\n\nRemark 2. This assumption of linear separability is NOT testable.\n\n\n\n\n\nPerceptron Algorithm\n\nThe perceptron algorithm is a mistaken-driven algorithm. It starts with \\(\\va\\theta=\\va 0\\) (the zero vector), and then tries to update \\(\\va\\theta\\) to correct any mistakes.\n\n\n\n\\begin{algorithm} \\caption{Perceptron (Through Origin)} \\begin{algorithmic} \\Procedure{Perceptron}{$D=\\qty{\\va x^{(i)}, y^{(i)}}_{i=1}^N$} \\State $k=0$; $\\va\\theta^{(0)}=\\va 0$ \\While{not all points are correctly classified} \\For{$i=1,\\dots, N$} \\State \\Comment{$\\color{green}\\text{if mistake}$} \\If{$y^{(i)}\\qty(\\va\\theta^{(k)}\\cdot\\va x^{(i)})\\leq 0$} \\State $\\va\\theta^{(k+1)}=\\va\\theta^{(k)}+y^{(i)}\\va x^{(i)}$ \\State $k++$ \\EndIf \\EndFor \\EndWhile \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\n\nTheorem 1 (Existence of Perceptron Solution) The perceptron algorithm (Algorithm 1) converges after a finite number of mistkaes if the training examples are linearly separable through the origin.\n\n\n\n\n\nHowever, :\n\nSolution is not unique\nMay need to loop through the dataset more than once, or not use some points at all.\n\n\n\n\n\\begin{algorithm} \\caption{Perceptron (With Offset)} \\begin{algorithmic} \\Procedure{Perceptron}{$D=\\qty{\\va x^{(i)}, y^{(i)}}_{i=1}^N$} \\State $k=0$; $\\va\\theta^{(0)}=\\va 0$; $b^{(0)}=0$ \\While{not all points are correctly classified} \\For{$i=1,\\dots, N$} \\State \\Comment{$\\color{green}\\text{if mistake}$} \\If{$y^{(i)}\\qty(\\va\\theta^{(k)}\\cdot\\va x^{(i)}+b^{(k)})\\leq 0$} \\State $\\va\\theta^{(k+1)}=\\va\\theta^{(k)}+y^{(i)}\\va x^{(i)}$ \\State $b^{(k+1)}=b^{(k)}+y^{(i)}$ \\State $k++$ \\EndIf \\EndFor \\EndWhile \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\nProof. Produce augmented vecotrs: \\[\\va x'=\\mqty[1, \\va x]^\\top\\quad\\text{and}\\quad\\va\\theta'=\\mqty[b,\\va\\theta]^\\top.\\] Then, we have implicit offset formula: \\[\\va\\theta'\\cdot\\va x'=b+\\va\\theta\\cdot\\va x.\\] Apply Algorithm 1 to \\(\\va x'\\) and \\(\\va\\theta'\\): \\[\n\\begin{aligned}\n\\va\\theta'^{(k+1)}&=\\va\\theta'^{(k)}+y^{(i)}\\va x'^{(i)}\\\\\n\\mqty[b^{(k+1)},\\va\\theta^{(k+1)}]&=\\mqty[b^{(k)},\\va\\theta^{(k)}]+y^{(i)}\\mqty[1,\\va x^{(i)}]\\\\\n\\implies \\va\\theta^{(k+1)}&=\\va\\theta^{(k)}+y^{(i)}\\va x^{(i)}\\\\\nb^{(k+1)}&=b^{(k)}+y^{(i)}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "notes/cs377.html",
    "href": "notes/cs377.html",
    "title": "CS 377 Database Systems",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n1 The Relational Model\n\n\n\n\n\n\nDatabase\n\n\nRelational Model\n\n\n\nThis lecture discusses the relational model, which is the foundation of modern database systems.\n\n\n\n\n\nAug 28, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n2 Relational Algebra\n\n\n\n\n\n\nDatabase\n\n\nRelational Model\n\n\nRelational Algebra\n\n\n\nThis lecture discusses the relational algebra, which is the foundation of modern database systems. Topics include select, project, and join operators.\n\n\n\n\n\nSep 8, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n3 SQL Introduction\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\nDatabase\n\n\n\nThis note introduces SQL, the Structured Query Language, which is used to interact with databases. We will cover basic queries, the use of *, AS, conditions, and ORDER BY in SQL.\n\n\n\n\n\nSep 10, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n4 SQL Aggregation\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\nDatabase\n\n\nAggregation\n\n\n\nThis lecture discusses SQL Aggregation, including computing on a column, GROUP BY, and HAVING clauses.\n\n\n\n\n\nSep 14, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n5 SQL Set Operations\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\nDatabase\n\n\nSet Operations\n\n\n\nThis lecture discusses SQL set operations, including UNION, INTERSECT, and EXCEPT. It also covers the difference between bag and set semantics in SQL.\n\n\n\n\n\nSep 18, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n6 SQL Join\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\nDatabase\n\n\nJoin\n\n\n\nThis lecture discusses the different types of joins in SQL, including inner, outer, and cross joins. It also covers the dangers of using NATURAL JOIN and the best practices for using joins in SQL.\n\n\n\n\n\nSep 28, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n7 SQL NULL\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\nDatabase\n\n\nJoin\n\n\nNULL\n\n\n\nThis lecture discusses the concept of NULL values in SQL, including how to represent missing information and inapplicable attributes. It also covers how to check for NULL values and the impact of NULL values on arithmetic expressions, comparison operators, and aggregation.\n\n\n\n\n\nOct 8, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n8 SQL Subqueries\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\nDatabase\n\n\nSubqueries\n\n\n\nThis lecture discusses subqueries in SQL, including subqueries in a FROM clause, subqueries in a WHERE clause, and the scope of subqueries. It also covers special cases of subqueries, such as when the subquery returns NULL or multiple values.\n\n\n\n\n\nOct 18, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n9 SQL DDL\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\nDatabase\n\n\nDDL\n\n\n\nThis lecture discusses Database Modification Language in SQL, including Insert, Delete, Update, and Create operations. It also covers SQL Schemas, Types, Keys, and Foreign Keys.\n\n\n\n\n\nOct 28, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n10 JDBC\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\nDatabase\n\n\nJava\n\n\nJDBC\n\n\n\nThis lecture discusses how to embed SQL in Java using JDBC. It covers the JDBC API, SQL Injection, and Prepared Statements.\n\n\n\n\n\nOct 30, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n11 ER Design\n\n\n\n\n\n\nDatabase\n\n\nDatabase Design\n\n\nER Design\n\n\n\nThis lecture discusses Entity-Relationship (ER) design, which is a technique for designing databases. It covers the ER model, ER diagrams, and the process of converting ER diagrams to relational schemas.\n\n\n\n\n\nNov 10, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n12 Database Design Theory: Normalization\n\n\n\n\n\n\nDatabase\n\n\nDatabase Design\n\n\nDB Design Theory\n\n\nBCNF\n\n\nNormalization\n\n\nFunctional Dependencies\n\n\nClosure Test\n\n\nFD Projection\n\n\n\nThis lecture discusses the concept of normalization in database design theory. The lecture covers functional dependencies, closure test, and FD projection. It finally introduces the concept of BCNF.\n\n\n\n\n\nNov 18, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n13 NOSQL: Not Only SQL\n\n\n\n\n\n\nDatabase\n\n\nNOSQL\n\n\n\nThis lecture introduces the concept of NOSQL databases and their applications.\n\n\n\n\n\nDec 4, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes/nonlinearOpt.html",
    "href": "notes/nonlinearOpt.html",
    "title": "Nonlinear Optimization",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes/Calc3.html",
    "href": "notes/Calc3.html",
    "title": "Multivariable Calculus/Calculus 3",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes/NumericalAnalysis2.html",
    "href": "notes/NumericalAnalysis2.html",
    "title": "PhD-Level Numerical Analysis I (Numerical Linear Algebra)",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes/ODE.html",
    "href": "notes/ODE.html",
    "title": "Ordinary Differential Equations",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes/cs377/05-sql-set-operations/05-SQL-Set-Operations.html",
    "href": "notes/cs377/05-sql-set-operations/05-SQL-Set-Operations.html",
    "title": "5 SQL Set Operations",
    "section": "",
    "text": "A table can have duplicate tuples, unless this would violate an integrity constraint.\nAnd SELECT-FROM-WHERE (SFW) statements leave duplicates in, unless you say not to!\nWhy?\n\nGetting rid of duplicates is expensive!\nWe may want the duplicates because they tell us how many times something occurred.\n\n\n\n\n\nSQL treats tables as “bags” (or “multisets”) rather than sets.\nBags are just like sets, but duplicates are allowed.\n\n\n\n\n\n\n\nTip 1: Example: Bag Semantics\n\n\n\n\n\n\n\\(\\{6, 2, 7, 1, 9\\}\\) is a set and a bag\n\\(\\{6, 2, 7, 1, 9, 1\\}\\) is not a set but a bag\n\n\n\n\n\nLet sets, order doesn’t matter: \\(\\{6, 2, 7, 1, 9, 1\\}=\\{1, 1, 2, 6, 7, 9\\}\\)\nOperations \\(\\cap\\), \\(\\cup\\), and \\(-\\) with bags:\n\nFor \\(\\cap\\), \\(\\cup\\), and \\(-\\) the number of occurrences of a tuple in the result requires some thought.\nSuppose tuple \\(t\\) occurs:\n\n\\(m\\) times in relation \\(R\\), and\n\\(n\\) times in relation \\(S\\)\n\n\n\n\n\n\nOperation\nNumber of Occurrences of \\(t\\) in tuples\n\n\n\n\n\\(R\\cap S\\)\n\\(\\min(m,n)\\)\n\n\n\\(R\\cup S\\)\n\\(m+n\\)\n\n\n\\(R-S\\)\n\\(\\max(m-n,0)\\)\n\n\n\n\n\\(\\cap\\), \\(\\cup\\), and \\(-\\) in SQL:\n\n(&lt;subquery&gt;) UNION (&lt;subquery&gt;)\n(&lt;subquery&gt;) INTERSECT (&lt;subquery&gt;)\n(&lt;subquery&gt;) EXCEPT (&lt;subquery&gt;)\n\nThe parentheses () are mandatory\nThe operands must be queries; you can’t simply use a relation name.\n\n\n\n\n\n\n\nTip 2: Example: Set Operations in SQL\n\n\n\n\n\n(SELECT sid\n FROM Took\n WHERE grade &gt; 95)\n    UNION\n(SELECT sid\n FROM Took\n WHERE grade &lt; 50);\n\n\n\n\nBag vs. Set Semantics: which is used and when\n\nA SELECT-FROM-WHERE statement uses bag semantics by default.\n\nDuplicates are kept in the result\n\nThe set (INTERSECT/UNION/EXCEPT) operations use set semantics by default\n\nDuplicates are eliminated from the result\n\n\nMotivation: Efficiency\n\nWhen doing projection, it is easier not to eliminate duplicate\n\nJust work one tuple at a time\n\nFor intersection or difference, it is most efficient to sort the relations first.\n\nAt that point you may was well eliminate the duplicates anyway\n\n\nHowever, we can control which semantic is used.\n\nWe can force the result of a SFW query to be a set by using SELECT DISTINCT\nWe can force the result of a set operation to be a bag by using ALL.\n\n\n\n\n\n\n\n\nTip 3: Example: Force to Use Set Operations in SQL\n\n\n\n\n\n(SELECT sid\n FROM Took\n WHERE grade &gt; 95)\n    UNION ALL\n(SELECT sid\n FROM Took\n WHERE grade &lt; 50);\n\n\n\n\n\n\n\n\n\nTip 4: Example: Comparison of Set and Bag Semantics\n\n\n\n\n\n\nA single occurrence of a value for x in B wipes out all occurrences of it from A:\n\n(SELECT x FROM A) EXCEPT (SELECT x FROM B)\n\nWith EXCEPT ALL, we match up the value one by one:\n\n(SELECT x FROM A) EXCEPT (SELECT x FROM B)"
  },
  {
    "objectID": "notes/cs377/05-sql-set-operations/05-SQL-Set-Operations.html#duplicates-in-sql",
    "href": "notes/cs377/05-sql-set-operations/05-SQL-Set-Operations.html#duplicates-in-sql",
    "title": "5 SQL Set Operations",
    "section": "",
    "text": "A table can have duplicate tuples, unless this would violate an integrity constraint.\nAnd SELECT-FROM-WHERE (SFW) statements leave duplicates in, unless you say not to!\nWhy?\n\nGetting rid of duplicates is expensive!\nWe may want the duplicates because they tell us how many times something occurred.\n\n\n\n\n\nSQL treats tables as “bags” (or “multisets”) rather than sets.\nBags are just like sets, but duplicates are allowed.\n\n\n\n\n\n\n\nTip 1: Example: Bag Semantics\n\n\n\n\n\n\n\\(\\{6, 2, 7, 1, 9\\}\\) is a set and a bag\n\\(\\{6, 2, 7, 1, 9, 1\\}\\) is not a set but a bag\n\n\n\n\n\nLet sets, order doesn’t matter: \\(\\{6, 2, 7, 1, 9, 1\\}=\\{1, 1, 2, 6, 7, 9\\}\\)\nOperations \\(\\cap\\), \\(\\cup\\), and \\(-\\) with bags:\n\nFor \\(\\cap\\), \\(\\cup\\), and \\(-\\) the number of occurrences of a tuple in the result requires some thought.\nSuppose tuple \\(t\\) occurs:\n\n\\(m\\) times in relation \\(R\\), and\n\\(n\\) times in relation \\(S\\)\n\n\n\n\n\n\nOperation\nNumber of Occurrences of \\(t\\) in tuples\n\n\n\n\n\\(R\\cap S\\)\n\\(\\min(m,n)\\)\n\n\n\\(R\\cup S\\)\n\\(m+n\\)\n\n\n\\(R-S\\)\n\\(\\max(m-n,0)\\)\n\n\n\n\n\\(\\cap\\), \\(\\cup\\), and \\(-\\) in SQL:\n\n(&lt;subquery&gt;) UNION (&lt;subquery&gt;)\n(&lt;subquery&gt;) INTERSECT (&lt;subquery&gt;)\n(&lt;subquery&gt;) EXCEPT (&lt;subquery&gt;)\n\nThe parentheses () are mandatory\nThe operands must be queries; you can’t simply use a relation name.\n\n\n\n\n\n\n\nTip 2: Example: Set Operations in SQL\n\n\n\n\n\n(SELECT sid\n FROM Took\n WHERE grade &gt; 95)\n    UNION\n(SELECT sid\n FROM Took\n WHERE grade &lt; 50);\n\n\n\n\nBag vs. Set Semantics: which is used and when\n\nA SELECT-FROM-WHERE statement uses bag semantics by default.\n\nDuplicates are kept in the result\n\nThe set (INTERSECT/UNION/EXCEPT) operations use set semantics by default\n\nDuplicates are eliminated from the result\n\n\nMotivation: Efficiency\n\nWhen doing projection, it is easier not to eliminate duplicate\n\nJust work one tuple at a time\n\nFor intersection or difference, it is most efficient to sort the relations first.\n\nAt that point you may was well eliminate the duplicates anyway\n\n\nHowever, we can control which semantic is used.\n\nWe can force the result of a SFW query to be a set by using SELECT DISTINCT\nWe can force the result of a set operation to be a bag by using ALL.\n\n\n\n\n\n\n\n\nTip 3: Example: Force to Use Set Operations in SQL\n\n\n\n\n\n(SELECT sid\n FROM Took\n WHERE grade &gt; 95)\n    UNION ALL\n(SELECT sid\n FROM Took\n WHERE grade &lt; 50);\n\n\n\n\n\n\n\n\n\nTip 4: Example: Comparison of Set and Bag Semantics\n\n\n\n\n\n\nA single occurrence of a value for x in B wipes out all occurrences of it from A:\n\n(SELECT x FROM A) EXCEPT (SELECT x FROM B)\n\nWith EXCEPT ALL, we match up the value one by one:\n\n(SELECT x FROM A) EXCEPT (SELECT x FROM B)"
  },
  {
    "objectID": "notes/cs377/05-sql-set-operations/05-SQL-Set-Operations.html#views",
    "href": "notes/cs377/05-sql-set-operations/05-SQL-Set-Operations.html#views",
    "title": "5 SQL Set Operations",
    "section": "Views",
    "text": "Views\n\nThe idea\n\nA view is a relation defined in terms of stored tables (called base tables) and possibly also other views.\nAccess a view like any base table.\nTwo kinds of view:\n\nVirtual: no tuples are stored; view is just a query for constructing the relation when needed.\nMaterialized: actually constructed and stored. Expensive to maintain.\n\nViews are particularly important when you want to give different access rights (i.e. permissions) to different users viewing data in your tables!\n\n\n\n\n\n\n\nTip 5: Example: Application of Views\n\n\n\n\n\nCanvas student page vs. instructor page\n\n\n\n\n\n\n\n\n\nTip 6: Example: Creating a View\n\n\n\n\n\n\nA view for students who earned an 80 or higher in a CSC course:\n\nCREATE VIEW topresults AS\n    SELECT firstname, surname, cnum\n    FROM Student, Took, Offering\n    WHERE\n        Student.sid = Took.sid AND\n        Took.oid = Offering.oid AND\n        grade &gt;= 80 AND dept = 'CSC';\n\n\n\n\n\nUses of Views\n\nBreak down a large query\nProvide another way of looking at the same data, e.g. for one category of user\nWrap commonly used complex queries"
  },
  {
    "objectID": "notes/cs377/07-sql-null/07-SQL-NULL.html",
    "href": "notes/cs377/07-sql-null/07-SQL-NULL.html",
    "title": "7 SQL NULL",
    "section": "",
    "text": "Missing Information:\n\nMissing value.\n\nE.g., we know a student has some email address, but we don’t know what it is.\n\nInapplicable attribute.\n\nE.g., the value of attribute spouse for a person who is single.\n\n\nRepresenting missing information:\n\nOne possibility: use a special value as a placeholder. E.g.,\n\nIf age unknown, use -1.\nIf StNum unknown, use 999999999.\n\nPros and cons?\n\nBetter solution: use a value not in any domain. We call this a null value.\nTuples in SQL relations can have NULL as a value for one or more components.\n\n\nCheck for NULL values\n\nYou can compare an attribute value to NULL with\n\nIS NULL\nIS NOT NULL\n\n\n\n\n\n\n\n\n\nTip 1: Example: Check for NULL values\n\n\n\n\n\nSELECT *\nFROM Course\nWHERE breadth IS NULL;\n\n\n\n\nNote: do not use WHERE breadth = NULL;\n\n\n\n\nAssume \\(x\\) is NULL\nArithmetic expression: Result is always NULL\n\n\n\n\n\n\n\nTip 2: Example: Arithmetic with NULL\n\n\n\n\n\n\\[x+\\texttt{grade}=\\texttt{NULL}\\] \\[x*0=\\texttt{NULL}\\] \\[x-x=\\texttt{NULL}\\]\n\n\n\n\nComparison operators (\\(&gt;\\), \\(&lt;\\), \\(=\\), \\(\\dots\\)): Result is UNKNOWN (neither TRUE nor FALSE)\n\n\n\n\n\n\n\nTip 3: Example: Comparison with NULL\n\n\n\n\n\n\\[x&lt;32 \\quad\\texttt{ --&gt; UNKNOWN}\\]\n\n\n\n\nThis UNKNOWN is a truth-value\nTruth-values in SQL are: TRUE, FALSE, UNKNOWN (a 3-value truth value system!)\n\nLogic with UNKNOWN: \\[\\begin{aligned}\n\\texttt{UNKNOWN} \\lor\\texttt{FALSE}&\\equiv\\texttt{UNKNOWN}\\\\\n\\texttt{UNKNOWN} \\lor\\texttt{TRUE}&\\equiv\\texttt{TRUE}\\\\\n\\texttt{UNKNOWN} \\land\\texttt{FALSE}&\\equiv\\texttt{FALSE}\\\\\n\\texttt{UNKNOWN} \\land\\texttt{TRUE}&\\equiv\\texttt{UNKNOWN}\\\\\n\\neg\\texttt{UNKNOWN}&\\equiv\\texttt{UNKONWN}\n\\end{aligned}\\]\nA tuple is in a query result \\(\\iff\\) the result of the WHERE clause is TRUE.\n\n\n\n\n\n\n“Aggregation ignores NULL.”\n\nNULL never contributes to a sum, average, or count, and can never be the minimum or maximum of a column (unless every value is NULL).\n\nIf ALL values are NULL in a column, then the result of the aggregation is NULL.\nException: COUNT of an empty set is 0. (think of COUNT(columnName) as a function that counts the non-null values in that column.)\n\n\n\n\n\n\n\nTip 4: Example: Aggregation with NULL\n\n\n\n\n\n\nR&S&T are defined as:\n\n R             |      S             |       T\n   x           |          x         |            x\n -----         |        -----       |          -----\n NULL          |        NULL        |\n 1             |                    |\n\nCOUNT()\n\nCOUNT(R.*)=2 and COUNT(R.x)=1\nCOUNT(S.*)=1 and COUNT(S.x)=0\nCOUNT(T.*)=0 and COUNT(T.x)=0\n\nOther aggregates:\n\nMIN(R.x)=1 and MAX(R.x)=1\nMIN(S.x)=NULL and MAX(S.x)=NULL\nMIN(T.x)=NULL and MAX(T.x)=NULL\n\n\n\n\n\n\n\n\n\nNULL is treated differently by the set operators UNION, EXCEPT, and INTERSECT than it is in search conditions.\nWhen comparing rows, set operators treat NULL values as equal to each other.\nIn contrast, when NULL is compared to NULL in a search condition the result is UNKNOWN (not true)."
  },
  {
    "objectID": "notes/cs377/07-sql-null/07-SQL-NULL.html#null-values-in-sql",
    "href": "notes/cs377/07-sql-null/07-SQL-NULL.html#null-values-in-sql",
    "title": "7 SQL NULL",
    "section": "",
    "text": "Missing Information:\n\nMissing value.\n\nE.g., we know a student has some email address, but we don’t know what it is.\n\nInapplicable attribute.\n\nE.g., the value of attribute spouse for a person who is single.\n\n\nRepresenting missing information:\n\nOne possibility: use a special value as a placeholder. E.g.,\n\nIf age unknown, use -1.\nIf StNum unknown, use 999999999.\n\nPros and cons?\n\nBetter solution: use a value not in any domain. We call this a null value.\nTuples in SQL relations can have NULL as a value for one or more components.\n\n\nCheck for NULL values\n\nYou can compare an attribute value to NULL with\n\nIS NULL\nIS NOT NULL\n\n\n\n\n\n\n\n\n\nTip 1: Example: Check for NULL values\n\n\n\n\n\nSELECT *\nFROM Course\nWHERE breadth IS NULL;\n\n\n\n\nNote: do not use WHERE breadth = NULL;\n\n\n\n\nAssume \\(x\\) is NULL\nArithmetic expression: Result is always NULL\n\n\n\n\n\n\n\nTip 2: Example: Arithmetic with NULL\n\n\n\n\n\n\\[x+\\texttt{grade}=\\texttt{NULL}\\] \\[x*0=\\texttt{NULL}\\] \\[x-x=\\texttt{NULL}\\]\n\n\n\n\nComparison operators (\\(&gt;\\), \\(&lt;\\), \\(=\\), \\(\\dots\\)): Result is UNKNOWN (neither TRUE nor FALSE)\n\n\n\n\n\n\n\nTip 3: Example: Comparison with NULL\n\n\n\n\n\n\\[x&lt;32 \\quad\\texttt{ --&gt; UNKNOWN}\\]\n\n\n\n\nThis UNKNOWN is a truth-value\nTruth-values in SQL are: TRUE, FALSE, UNKNOWN (a 3-value truth value system!)\n\nLogic with UNKNOWN: \\[\\begin{aligned}\n\\texttt{UNKNOWN} \\lor\\texttt{FALSE}&\\equiv\\texttt{UNKNOWN}\\\\\n\\texttt{UNKNOWN} \\lor\\texttt{TRUE}&\\equiv\\texttt{TRUE}\\\\\n\\texttt{UNKNOWN} \\land\\texttt{FALSE}&\\equiv\\texttt{FALSE}\\\\\n\\texttt{UNKNOWN} \\land\\texttt{TRUE}&\\equiv\\texttt{UNKNOWN}\\\\\n\\neg\\texttt{UNKNOWN}&\\equiv\\texttt{UNKONWN}\n\\end{aligned}\\]\nA tuple is in a query result \\(\\iff\\) the result of the WHERE clause is TRUE.\n\n\n\n\n\n\n“Aggregation ignores NULL.”\n\nNULL never contributes to a sum, average, or count, and can never be the minimum or maximum of a column (unless every value is NULL).\n\nIf ALL values are NULL in a column, then the result of the aggregation is NULL.\nException: COUNT of an empty set is 0. (think of COUNT(columnName) as a function that counts the non-null values in that column.)\n\n\n\n\n\n\n\nTip 4: Example: Aggregation with NULL\n\n\n\n\n\n\nR&S&T are defined as:\n\n R             |      S             |       T\n   x           |          x         |            x\n -----         |        -----       |          -----\n NULL          |        NULL        |\n 1             |                    |\n\nCOUNT()\n\nCOUNT(R.*)=2 and COUNT(R.x)=1\nCOUNT(S.*)=1 and COUNT(S.x)=0\nCOUNT(T.*)=0 and COUNT(T.x)=0\n\nOther aggregates:\n\nMIN(R.x)=1 and MAX(R.x)=1\nMIN(S.x)=NULL and MAX(S.x)=NULL\nMIN(T.x)=NULL and MAX(T.x)=NULL\n\n\n\n\n\n\n\n\n\nNULL is treated differently by the set operators UNION, EXCEPT, and INTERSECT than it is in search conditions.\nWhen comparing rows, set operators treat NULL values as equal to each other.\nIn contrast, when NULL is compared to NULL in a search condition the result is UNKNOWN (not true)."
  },
  {
    "objectID": "notes/NumericalAnalysis1.html",
    "href": "notes/NumericalAnalysis1.html",
    "title": "Undergraduate-Level Numerical Analysis",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes/Proofs.html#proof-practice",
    "href": "notes/Proofs.html#proof-practice",
    "title": "IB Math AA HL Notes",
    "section": "Proof Practice",
    "text": "Proof Practice"
  },
  {
    "objectID": "notes/Calc2.html",
    "href": "notes/Calc2.html",
    "title": "Calculus 2",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes/cs334.html",
    "href": "notes/cs334.html",
    "title": "CS 334 Machine Learning",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n1 Linear Classification\n\n\n\n\n\n\nLinear Cassification\n\n\nLoss Function\n\n\nTraining Error\n\n\nPerceptron\n\n\n\nThis lecture discusses the linear classification, a fundamental concept in machine learning. It introduces loss functions, training error, and the perceptron algorithm.\n\n\n\n\n\nJan 23, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n2 Gradient Descent on Classification\n\n\n\n\n\n\nClassification\n\n\nGradient Descent\n\n\nHinge Loss\n\n\n\nThis lecture discusses the gradient descent algorithm and its application to classification problem.It introduces the hinge loss function and the update rule for gradient descent.\n\n\n\n\n\nJan 28, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n3 Linear Regression\n\n\n\n\n\n\nLinear Regreesion\n\n\nGradient Descent\n\n\nSquared Loss\n\n\nLeast Squares\n\n\nNormal Equation\n\n\n\nThis lecture discusses the linear regression, a fundamental concept in machine learning. It introduces the squared loss function and the update rule for gradient descent. It also derived the closed-form solution for linear regression using matrix notation.\n\n\n\n\n\nJan 30, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n4 Regularization\n\n\n\n\n\n\nRegularization\n\n\nBias-Variance Tradeoff\n\n\nRidge Regression\n\n\nLasso Regression\n\n\nElastic Net\n\n\n\nThis lecture starts from the bias-variance tradeoff, and then introduces regularization as a way to control the tradeoff. We will discuss the \\(L_2\\) regularization (Ridge Regression), \\(L_1\\) regularization (Lasso Regression), and Elastic Net.\n\n\n\n\n\nFeb 4, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n5 Logistic Regression\n\n\n\n\n\n\nLogistic Regression\n\n\nClassification\n\n\nSigmoid Function\n\n\n\nThis lecture introduces the logistic regression model, which is used for binary classification. We will discuss the sigmoid function, the likelihood function, and the cost function.\n\n\n\n\n\nFeb 6, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n6 Model Selection and Model Assessment\n\n\n\n\n\n\nModel Selection\n\n\nModel Assessment\n\n\nClassification Metrics\n\n\nRegression Metrics\n\n\nCross Validation\n\n\n\nThis lecture discusses the model assessment and model selection process. We will cover classification performance metrics, regression metrics, model assessment process, and model selection.\n\n\n\n\n\nFeb 11, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n7 Feature Selection and Kernels\n\n\n\n\n\n\nFeature Engineering\n\n\nFeature Selection\n\n\nKernels\n\n\n\nThis lecture discusses the feature selection and kernel methods. We will cover feature engineering and selection methods, kernel methods, and kernel tricks.\n\n\n\n\n\nFeb 18, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n8 Decision Trees and Random Forest\n\n\n\n\n\n\nDecision Trees\n\n\nEnsemble Methods\n\n\nRandom Forest\n\n\n\nThis lecture discusses the basics od decision trees including how to build a decision tree. It also introduces ensemble methods and discusses the random forest algorithm as an example of ensemble methods.\n\n\n\n\n\nMar 6, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n9 Boosting\n\n\n\n\n\n\nBoosting\n\n\nAdaBoost\n\n\nEnsemble Methods\n\n\n\nThis lecture discusses Boosting, a powerful ensemble learning technique that combines weak learners to create a strong learner.\n\n\n\n\n\nMar 20, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n10 Introduction to Neural Networks\n\n\n\n\n\n\nNeural Networks\n\n\nBackpropagation\n\n\nActivation Functions\n\n\n\nThis lecture discusses the basics of neural networks, including their architecture, activation functions, and training process. It also covers the concept of backpropagation and its role in optimizing neural networks.\n\n\n\n\n\nMar 25, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n11 Convolutional Neural Networks\n\n\n\n\n\n\nNeural Networks\n\n\nCNNs\n\n\nImage Processing\n\n\nDeep Learning\n\n\n\nThis lecture discusses the architecture and functioning of Convolutional Neural Networks (CNNs), including their layers, operations, and applications in image processing and computer vision. It also covers the concept of pooling layers and their role in reducing dimensionality.\n\n\n\n\n\nApr 1, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n12 Recurrent Neural Networks\n\n\n\n\n\n\nNeural Networks\n\n\nRecurrent Neural Networks\n\n\nLSTM\n\n\nDeep Learning\n\n\n\nThis lecture discusses the basics of recurrent neural networks (RNNs), including their architecture, training process, and applications. It also covers the concept of long short-term memory (LSTM) networks and their role in handling sequential data.\n\n\n\n\n\nApr 7, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n13 Reinforcement Learning\n\n\n\n\n\n\nReinforcement Learning\n\n\nMulti-Armed Bandit\n\n\nExploration-Exploitation\n\n\n\nThis lecture discusses the basics of reinforcement learning, including the concepts of agents, environments, rewards, and policies. It also covers the exploration-exploitation trade-off and multi-armed bandit problem.\n\n\n\n\n\nApr 13, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n14 Recommender Systems\n\n\n\n\n\n\nRecommender Systems\n\n\nCollaborative Filtering\n\n\nMatrix Factorization\n\n\n\nThis lecture discusses the basics of recommender systems, focusing on collaborative filtering. It introduces the nearest neighbor algorithm and matrix factorization techniques, including low-rank approximation via alternative minimization algorithm.\n\n\n\n\n\nApr 15, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n15 Clustering\n\n\n\n\n\n\nClustering\n\n\nUnsupervised Learning\n\n\nK-means\n\n\n\nThis lecture discusses the basics of clustering, focusing on the k-means algorithm. It covers the algorithm’s initialization, convergence, and the concept of local minima.\n\n\n\n\n\nApr 20, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "My Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nPolynomial Representation of Arnoldi\n\n\n\nNumerical Analysis\n\n\nIterative Method\n\n\nLinear Algebra\n\n\nArnoldi\n\n\n\nI’m recently learning about the Arnoldi’s method, and the textbook mentioned that the it can be viewed from a polynomial approximation point of view. However, I think the…\n\n\n\nJiuru Lyu\n\n\nDec 10, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "photo.html",
    "href": "photo.html",
    "title": "Photograph",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "High School Level Math\n\nIB Math AA HL Notes\n\n\n\nCollege Level Math\n\nMath Fundamentals\n\nCalculus II\nMultivariable Calculus/Advanced Calculus/Calculus III\nLinear Algebra\nMathematical Proofs\n\n\n\nApplied Mathematics\n\nOrdinary Differential Equations\nNonlinear Optimization\nUndergraduate-Level Numerical Analysis\nNumerical ODEs and PDEs\nPhD-Level Numerical Analysis I (Numerical Linear Algebra)\n\n\n\nPure Mathematics\n\nReal Analysis\n\n\n\nData Science, Statistics, and Causal Inference\n\nIntroduction to Causal Inference\nGoogle Data Analytics Learning Notes\nMathematical Statistics\nCausal Designs and Inference\nMachine Learning\n\n\n\n\nComputer Science\n\nObject-Oriented Programming & Introduction to Data Structures\nIntroduction to Data Structure and Algorithms\nDatabase Systems\n\n\n\nOther Fields\n\nIntroduction to Sociology\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jiuru Lyu",
    "section": "",
    "text": "Hi! My name is Jiuru Lyu, and I am a junior at Emory University studying Applied Mathematics. In my leisure time, I enjoy coffee brewing, traveling, photography, and commercial aviation."
  },
  {
    "objectID": "notes/cs334/02-Gradient-Descent-on-Classification/02-Gradient-Descent-on-Classification.html",
    "href": "notes/cs334/02-Gradient-Descent-on-Classification/02-Gradient-Descent-on-Classification.html",
    "title": "2 Gradient Descent on Classification",
    "section": "",
    "text": "Goal: Learning to classify non-linearly separable data (by considering a different objective function)."
  },
  {
    "objectID": "notes/cs334/02-Gradient-Descent-on-Classification/02-Gradient-Descent-on-Classification.html#objective-function-and-hinge-loss",
    "href": "notes/cs334/02-Gradient-Descent-on-Classification/02-Gradient-Descent-on-Classification.html#objective-function-and-hinge-loss",
    "title": "2 Gradient Descent on Classification",
    "section": "Objective Function and Hinge Loss",
    "text": "Objective Function and Hinge Loss\n\n\n\n\n\n\n\nDefinition 1 (Objective Function for Perceptron Algorithm) \\[\n\\epsilon_{N}(\\va\\theta)=\\sum_{i=1}^N\\1\\qty{-y^{(i)}(\\va\\theta\\cdot\\va x^{(i)})\\leq0},\n\\] for conciseness, we assume the offset parameter is implicity. Note that the empirical risk is \\[\nR_N(\\va\\theta)=\\frac{1}{N}\\sum_{i=1}^N\\operatorname{loss}\\qty(y^{(i)}\\qty(\\va\\theta\\cdot\\va x^{(i)})),\n\\] so, training error is a special case of empirical risk.\n\n\n\n\n\nPreviously, we were using “zero-one loss”. Let \\(z=y(\\va\\theta\\cdot\\va x)\\), then the zero-one loss is \\[\n\\operatorname{loss}_{0-1}(z)=\\1\\qty{z\\leq0}=\\begin{cases}1\\quad&\\text{if }z\\leq 0\\\\0\\quad&\\text{o.w}\\end{cases}.\n\\]\n\nThe graph of the zero-one loss is shown below:\n\n\n\n\n\n\nFigure 1: Zero One Loss\n\n\n\n\nHowever, there are some issues with the zero-one loss:\n\nNot continuous at \\(z=0\\).\nNot differentiable at \\(z=0\\).\nNot convex.\n\nDue to these issues, direct minimization of empirical risk with \\(0-1\\) loss is chanllenging for the general case (NP-hard).\n\n\n\n\n\n\n\nTip 1: Calculus Refresher\n\n\n\n\n\n\nContinuous: A function \\(f\\) is continuous at \\(x_0\\) if \\[\\lim_{x\\to x_0}f(x)=f(x_0)\\].\nDifferentiable: A function \\(f\\) is differentiable at \\(x_0\\) if the following limit exists: \\[\\lim_{h\\to 0}\\frac{f(x_0+h)-f(x_0)}{h}\\] Also, Differentiable \\(\\implies\\) continuous.\nConvex/concave up: \\[f(\\lambda x+(1-\\lambda)y)\\leq\\lambda f(x)+(1-\\lambda)f(y).\\]\n\n\n\n\n\n\n\nFigure 2: Convex Function\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 1 (Motivation of Hinge Loss: Misclassification)  \n\n\n\n\n\n\nFigure 3: Misclassification on Non-Linearly Separable Data\n\n\n\n\nLoss (or cost) associated with the ==misclassification== of these points is identified under \\(0-1\\) loss. Then, loss is \\(1\\) for each misclassified point.\nHowever, these two misclassifications are not equally bad.\nWe need a loss function that treats these mistakes differently.\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2 \\[\n\\operatorname{loss}_h(z)=\\max\\qty{0,1-z}=\\begin{cases}1-z\\quad&\\text{if }z\\leq 1\\\\0\\quad&\\text{if }z&gt;1\\end{cases}\n\\]\nThe graph of the hinge loss is shown below:\n\n\n\n\n\n\nFigure 4: Hinge Loss\n\n\n\n\n\n\n\n\nProperties of hinge loss:\n\nNon-negative.\nIf \\(z=y\\qty(\\va\\theta\\cdot\\va x)&lt;1\\), we incur a non-zero cost.\nThis forces predictions to be more than just correct, but we need \\[y\\qty(\\va\\theta\\cdot\\va x)\\geq 1.\\] Note, previously, we only imposed \\(y\\qty(\\va\\theta\\cdot\\va x)&gt;0\\).\n\n\n\n\n\n\n\n\n\nRemark 1 (What does \\(z=y\\qty(\\va\\theta\\cdot\\va x)\\) tell us?). \n\nSign: correct (\\(&gt;0\\)) or wrong (\\(&lt;0\\)).\nMagnitude: how wrong (the larger, the more wrong). \\[\n\\abs{y\\qty(\\va\\theta\\cdot\\va x)}=\\abs{y}\\cdot\\abs{\\va\\theta\\cdot\\va x},\n\\] where \\(\\abs{y}=1\\) since \\(y\\in{-1,+1}\\) and \\(\\dfrac{\\abs{\\va\\theta\\cdot\\va x}}{\\norm{\\va\\theta}}\\) is the distance from the point to the decision boundary.\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3 (Empirical Risk with Hinge Loss) \\[\nR_N(\\va\\theta)=\\dfrac{1}{N}\\sum_{i=1}^N\\max\\qty{0, 1-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}}\n\\] Why is it better/easier to minimize? - Continuous - (Sub)differentiable - Convex \\(\\implies\\) We can use a simple algorithm to minimize it: Gradient Descent."
  },
  {
    "objectID": "notes/cs334/02-Gradient-Descent-on-Classification/02-Gradient-Descent-on-Classification.html#gradient-descent",
    "href": "notes/cs334/02-Gradient-Descent-on-Classification/02-Gradient-Descent-on-Classification.html#gradient-descent",
    "title": "2 Gradient Descent on Classification",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nGradient:\n\nUnivariate case: \\(f:\\R\\to\\R\\), \\(y=f(x)\\). Its derivative is \\(f'(x)=\\dsst\\dv{x} f(x)\\). \\(f'(x)\\) represents the rate of change:\n\n\\(f'(x)&gt;0\\): increasing\n\\(f'(x)=0\\): a critical point\n\\(f'(x)&lt;0\\): decreasing.\n\nMultivariate case: \\(f:\\R^d\\to\\R\\), \\(y=f(\\va x)=f(x_1,x_2,\\dots,x_d)\\). Its gradient is \\[\\grad_{\\va x} f(\\va x)=\\mqty[\\dsst\\pdv{x_1} f(\\va x),\\dots,\\dsst\\pdv{x_d} f(\\va x)]^\\top.\\] The gradient points in the direction of the steepest ascent.\n\nGradient Descent (GD):\n\nSuppose we want to minimize some \\(f(\\va\\theta)\\) with respect to \\(\\va\\theta\\). We know gradient \\(\\grad_{\\va\\theta}f(\\va\\theta)\\) points in the direction of steepest increase.\nIdea: start somewhere, and take small steps in the opposite direction as gradient.\n\n\n\n\n\\begin{algorithm} \\caption{Gradient Descent (GD)} \\begin{algorithmic} \\State $k=0$; $\\va\\theta^{(0)}=\\va 0$ \\While{not converged} \\State $\\va\\theta^{(k+1)}=\\va\\theta^{(k)}-\\underbrace{\\eta_k}_{\\substack{\\text{learning}\\\\\\text{rate}}}\\overbrace{\\eval{\\grad_{\\va\\theta} f(\\va\\theta)}_{\\va\\theta=\\va\\theta^{(k)}}}^{\\substack{\\text{evaluate gradient}\\\\\\text{at current }\\va\\theta}}$ \\State $k++$ \\EndWhile \\end{algorithmic} \\end{algorithm}\n\n\n\nImprove GD (Algorithm 1): Stochastic Gradient Descnet (SGD):\n\nIn GD (Algorithm 1), we need to compute the gradient over the entire dataset. This can be expensive for large datasets.\nIn SGD (Algorithm 2), we compute the gradient for each data point and update \\(\\va\\theta\\) accordingly.\n\n\n\n\n\\begin{algorithm} \\caption{Stochastic Gradient Descent (SGD)} \\begin{algorithmic} \\State $k=0$; $\\va\\theta^{(0)}=\\va 0$ \\While{not converged} \\State randomly shuffle points \\For{$i=1,\\dots, N$} \\State $\\va\\theta^{(k+1)}=\\va\\theta^{(k)}-\\eta_k\\eval{\\grad_{\\va\\theta}\\operatorname{loss}(y^{(i)}\\va\\theta\\cdot\\va x^{(i)})}_{\\va\\theta=\\va\\theta^{(k)}}$ \\State $k++$ \\EndFor \\EndWhile \\end{algorithmic} \\end{algorithm}\n\n\n\nGradient for Hinge Loss: \\[\n\\operatorname{loss}_h(z)=\\max\\qty{0,1-y\\va\\theta\\cdot\\va x}\n\\]\n\n\\(\\boxed{\\text{Case I}}\\) If \\(y^{(i)}\\va\\theta\\cdot\\va x^{(i)}\\geq 1\\): loss is zero. Already at minimum. No update.\n\\(\\boxed{\\text{Case II}}\\) If \\(y^{(i)}\\va\\theta\\cdot\\va x^{(i)}&lt;1\\): loss is \\[1-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}\\] Note that \\(\\va\\theta\\cdot\\va x=\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_dx_d\\), we have \\[\n\\grad_{\\va\\theta}(\\va\\theta\\cdot\\va x)=\\mqty[x_1,x_2,\\dots,x_d]^\\top=\\va x.\n\\] So, the gradient is \\[\n\\begin{aligned}\n\\grad_{\\va\\theta}\\operatorname{loss}_h(y^{(i)}\\va\\theta\\cdot\\va x^{(i)})&=\\grad_{\\va\\theta}(1-y^{(i)}\\va\\theta\\cdot\\va x)\\\\\n&=0-y^{(i)}\\grad_{\\va\\theta}(\\va\\theta\\cdot\\va x)\\\\\n&=-y^{(i)}\\va x^{(i)}.\n\\end{aligned}\n\\]\nSo, in Algorithm 2, the update rule is\n\\[\n\\begin{aligned}\n\\va\\theta^{(k+1)}&=\\va\\theta^{(k)}-\\eta_k\\eval{\\grad_{\\va\\theta}\\operatorname{loss}(y^{(i)}\\va\\theta\\cdot\\va x^{(i)})}_{\\va\\theta=\\va\\theta^{(k)}}\\\\\n&=\\va\\theta^{(k)}+\\eta_ky^{(i)}\\va x^{(i)}.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nRemark 2 (Why shuffle points in SGD?). \n\nRemove the effect of any unintended/aritifical ordering in the dataset.\nFaster convergence\n\n\n\n\n\n\nStopping criteria\n\nCheck if empirical risk stops decreasing.\nCheck if parameter vector stop changing.\nNote: make \\(\\operatorname{loss}=0\\) does not work here since the data is not perfectly linearly separable. \n\nLearning rate/Step size/\\(\\eta\\):\n\nToo small: slow convergence.\nToo large: overshot & never converge\nA hyperparameter that can (and should) be tuned.\nMay set \\(\\eta\\) as a constant or a function of \\(k\\). For example, \\[\\eta_k=\\dfrac{1}{k}.\\]\n\n\n\n\n\n\n\n\n\nTheorem 1 (GD (Algorithm 1) and SGD (Algorithm 2) Convergence Theorem)  \n\nWith appropriate learning rate \\(\\eta\\), if \\(R_N(\\va\\theta)\\) is convex, (S)GD will converge to the global minimum almost surely.\nSGD is a general algorithm that can be applied to non-convex functions as well, in which case it converges to a local minimum.\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 4 (Robbins-Monro Conditions) A good learning rate ensures convergence satisfying the Robbins-Monro conditions: \\[\n\\sum_{k=1}^\\infty\\eta_k=\\infty\\quad\\text{and}\\quad\\sum_{k=1}^\\infty\\eta_k^2&lt;\\infty.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nRemark 3 (Differentiability of Loss Function). \\(R_N(\\va\\theta)\\) with hinge loss is not everywhere differentiable since it si piecewise linear. What do we do? - When differentiable, use gradient. - When sub-differentiable, choose any gradient around the kink."
  },
  {
    "objectID": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html",
    "href": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html",
    "title": "3 Linear Regression",
    "section": "",
    "text": "Goal: Given data \\(\\qty{\\va x^{(i)}, y^{(i)}}_{i=1}^N\\), find a linear model \\[\nf(\\va x;\\va\\theta)=\\va\\theta\\cdot\\va x,\n\\] such that minimizes the empirical loss."
  },
  {
    "objectID": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html#the-loss-function",
    "href": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html#the-loss-function",
    "title": "3 Linear Regression",
    "section": "The Loss Function",
    "text": "The Loss Function\n\n\n\n\n\n\n\nDefinition 1 (Squared Loss Function) The empirical loss for linear regression is defined by \\[\nR_N(\\va\\theta)=\\dfrac{1}{N}\\sum_{i=1}^N\\operatorname{loss}\\qty(y^{(i)}, \\va\\theta\\cdot\\va x^{(i)}),\n\\] where loss is the squared loss: \\[\n\\operatorname{loss}(z)=\\dfrac{z^2}{2}.\n\\]\nThis loss function is continuous, differentiable, and convex.\nHere is a graph of the squared loss function:\n\n\n\n\n\n\nFigure 1: Squared Loss\n\n\n\nIntuition: we permits small discrepancies but penalizes large deviations."
  },
  {
    "objectID": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html#ordinary-least-squares-ols",
    "href": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html#ordinary-least-squares-ols",
    "title": "3 Linear Regression",
    "section": "Ordinary Least Squares (OLS)",
    "text": "Ordinary Least Squares (OLS)\n\nWe will minimize the empirical loss with squared loss, i.e., \\[\nR_N(\\va\\theta)=\\dfrac{1}{N}\\sum_{i=1}^N\\dfrac{\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})^2}{2},\n\\] using SGD algorithm\nRecall the gradient descent update rule: \\[\n\\va\\theta^{(k+1)}=\\va\\theta^{(k)}-\\eta_k\\eval{\\grad_{\\va\\theta}\\operatorname{loss}(y^{(i)}\\va\\theta\\cdot\\va x^{(i)})}_{\\va\\theta=\\va\\theta^{(k)}}\n\\]\n\nIn OLS case, we have \\[\n\\begin{aligned}\n\\grad_{\\va\\theta}\\operatorname{loss}(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})&=\\grad_{\\va\\theta}\\dfrac{\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})^2}{2}\\\\\n&=-\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})\\va x^{(i)}.\n\\end{aligned}\n\\]\nSo, the update rule is \\[\n\\va\\theta^{(k+1)}=\\va\\theta^{(k)}+\\eta_k\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})\\va x^{(i)}.\n\\]\n\n\n\\begin{algorithm} \\caption{SGD for Least Squares Regression} \\begin{algorithmic} \\State $k=0$; $\\va\\theta^{(0)}=\\va 0$ \\While{not converged} \\State randomly shuffle points \\For{$i=1,\\dots, N$} \\State $\\va\\theta^{(k+1)}=\\va\\theta^{(k)}+\\eta_k\\eval{\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})\\va x^{(i)}}_{\\va\\theta=\\va\\theta^{(k)}}$ \\State $k++$ \\EndFor \\EndWhile \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\n\nRemark 1 (Compare with Hinge Loss Classification). \n\nWe make updates on every data points\nStopping criteria, learning rate, shuffling, etc., are the same."
  },
  {
    "objectID": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html#closed-form-solution",
    "href": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html#closed-form-solution",
    "title": "3 Linear Regression",
    "section": "Closed-Form Solution",
    "text": "Closed-Form Solution\n\nSince \\(R_N(\\va\\theta)\\) with squared loss is a covex function and differentiable everywhere, we can try to minimize it analytically by setting the gradient to \\(0\\).\nRewrite empirical risk in matrix notation: \\[\nX=\\mqty[-&\\va x^{(1)\\top}&-\\\\&\\vdots&\\\\-&\\va x^{(N)\\top}&-]_{N\\times d}\\quad\\text{and}\\quad \\va y=\\mqty[y^{(1)}\\\\\\vdots\\\\y^{(N)}]_{N\\times 1}.\n\\] Then, the empirical risk is \\[\n\\begin{aligned}\nR_N(\\va\\theta)=\\dfrac{1}{2}\\dfrac{1}{N}\\sum_{i=1}^N\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})^2&=\\dfrac{1}{2N}\\qty(X\\va\\theta-\\va y)^\\top\\qty(X\\va\\theta-\\va y)\\\\\n&=\\dfrac{1}{2N}\\qty(\\va\\theta^\\top X^\\top X\\va\\theta-2\\va\\theta^\\top (X^\\top\\va y)+\\va y^\\top\\va y).\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nTip 1: Commonly Used Differentiation Rules\n\n\n\n\n\n\n\n\n\\(f(\\va x)\\)\n\\(\\grad_{\\va x}f(\\va x)\\)\n\n\n\n\n\\(\\va v^\\top\\va x\\)\n\\(\\va v\\)\n\n\n\\(\\va x^\\top\\va x\\)\n\\(2\\va x\\)\n\n\n\\(\\va x^\\top A\\va x\\)\n\\((A+A^\\top)\\va x\\) 1\n\n\n\n\n\n\n\nTherefore, the gradient of \\(R_N(\\va\\theta)\\) is\n\n\\[\n\\begin{aligned}\n\\grad_{\\va\\theta}R_N(\\va\\theta)&=\\dfrac{1}{2N}\\qty(2X^\\top X\\va\\theta-2(X^\\top\\va y))\\\\\n&=\\dfrac{1}{N}\\qty(X^\\top X\\va\\theta-X^\\top\\va y).\n\\end{aligned}\n\\]\n\nSet gradient to \\(0\\), we have\n\n\\[\n\\begin{aligned}\n\\grad_{\\va\\theta}R_N(\\va\\theta)&=0\\\\\n\\dfrac{1}{N}\\qty(X^\\top X\\va\\theta-X^\\top\\va y)&=0\\\\\nX^\\top X\\va\\theta-X^\\top\\va y&=0\\\\\nX^\\top X\\va\\theta&=X^\\top\\va y\\\\\n\\va\\theta^*&=\\qty(X^\\top X)^{-1}X^\\top\\va y.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nRemark 2 (Normal Equation and Inveritibility). \n\nThe solutions is called the Normal Equation or the OLS solution.\nOLS solution \\(\\va\\theta^*=\\qty(X^\\top X)^{-1}X^\\top\\va y.\\) exists only when \\(X^\\top X\\) is invertible.\nInvertibility of \\(X^\\top X\\): “Gram matrix”\n\nWhy might it be non-invertible/singular/degenerate? \\[\\rank(X^\\top X)\\leq\\rank(X)\\leq\\min\\qty{N,d}\\]\n\nIf \\(d&gt;N\\), then \\(\\rank(X^\\top X)\\leq N&lt;d\\). Since \\(X^\\top X\\in\\R^{d\\times d}\\), we know \\(X^\\top X\\) is not full rank, and thus non-invertible.\nFor example, if we have duplicated features, say \\(x_1=x_2\\), then if \\(\\mqty[\\theta_1,\\theta_2]\\) is a solution, then \\(\\mqty[\\theta_1-c,\\theta_2+c]\\) is also a solution.\n\nWhat to do if \\(X^\\top X\\) is not invertible?\n\nMonro-Penrose Pseudo-Inverse: \\[\\va\\theta^*=(X^\\top X)^{\\dagger}X^\\top \\va y=X^{\\dagger}\\va y\\]\nRegularization: See next lecture."
  },
  {
    "objectID": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html#footnotes",
    "href": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html#footnotes",
    "title": "3 Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf \\(A\\) is symmetric, then \\(\\grad_{\\va x}f(\\va x)=2A\\va x\\).↩︎"
  },
  {
    "objectID": "notes/cs334/04-Regularization/04-Regulairzation.html",
    "href": "notes/cs334/04-Regularization/04-Regulairzation.html",
    "title": "4 Regularization",
    "section": "",
    "text": "Polynomial regression:\n\nInstead of only using \\(\\phi(x)=[1,x]^\\top\\) , we can add higher dimension terms of feature: \\[\\phi(x)=[1,x,x^2,x^3,\\dots]^\\top\\]\nHowever, if we get too far, we will encounter overfitting.\n\nOverfitting and Underfitting:\n\nIf we optimize \\(R(\\va\\theta, D)\\), \\(R(\\va\\theta, D_\\text{test})\\) can be learge due to:\n\n\n\n\n\nUnderfitting\nOverfitting\n\n\n\n\nBias\nVariance\n\n\nApproximation error\nEstimation error\n\n\nPoor on training set\nGood on training set\n\n\nPoor on test set\nPoor on test set\n\n\nAdding more data will not help\nAdding more data will help\n\n\n\n\n\n\n\n\n\nFigure 1: Model Complexity vs. Error: Bias-Variance Tradeoff\n\n\n\n\nSources of Bias:\n\nModel class too small: unable to represent the underlying relationship\nModels are “too global” (e.g., constant output, single linear separator)\n\nReduing Bias: Use more complex models\n\nInteraction terms\nAdd more features\nKernels\nAlgorithm-specific approaches\n\nSources of Variance:\n\nNoise in labels or features\nModels are too “local” – sensitive to small changes in feature values\nTraining dataset too small.\n\nReducing Variance: Collect more data, or less complex models\n\nDrop interaction terms\nRegualrization\nFeature selection\nDon’t use kernels\nEnsemble\n\nBalancing Bias and Variance: Generalizable Model\n\nBut how to control the tradeoff? Regularization."
  },
  {
    "objectID": "notes/cs334/04-Regularization/04-Regulairzation.html#introduction-bias-variance-tradeoff",
    "href": "notes/cs334/04-Regularization/04-Regulairzation.html#introduction-bias-variance-tradeoff",
    "title": "4 Regularization",
    "section": "",
    "text": "Polynomial regression:\n\nInstead of only using \\(\\phi(x)=[1,x]^\\top\\) , we can add higher dimension terms of feature: \\[\\phi(x)=[1,x,x^2,x^3,\\dots]^\\top\\]\nHowever, if we get too far, we will encounter overfitting.\n\nOverfitting and Underfitting:\n\nIf we optimize \\(R(\\va\\theta, D)\\), \\(R(\\va\\theta, D_\\text{test})\\) can be learge due to:\n\n\n\n\n\nUnderfitting\nOverfitting\n\n\n\n\nBias\nVariance\n\n\nApproximation error\nEstimation error\n\n\nPoor on training set\nGood on training set\n\n\nPoor on test set\nPoor on test set\n\n\nAdding more data will not help\nAdding more data will help\n\n\n\n\n\n\n\n\n\nFigure 1: Model Complexity vs. Error: Bias-Variance Tradeoff\n\n\n\n\nSources of Bias:\n\nModel class too small: unable to represent the underlying relationship\nModels are “too global” (e.g., constant output, single linear separator)\n\nReduing Bias: Use more complex models\n\nInteraction terms\nAdd more features\nKernels\nAlgorithm-specific approaches\n\nSources of Variance:\n\nNoise in labels or features\nModels are too “local” – sensitive to small changes in feature values\nTraining dataset too small.\n\nReducing Variance: Collect more data, or less complex models\n\nDrop interaction terms\nRegualrization\nFeature selection\nDon’t use kernels\nEnsemble\n\nBalancing Bias and Variance: Generalizable Model\n\nBut how to control the tradeoff? Regularization."
  },
  {
    "objectID": "notes/cs334/04-Regularization/04-Regulairzation.html#regularization",
    "href": "notes/cs334/04-Regularization/04-Regulairzation.html#regularization",
    "title": "4 Regularization",
    "section": "Regularization",
    "text": "Regularization\n\n\n\n\n\n\n\nDefinition 1 (Regularization Training Error) A “knob” for controlling model complexity: \\[\nJ(\\va\\theta)=\\underbrace{R_N(\\va\\theta)}_\\text{emiprical risk}+\\underbrace{\\lambda\\Omega(\\va\\theta)}_\\text{regularization term},\n\\] where \\(\\lambda\\) is the regularization strength, \\(\\lambda&gt;0\\), and \\(\\Omega(\\va\\theta)\\) is the regularizer/regularization penalty.\nThe regularization strength \\(\\lambda\\) balances: - How well we fit the data, and - Complexity of model.\nThen, our goal is to \\(\\displaystyle\\min_{\\va\\theta}J(\\va\\theta)\\).\n\n\n\n\n\nCommon Regularizer:\n\n\\(L_2\\): Ridge Regression \\[\\Omega(\\va\\theta)=\\dfrac{\\|\\va\\theta\\|_2^2}{2}=\\dfrac{1}{2}\\sum_{j=1}^d\\theta_j^2\\]\n\\(L_1\\): Lasso Regression \\[\\Omega(\\va\\theta)=\\|\\va\\theta\\|_1=\\sum_{j=1}^d\\abs{\\theta_j}\\]\nElastic net: \\[\\Omega(\\va\\theta)=\\lambda_2\\|\\va\\theta\\|_2^2+\\lambda_1\\|\\va\\theta\\|_1\\]\n\nEffect of Regularization:\n\nWhen minimizing \\(J(\\va\\theta)\\), we are still trying to minimize \\(R_N(\\va\\theta)\\), but at the same time, minimize \\(\\Omega(\\va\\theta)\\)\n\nPush parameters to small values\nResist setting parameters away from default of \\(0\\) unless data strong suggest otherwise.\n\nWhy are small values in \\(\\va\\theta\\) good?\n\nLimit effect of small perturbations in individual factors on putput.\nIf \\(\\theta_j=0\\) exactly, then \\(j\\)-th parameter is effective unused \\(\\implies\\) feature selection.\nOccam’s Razor (\\(13\\)-th century philosopher, William of Ockham): “When you have two competing hypotheses, the simpler one is preferred.”\n\n\nRidge Regression (\\(L_2\\)-Regularized Linear Regression) \\[J(\\va\\theta)=\\sum_{i=1}^N\\dfrac{(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})^2}{2}+\\lambda\\dfrac{\\|\\va\\theta\\|_2^2}{2},\\] where \\(\\dfrac{1}{N}\\) is absorbed into \\(\\lambda\\).\n\nSGD: \\[\\grad_{\\va\\theta}\\qty(\\dfrac{\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})^2}{2}+\\lambda\\dfrac{\\|\\va\\theta\\|_2^2}{2})\\] gradient of squared \\(L_2\\) norm: \\(\\|\\va\\theta\\|_2^2=\\theta_1^2+\\cdots+\\theta_d^2=\\va\\theta\\cdot\\va\\theta\\). So, \\[\\pdv{\\theta_j}\\qty(\\|\\va\\theta\\|_2^2)=2\\theta_j.\\] Then, we have \\[\n\\begin{aligned}\n  \\grad_{\\va\\theta}\\qty(\\|\\va\\theta\\|_2^2)=\\grad_{\\va\\theta}(\\va\\theta\\cdot\\va\\theta)&=\\mqty[\\displaystyle\\pdv{\\theta_1}\\qty(\\|\\va\\theta\\|_2^2),\\dots,\\pdv{\\theta_d}\\qty(\\|\\va\\theta\\|_2^2)]\\\\\n  &=\\mqty[2\\theta_1,2\\theta_2,\\dots,2\\theta_d]\\\\\n  &=2\\va\\theta.\n\\end{aligned}\n\\] Therefore, \\[\n\\grad_{\\va\\theta}\\qty(\\dfrac{\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})^2}{2}+\\lambda\\dfrac{\\|\\va\\theta\\|_2^2}{2})=-\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})\\va x^{(i)}+\\lambda\\va\\theta.\n\\] Hence, the SGD update rule is \\[\n\\begin{aligned}\n\\va\\theta^{(k+1)}=\\va\\theta^{(k)}-\\eta_k\\eval{\\qty[-\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})\\va x^{(i)}+\\lambda\\va\\theta]}_{\\va\\theta=\\va\\theta^{(k)}}\\\\\n&=\\va\\theta^{(k)}+\\eta_k\\qty(y^{(i)}-\\va\\theta^{(k)}\\cdot\\va x^{(i)})\\va x^{(i)}-\\eta_k\\lambda\\va\\theta^{(k)}\\\\\n&=\\underbrace{(1-\\eta_k\\lambda)}_{\\text{between }0\\text{ and }1}\\va\\theta^{(k)}+\\underbrace{\\eta_k\\qty(y^{(i)}-\\va\\theta^{(k)}\\cdot\\va x^{(i)})\\va x^{(i)}}_\\text{same as before}.\n\\end{aligned}\n\\] The term \\(1-\\eta_k\\lambda\\) shrinks parameters towards \\(0\\) in each update.\nClosed Form Solution: \\[\n\\begin{aligned}\nJ(\\va\\theta)&=\\dfrac{1}{2}(X\\va\\theta-\\va y)^\\top(X\\va\\theta-\\va y)+\\dfrac{\\lambda}{2}\\va\\theta^\\top\\va\\theta\\\\\n\\grad_{\\va\\theta} J(\\va\\theta)&=(XX)\\va\\theta-X^\\top y+\\lambda\\va\\theta\\\\\n&=(XX+\\lambda I_d)\\va\\theta-X^\\top y=0\\\\\n\\va\\theta^*&=(\\underbrace{XX+\\lambda I_d}_\\text{always invertible})^{-1}X^\\top y.\n\\end{aligned}\n\\]\n\nGeometric Interpretation of Regularization: \\[\n\\va\\theta^*=\\argmin_{\\va\\theta}R_N(\\va\\theta)+\\lambda\\Omega(\\va\\theta) \\iff \\min_{\\va\\theta}R_N(\\va\\theta)\\quad\\text{s.t.}\\quad\\Omega(\\va\\theta)\\leq B.\n\\]\n\nWe can transfer a constrained optimization problem to an unconstrained one by adding a penalty term.\nThey are equivalent by the Lagrange multiplier.\n\n\n\n\n\n\n\n\nFigure 2: Geometric Interpretation of Regularization\n\n\n\n\nWhere is the optimal soltuion of the constrained optimization problem? The solution is the intersection of the level set of \\(R_N(\\va\\theta)\\) and the level set of \\(\\Omega(\\va\\theta)\\).\nMethod of Lagrange Multtiplier:\n\nThe pint where the level curve of function to minimize is tangent to (or just touching) the constraint.\n\nIntuition: two forces play\n\n\\(\\Omega(\\va\\theta)\\leq B\\): shrink \\(\\va\\theta\\) towards winthin the constraint\n\\(\\min R_N(\\va\\theta)\\): move \\(\\va\\theta\\) towards the uncosntraied minimizer.\n\n\\(L_2\\) regularization will make parameters generally small.\n\\(L_1\\) regularization will make parameters sparse, and make some parameters exeactly \\(0\\).\n\n\n\n\n\n\n\n\nRemark 1 (Remarks on Regularization). \n\nIf there is an offset/intercept, this coefficient is usually left unpenalized (depending on the implementation).\n\nIf we write \\(\\va\\theta\\cdot\\va x+b\\) explicity, no penalty on \\(b\\).\nIf we write augmented parameters, then we penalize \\(b\\).\n\nRegularization can be “unfair” if features are on the different scales. So, we need feature pre-procesing (e.g., centering and normalization).\nPratical usage of regression: sklearn\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nsklearn.linear_model.LinearRegression\nclosed form solution (wrapper of scipy.linalg.lstsq that uses SVD)\n\n\nsklearn.linear_model.Ridge\nRidge regressionm, can select solver (SGD, SVD, etc.)\n\n\nsklearn.linear_model.Lasso\nLasso regression, coordinate descent\n\n\nsklearn.linear_model.ElasticNet\nElastic net, coordinate descent\n\n\nsklearn.linear_model.SGDRegressor\ngeneric SGD, mix and match different loss and regularizers\n\n\n\n\nPratical use of linear classification: sklearn\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nsklearn.linear_model.LogisticRegression\nlogistic regression, can select solver (SGD, Newton-CG, etc.)\n\n\nsklearn.linear_model.SGDClassifier\ngeneric SGD, mix and match different loss and regularizers\n\n\nsklearn.linear_model.Perceptron\nperceptron, SGD with hinge loss\n\n\nsklearn.svm.LinearSVC\nlinear SVM, hinge loss (support vector machine)\n\n\nsklearn.svm.SVC\nnon-linear SVM, kernel trick"
  },
  {
    "objectID": "notes/cs334/05-Logistic-Regression/05-Logistic-Regression.html",
    "href": "notes/cs334/05-Logistic-Regression/05-Logistic-Regression.html",
    "title": "5 Logistic Regression",
    "section": "",
    "text": "The problem: Given binar labels \\(y\\in\\qty{-1,+1}\\). We want to predict the probability of positive class, \\(\\hat y\\in[0, 1]\\).\nHow to make a linear model output a probability? We already have \\(\\va\\theta\\cdot\\va x\\in(-\\infty,\\infty)\\).\n\nApply a sequashing function: \\(\\sigma(\\va\\theta\\cdot\\va x)\\in[0,1]:\\R\\to[0,1]\\).\nWe will use a sigmoid function*: \\[\\sigma(z)=\\dfrac{1}{1+e^{-z}}\\]\n\nRange of sigmoid:\n\nfor \\(z\\to\\infty\\), \\(\\sigma(z)\\to 1\\)\nfor \\(z\\to-\\infty\\), \\(\\sigma(z)\\to 0\\)\n\\(\\sigma(0)=0.5\\)\n\nUseful properties:\n\n\\(\\sigma(-z)=1-\\sigma(z)\\)\n\\(\\sigma'(z)=\\sigma(z)(1-\\sigma(z))\\)\ncontinuous\ndifferentiable\nnot convex\n\n\n\n\n\n\n\n\n\n\nFigure 1: Sigmoid Function"
  },
  {
    "objectID": "notes/cs334/05-Logistic-Regression/05-Logistic-Regression.html#motivation",
    "href": "notes/cs334/05-Logistic-Regression/05-Logistic-Regression.html#motivation",
    "title": "5 Logistic Regression",
    "section": "",
    "text": "The problem: Given binar labels \\(y\\in\\qty{-1,+1}\\). We want to predict the probability of positive class, \\(\\hat y\\in[0, 1]\\).\nHow to make a linear model output a probability? We already have \\(\\va\\theta\\cdot\\va x\\in(-\\infty,\\infty)\\).\n\nApply a sequashing function: \\(\\sigma(\\va\\theta\\cdot\\va x)\\in[0,1]:\\R\\to[0,1]\\).\nWe will use a sigmoid function*: \\[\\sigma(z)=\\dfrac{1}{1+e^{-z}}\\]\n\nRange of sigmoid:\n\nfor \\(z\\to\\infty\\), \\(\\sigma(z)\\to 1\\)\nfor \\(z\\to-\\infty\\), \\(\\sigma(z)\\to 0\\)\n\\(\\sigma(0)=0.5\\)\n\nUseful properties:\n\n\\(\\sigma(-z)=1-\\sigma(z)\\)\n\\(\\sigma'(z)=\\sigma(z)(1-\\sigma(z))\\)\ncontinuous\ndifferentiable\nnot convex\n\n\n\n\n\n\n\n\n\n\nFigure 1: Sigmoid Function"
  },
  {
    "objectID": "notes/cs334/05-Logistic-Regression/05-Logistic-Regression.html#logistic-regression",
    "href": "notes/cs334/05-Logistic-Regression/05-Logistic-Regression.html#logistic-regression",
    "title": "5 Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nLogistic Regression: \\[\nh(\\va x;\\va\\theta)=\\sigma(\\va\\theta\\cdot\\va x)=\\dfrac{1}{1+e^{-\\va\\theta\\cdot\\va x}}\n\\]\nHow to train this classifier? What loss function should we use?\n\nWhat we want: \\(\\sigma(\\va\\theta\\cdot\\va x)\\) should be the probability of positive class: \\(\\P[y=+1\\mid\\va x]\\).\nIdea: If \\(\\sigma(\\va\\theta\\cdot\\va x)\\) is truely the probability, then we can use it to wrtie down the likelihood of the training data \\(\\qty{\\va x^{(i)}, y^{(i)}}_{i=1}^N\\).\n\nFor each example \\(\\va x^{(i)}\\), the likelihood of seeing its label to be \\(y^{(i)}\\) is \\[\n\\P[y=y^{(i)}\\mid \\va x^{(i)};\\va\\theta]=\\begin{cases}\\sigma(\\va\\theta\\cdot\\va x^{(i)})&\\quad\\text{if }y^{(i)}=+1\\\\\\underbrace{1-\\sigma(\\va\\theta\\cdot\\va x^{(i)})}_{=\\sigma(-\\va\\theta\\cdot\\va x^{(i)})}&\\quad\\text{if }y^{(i)}=-1\\end{cases}=\\sigma(y^{(i)}\\va\\theta\\cdot\\va x^{(i)})\n\\]\nSince each training example is generated independently, \\[\\P\\qty[\\qty{y^{(i)}}_{i=1}^N\\mid\\qty{\\va x^{(i)}}_{i=1}^N;\\va\\theta]=\\prod_{i=1}^N\\sigma(y^{(i)}\\va\\theta\\cdot\\va x^{(i)})=\\prod_{i=1}^N\\dfrac{1}{1+e^{-y^(i)\\va\\theta\\cdot\\va x}}\\]\n\nGoal: Find \\(\\va\\theta^*\\) such that maximizes the likelihood of the training data: \\[\\va\\theta^*=\\argmax_{\\va\\theta}\\prod_{i=1}^N\\dfrac{1}{1+e^{-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}}}\\]\n\nTake the log: \\(\\prod\\to\\sum\\): \\[\\argmax_{\\va\\theta}\\log\\prod_{i=1}^N\\dfrac{1}{1+e^{-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}}}=\\argmax_{\\va\\theta}\\sum_{i=1}^N\\log\\qty(\\dfrac{1}{1+e^{-y^{(i)\\va\\theta\\cdot\\va x^{(i)}}}})\\]\nTake the negative: \\[\\argmax_{\\va\\theta}\\sum_{i=1}^N\\mathrm{loss}=\\argmin{\\va\\theta}\\qty(\\sum_{i=1}^N-\\log\\qty(\\dfrac{1}{1+e^{-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}}}))=\\argmin_{\\va\\theta}\\sum_{i=1}^N\\log\\qty(1+e^{-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}})\\]\n\n\n\n\n\n\n\n\n\n\nDefinition 1 (Logistic Loss:) \\[\\mathrm{loss}_\\text{log}\\qty(\\va x^{(i)}, y^{(i)};\\va\\theta)=\\log\\qty(1+e^{-y^{(i)}\\va\\theta\\cdot\\va x})\\]\n\n\n\n\n\n\nFigure 2: Logistic Loss\n\n\n\nThis loss is: - continuous - Differentiable, and - Convex\nSo, we can use SGD to minimize it."
  },
  {
    "objectID": "notes/cs334/05-Logistic-Regression/05-Logistic-Regression.html#sgd-update-rule",
    "href": "notes/cs334/05-Logistic-Regression/05-Logistic-Regression.html#sgd-update-rule",
    "title": "5 Logistic Regression",
    "section": "SGD Update Rule",
    "text": "SGD Update Rule\n\\[\n\\begin{aligned}\n\\grad_{\\va\\theta}\\log\\qty(1+e^{-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}})&=\\dfrac{1}{1+e^{-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}}}\\qty(e^{-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}})\\grad_{\\va\\theta}\\qty(-y^{(i)}\\va\\theta\\cdot\\va x^{(i)})\\\\\n&=\\dfrac{1}{e^{y^{(i)}\\va\\theta\\cdot\\va x^{(i)}}+1}\\qty(-y^{(i)}\\va x^{(i)})\\\\\n&=\\sigma\\qty(-y^{(i)}\\va\\theta\\cdot\\va x^{(i)})\\qty(-y^{(i)}\\va x^{(i)})\\\\\n&=-y^{(i)}\\va x^{(i)}\\qty(1-\\sigma(y^{(i)}\\va\\theta\\cdot\\va x^{(i)})).\n\\end{aligned}\n\\]\nSo, the update rule is: \\[\n\\begin{aligned}\n\\va\\theta^{(k+1)}&=\\va\\theta^{(k)}-\\eta_k\\eval{\\qty[-y^{(i)}\\va x^{(i)}\\qty(1-\\sigma(y^{(i)}\\va\\theta\\cdot\\va x^{(i)}))]}_{\\va\\theta=\\va\\theta^{(k)}}\\\\\n&=\\va\\theta^{(k)}+\\eta_ky^{(i)}\\va x^{(i)}\\qty(1-\\sigma(y^{(i)}\\va\\theta^{(k)}\\cdot\\va x^{(i)}))\n\\end{aligned}\n\\]\n\nThere’s no closed-form solution for logistic regression in general case. However, since the empirical risk function is convex, \\(\\exists\\) unique global minimum. When there are linearly dependent (redundant) feature, there are infinitely many equally good local minima.\nIf the data is linear separable, \\(\\|\\va\\theta\\|=\\infty\\) is bad. So, we need to add regularization."
  },
  {
    "objectID": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html",
    "href": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html",
    "title": "6 Model Selection and Model Assessment",
    "section": "",
    "text": "Given \\(\\hat y=h(\\va x;\\va\\theta)\\) and \\(D=\\qty{\\va x^{(i)}, y^{(i)}}_{i=1}^N\\), \\(\\va x^{(i)}\\in\\R^d\\) and \\(y^{(i)}\\in\\qty{-1,+1}\\).\n\nMisclassification error: \\[\\dfrac{1}{N}\\sum_{i=1}^N\\1[\\hat y^{(i)}\\neq y^{(i)}].\\]\nAccuracy: \\[\\dfrac{1}{N}\\sum_{i=1}^N\\1[\\hat y^{(i)}=y^{(i)}].\\]\nWhat’s wrong with accuracy: It can be misleading when we have disproportional groups. E.g., rare disease.\n\nConfusion Matrix and Classification Metrics:\n\n\n\n\n\nPredicted (\\(-\\))\nPredicted (\\(+\\))\n\n\n\n\nActual (\\(-\\))\nTrue Negative (TN)\nFalse Positive (FP) Type I Error\n\n\nActual (\\(+\\))\nFalse Negative (FN) Type II Error\nTrue Positive (TP)\n\n\n\n\nAccuracy: \\(\\dfrac{TP+TN}{N}\\)\nSensitivity / True Positive Rate (TRR) / Recall: \\(\\dfrac{TP}{TP+FN}\\).\nSpecificity / True Negative Rate (TNR): \\(\\dfrac{TN}{TN+FP}\\).\nFalse Positive Rate (FPR): \\(\\dfrac{FP}{TN+FP}=1-\\text{specificity}\\).\nPrecision / Positive Predictive Value (PPV): \\(\\dfrac{TP}{TP+FP}\\).\n\n\n\n\n\n\n\n\nRemark 1. If we predict everything to be positive, \\(TN=FB=0\\). Then, - Accuracy: \\(\\dfrac{TP}{N}\\). - Sensitivity: \\(1\\). - Specificity: \\(0\\). - False Positive Rate: \\(1\\). - Precision: \\(\\dfrac{TP}{N}\\).\n\n\n\n\n\nComposite Metrics: Trade-offs:\n\nBalanced Accuracy: mean of sensitivity and specificity. \\[\\dfrac{1}{2}\\qty(\\dfrac{TP}{TP+FN}+\\dfrac{TN}{TN+FP})\\]\nF1 Score: harmonic mean of precision and recall \\[2\\dfrac{\\cdot\\text{Precision}\\cdot\\text{Recall}}{\\text{Precision}+\\text{Recall}}\\]\n\nDiscrimination Thresholds: \n\nReceiver Operating Characteristic (ROC) Curve: TPR vs. FPR. \n\nEach point on the ROC curve corresponds to a threshold / a decision boundary.\nEach point represents a differenet trade-off between FPR and TPR.\nProperties of ROC:\n\nSlope is always upward.\nTwo non-intersecting curves means one model dominate the other.\nPerfect prediction vs. random prediction \nROC shows the trade-off between sensitivity and specificity.\nbut still not a very good summary metric: it is not a single number.\n\n\nAread under the ROC Curve (AUROC, ROC-AUC):\n\nCalculated using the trapezoid rule: sklearn.metrics.auc(x, y).\nIntuitive meaning: Given two randomly chosen examples, one positive and one negative, the probability of ranking positive example higher than the negative example.\n\\(AUC=1\\) for perfect prediction, \\(0.5\\) for random prediction.\n\\(AUC&gt;0.9\\): excellent prediction, but consider information leakage.\n\\(AUC\\approx0.8\\): good prediction.\n\\(AUC&lt;0.5\\): something is rong.\n\nPrecision-Recall Curve and AUPRC:\n\nA high AUPRC represents both higher recall and precision.\nROC curves should be used when there are rounghly equal numbers of observations for each class.\nPrecision-Recall curves may be used when there is a moderate to large class imbalance.\n\n\nClassifier Probability Calibration:\n\nWhen a model produce a probability of positive class, is that number actually a meaningful probability?\nFor example, if my model predicts 90% for a set of examples, does that mean 90% of those examples ahve label \\(+1\\)?\n\nMultiple classes metrics:\n\nAccuracy: \\[ACC=\\dfrac{TP1+TP2+TP3+\\cdots}{Total}\\]\nMacro-average precision: \\[PRE_\\text{macro}=\\dfrac{PRE_1+PRE_2+\\cdots+PRE_n}{n}\\]\nMicro-average precision (should not use): \\[PRE_\\text{micro}=\\dfrac{TP1+TP2+TP3+\\cdots}{TP1+TP2+TP3+\\cdots+FP1+FP2+FP3+\\cdots}\\]"
  },
  {
    "objectID": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html#classification-performance",
    "href": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html#classification-performance",
    "title": "6 Model Selection and Model Assessment",
    "section": "",
    "text": "Given \\(\\hat y=h(\\va x;\\va\\theta)\\) and \\(D=\\qty{\\va x^{(i)}, y^{(i)}}_{i=1}^N\\), \\(\\va x^{(i)}\\in\\R^d\\) and \\(y^{(i)}\\in\\qty{-1,+1}\\).\n\nMisclassification error: \\[\\dfrac{1}{N}\\sum_{i=1}^N\\1[\\hat y^{(i)}\\neq y^{(i)}].\\]\nAccuracy: \\[\\dfrac{1}{N}\\sum_{i=1}^N\\1[\\hat y^{(i)}=y^{(i)}].\\]\nWhat’s wrong with accuracy: It can be misleading when we have disproportional groups. E.g., rare disease.\n\nConfusion Matrix and Classification Metrics:\n\n\n\n\n\nPredicted (\\(-\\))\nPredicted (\\(+\\))\n\n\n\n\nActual (\\(-\\))\nTrue Negative (TN)\nFalse Positive (FP) Type I Error\n\n\nActual (\\(+\\))\nFalse Negative (FN) Type II Error\nTrue Positive (TP)\n\n\n\n\nAccuracy: \\(\\dfrac{TP+TN}{N}\\)\nSensitivity / True Positive Rate (TRR) / Recall: \\(\\dfrac{TP}{TP+FN}\\).\nSpecificity / True Negative Rate (TNR): \\(\\dfrac{TN}{TN+FP}\\).\nFalse Positive Rate (FPR): \\(\\dfrac{FP}{TN+FP}=1-\\text{specificity}\\).\nPrecision / Positive Predictive Value (PPV): \\(\\dfrac{TP}{TP+FP}\\).\n\n\n\n\n\n\n\n\nRemark 1. If we predict everything to be positive, \\(TN=FB=0\\). Then, - Accuracy: \\(\\dfrac{TP}{N}\\). - Sensitivity: \\(1\\). - Specificity: \\(0\\). - False Positive Rate: \\(1\\). - Precision: \\(\\dfrac{TP}{N}\\).\n\n\n\n\n\nComposite Metrics: Trade-offs:\n\nBalanced Accuracy: mean of sensitivity and specificity. \\[\\dfrac{1}{2}\\qty(\\dfrac{TP}{TP+FN}+\\dfrac{TN}{TN+FP})\\]\nF1 Score: harmonic mean of precision and recall \\[2\\dfrac{\\cdot\\text{Precision}\\cdot\\text{Recall}}{\\text{Precision}+\\text{Recall}}\\]\n\nDiscrimination Thresholds: \n\nReceiver Operating Characteristic (ROC) Curve: TPR vs. FPR. \n\nEach point on the ROC curve corresponds to a threshold / a decision boundary.\nEach point represents a differenet trade-off between FPR and TPR.\nProperties of ROC:\n\nSlope is always upward.\nTwo non-intersecting curves means one model dominate the other.\nPerfect prediction vs. random prediction \nROC shows the trade-off between sensitivity and specificity.\nbut still not a very good summary metric: it is not a single number.\n\n\nAread under the ROC Curve (AUROC, ROC-AUC):\n\nCalculated using the trapezoid rule: sklearn.metrics.auc(x, y).\nIntuitive meaning: Given two randomly chosen examples, one positive and one negative, the probability of ranking positive example higher than the negative example.\n\\(AUC=1\\) for perfect prediction, \\(0.5\\) for random prediction.\n\\(AUC&gt;0.9\\): excellent prediction, but consider information leakage.\n\\(AUC\\approx0.8\\): good prediction.\n\\(AUC&lt;0.5\\): something is rong.\n\nPrecision-Recall Curve and AUPRC:\n\nA high AUPRC represents both higher recall and precision.\nROC curves should be used when there are rounghly equal numbers of observations for each class.\nPrecision-Recall curves may be used when there is a moderate to large class imbalance.\n\n\nClassifier Probability Calibration:\n\nWhen a model produce a probability of positive class, is that number actually a meaningful probability?\nFor example, if my model predicts 90% for a set of examples, does that mean 90% of those examples ahve label \\(+1\\)?\n\nMultiple classes metrics:\n\nAccuracy: \\[ACC=\\dfrac{TP1+TP2+TP3+\\cdots}{Total}\\]\nMacro-average precision: \\[PRE_\\text{macro}=\\dfrac{PRE_1+PRE_2+\\cdots+PRE_n}{n}\\]\nMicro-average precision (should not use): \\[PRE_\\text{micro}=\\dfrac{TP1+TP2+TP3+\\cdots}{TP1+TP2+TP3+\\cdots+FP1+FP2+FP3+\\cdots}\\]"
  },
  {
    "objectID": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html#regression-metrics",
    "href": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html#regression-metrics",
    "title": "6 Model Selection and Model Assessment",
    "section": "Regression Metrics",
    "text": "Regression Metrics\nGiven \\(\\hat y=f(\\va x;\\va\\theta)\\) and \\(D=\\qty{\\va x^{(i)}, y^{(i)}}_{i=1}^N\\), \\(x^{(i)}\\in\\R^d\\) and \\(y^{(i)}\\in\\R\\).\n\nMean Squared Error (MSE): \\[MSE=\\dfrac{1}{N}\\sum_{i=1}^N\\qty(\\hat y^{(i)}-y^{(i)})^2\\]\nRoot Mean Squared Error (RMSE): \\[RMSE=\\sqrt{MSE}=\\sqrt{\\dfrac{1}{N}\\sum_{i=1}^N\\qty(\\hat y^{(i)}-y^{(i)})^2}\\]\nMean Absolute Error (MAE): \\[MAE=\\dfrac{1}{N}\\sum_{i=1}^N\\abs{\\hat y^{(i)}-y^{(i)}}\\]\nMean Bias Error (MBE): \\[MBE=\\dfrac{1}{N}\\sum_{i=1}^N\\qty(y^{(i)}-y^(i))\\]"
  },
  {
    "objectID": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html#model-assessment-process",
    "href": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html#model-assessment-process",
    "title": "6 Model Selection and Model Assessment",
    "section": "Model Assessment Process",
    "text": "Model Assessment Process\n\nHouldout: forming a test set:\n\nHold out some data (i.e., test data) that are not used for training the modedl.\nProxy for “everything you might see.”\nProcedure:\n\nOn training data, we train our model.\nOn test data, we use the model to make predictions.\nWe compare the predictions with the true labels to get the performance.\n\nProblem of holdout:\n\nToo few data for traiing: unable to properly learn from the data\nToo few for testing: bad approximation of the true error\nRule of thumb: enough test samples to form a reasonable estimate.\nCommon split size is \\(70\\%-30\\%\\) or \\(80\\%-20\\%\\).\n\n\nQuestion: what to do if we don’t have enough data? \\(K\\)-Fold Cross Validation (CV):\n\nUse all the data to train / test (but don’t use all data to train at the same time).\nProcesudre:\n\nSplit the data into \\(K\\) parts or “folds.”\nTrain on all but the \\(k\\)-th part and test / validate on the \\(k\\)-th part.\nRepeat for each \\(k=1,2,\\ldots,K\\).\nReport average performance over \\(K\\) experiments.\n\nCommon values of \\(K\\):\n\n\\(K=2\\): two-fold cross validation.\n\\(K=5\\) or \\(K=10\\): 5-fold or 10-fold cross validation – common choices.\n\\(K=N\\): leave-one-out cross validation (LOOCV).\n\nThe choice of \\(K\\) is based on how much data we have.\n\nMonte-Carlo Cross Validation:\n\nAKA repeated random sub-sampling validation.\nProcedure:\n\nRandomly select (without replacement) some fraction of the data to form training set.\nAssign rest to test set.\nRepeat multiple times with different partitions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nHow many possible partitions do we see?\nHow many models do we need to train?\nWhat is the final model?\nHow to get error estimates?\n\n\n\n\nHoldout\n\\(1\\)\n\\(1\\)\nThe model\nBootstrapping\n\n\n\\(K\\)-Fold CV\n\\(K\\)\n\\(K\\)\nAverage of \\(K\\) models\nAverage of \\(K\\) error estimates\n\n\nMonte-Carlo CV\nas many as possible\nas many as possible\nAverage of all models\nAverage of all error estimates\n\n\n\n\nBootstrapping to find Confidence Intervals:\n\nSample with replacement \\(N\\) times.\nCalculate performance metric on each boostrap iterate.\nThe 95% confidence interval is the \\(2.5\\)-th and \\(97.5\\)-th percentile of the boostrap distribution."
  },
  {
    "objectID": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html#model-selection",
    "href": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html#model-selection",
    "title": "6 Model Selection and Model Assessment",
    "section": "Model Selection",
    "text": "Model Selection\nGoal: selecting the proper level of flexibility for a model (e.g., regularization strength in logistic regression)\n\nSelect hyperparameters of the model: “meta-optimization”\n\nRegularization strength\nRegularization type\nLoss function\nPolynomial degree\nKernal type\n\nDifferent from model parameters:\n\nFeature coefficients\n\nSimple, popular solution: \\(K\\)-fold CV for Hyperparameter Selection\nAssessment + Selection Guidelines:\n\nDo not use same samples to choose optimal hyperparameters and to estimate test / generalization error.\nChoice of methodology will depend on your problem and dataset.\n\nThree-way split:\n\nTraining set: to train the model\nValidation set: to select hyperparameters\nTest set: to estimate generalization error\n\nHoldout + \\(K\\)-Fold CV:\n\nHoldout: test dataset to assess the performance.\nTraining set: use \\(K\\)-fold CV to find optimal hyperparameters.\nUse optimal hyperparameters to train on the training data and assess on test.\n\nNested CV:\n\nOuter \\(K\\)-fold loop: assess the performance.\nInner \\(K\\)-fold loop: choose the optimal hyperparameters.\n\n\n\n\n\n\n\n\nTip 1: An Example in sklearn\n\n\n\n\n\nMost commonly used function/classes: train_test_split, KFold, and StratifiedKFold.\n# generate an 80-10-10 train-validation-test three-way split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\nX_test, X_val, y_test, y_val = train_test_split(X_train, y_train, test_size=1/9)\n\n# generate a 4-fold CV in a for-loop\n# stratified k-foldL maintains label proportions\nfrom sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=5)\nfor train_idx, test_idx in skf.split(X, y):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n\n\n\n\nHyperparameter Search Space:\n\nGrid Search CV: exhaustive search for the best hyperparameter values using cross validation. sklearn.model_selection.GridSearchCV.\nRandomized Search CV: random search over hyperparameters. sklearn.model_selection.RandomizedSearchCV."
  },
  {
    "objectID": "notes/cs334/07-Feature-Selection-and-Kernels/07-Feature-Selection-and-Kernels.html",
    "href": "notes/cs334/07-Feature-Selection-and-Kernels/07-Feature-Selection-and-Kernels.html",
    "title": "7 Feature Selection and Kernels",
    "section": "",
    "text": "Types of Variables:\n\nNumerical: Discrete / Continuous.\nCategorical\nOrdinal: categocial with order. E.g., Temperature: Low, Medium, High.\n\nDiscretization of Numerical Features (Quantization / Bining)\n\nNumerical \\(\\to\\) Categorical\n\nE.g., age: \\([18,25], (25, 45], (45, 65], (65, 85], &gt;85\\) or binary: \\(\\leq 65, &gt;65\\)\n\nWhy? Allow a linear model to learn nonlinear relationships\n\nMissing data:\n\nDepend on how missing they are:\n\nDrop examples with missing values\nDrop features with missing values\n\nImputation approaches:\n\nUnivariate imputation\nMultivariate imputation\nNearest neighbor imputation\nMissing indicators\n\n\nTime Series Data:\n\nSummary statistics:\n\nmean, std\nmin, median, max, Q1, Q3\ncumulative sum\ncount\ncount above/below threshold\n\nTrend:\n\nlinear slopes\npiece-wise slopes\n\nPeriodic trends: fast fourier transform"
  },
  {
    "objectID": "notes/cs334/07-Feature-Selection-and-Kernels/07-Feature-Selection-and-Kernels.html#feature-engineering",
    "href": "notes/cs334/07-Feature-Selection-and-Kernels/07-Feature-Selection-and-Kernels.html#feature-engineering",
    "title": "7 Feature Selection and Kernels",
    "section": "",
    "text": "Types of Variables:\n\nNumerical: Discrete / Continuous.\nCategorical\nOrdinal: categocial with order. E.g., Temperature: Low, Medium, High.\n\nDiscretization of Numerical Features (Quantization / Bining)\n\nNumerical \\(\\to\\) Categorical\n\nE.g., age: \\([18,25], (25, 45], (45, 65], (65, 85], &gt;85\\) or binary: \\(\\leq 65, &gt;65\\)\n\nWhy? Allow a linear model to learn nonlinear relationships\n\nMissing data:\n\nDepend on how missing they are:\n\nDrop examples with missing values\nDrop features with missing values\n\nImputation approaches:\n\nUnivariate imputation\nMultivariate imputation\nNearest neighbor imputation\nMissing indicators\n\n\nTime Series Data:\n\nSummary statistics:\n\nmean, std\nmin, median, max, Q1, Q3\ncumulative sum\ncount\ncount above/below threshold\n\nTrend:\n\nlinear slopes\npiece-wise slopes\n\nPeriodic trends: fast fourier transform"
  },
  {
    "objectID": "notes/cs334/07-Feature-Selection-and-Kernels/07-Feature-Selection-and-Kernels.html#feature-selection",
    "href": "notes/cs334/07-Feature-Selection-and-Kernels/07-Feature-Selection-and-Kernels.html#feature-selection",
    "title": "7 Feature Selection and Kernels",
    "section": "Feature Selection",
    "text": "Feature Selection\n\nMotivation:\n\nWhen having few examples, but a learge number of features (\\(d\\gg N\\)), it becomes very easy to overfit the model.\nWe need to remove uninformative features.\n\nMethods:\n\nFilter method: before model fitting\nWrapper method: after model fitting\nEmbedded method: during model fitting\n\nFilter Method: preprocessing*\n\nRank individual features according to some statistical measure\nFilter out features that fall below a certain threshold\n\nE.g., sample variance, Person’s correlation coerfficient between feature and label, Mutal information, \\(\\chi^2\\)-statistic for categorical/ordinal features.\n\n\nWrapper Method: Search\n\nView feature selection as a model selection problem\nWhich feature to use: hyperparameter\n\nE.g,b Brute-force, exhausitve search: If we have \\(d\\) features, there are \\(2^d-1\\) possible combinations. Computatianlly infeasible.\nGreedy search: Forward selection, backward elimination.\n\nForward selection:\n\n\n\\begin{algorithm} \\caption{Forward Selection} \\begin{algorithmic} \\State initialize $F$ to the set of all features \\State initialize $S$ to the empty set $\\qquad$\\Comment{Start with $0$ features} \\For{$i=1:d-1$} \\For{ each feature $f\\in F$} \\State frain model using $(S, f)$ \\State evaluate model \\EndFor \\State Select best set $\\qty{S, f^*}$. $S=S+F^*$, and $F=F-f^*$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\nThis algorithm has complexity \\(\\sim\\bigO(d^2)\\).\n\nBackward elimination:\n\n\n\\begin{algorithm} \\caption{Forward elimination} \\begin{algorithmic} \\State initialize $F$ to the set of all features \\For{$i=1:d-1$} \\State $S=$ set of all subsets of $F$ of size $d-i$. $\\qquad$\\Comment{Start with $d$ features} \\For{ each subset $s\\in S$} \\State train model using $s$ \\State evaluate model \\EndFor \\State Select best subset $s^*$, and $F=s^*$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\nThis algorithm has complexity \\(\\sim\\bigO(d^2)\\). However, forward selection is less computationally expensive because we train more smaller modedls.\n\n\nEmbedded Method: Regularization\n\nIncorporate feature selection as part of the model fitting process\nUse \\(L_1\\) (LASSO) regularization\n\n\n\n\n\n\n\n\n\n\n\n\nFilter\nWrapper\nEmbedded\n\n\n\n\nComputational Cost\nFast. Only run once scalable to high dimension\nSlow\nBetween filter and wrapper\n\n\nModel/Algorithm Specific\nGeneric, agnostic to models and algorithms\nSpecific, considers model performance\nOnly applies to specific model / algorithms\n\n\nOverfitting\nUnlikely\nHigher risk of overfitting\nRegualrization controls overfitting\n\n\n\n\nIn practice, we use combinations of these methods."
  },
  {
    "objectID": "notes/cs334/07-Feature-Selection-and-Kernels/07-Feature-Selection-and-Kernels.html#fitting-nonlinear-functions-kernel-methods",
    "href": "notes/cs334/07-Feature-Selection-and-Kernels/07-Feature-Selection-and-Kernels.html#fitting-nonlinear-functions-kernel-methods",
    "title": "7 Feature Selection and Kernels",
    "section": "Fitting Nonlinear Functions – Kernel Methods",
    "text": "Fitting Nonlinear Functions – Kernel Methods\n\nQuestion: How can we use linear models to learn nonlinear trends?\n\nMap to higher order dimensions: e.g., \\(\\mqty[1&x&x^2&\\cdots&x^m]\\)\n\nFor example, classification:\n\n\n\n\n\n\nFigure 1: Higher Dimensional Classification\n\n\n\n\nLet \\(\\va x=\\mqty[x_1&x_2]^\\top\\) and \\(\\phi(\\va x)=\\mqty[1&x_1&x_2&x_1x_2&x_1^2&x_2^2]^\\top\\).\nFind a linear classifier such that \\(\\va\\theta\\cdot\\phi(\\va x)=0\\) that perfectlly separates the two classes.\nEquation of a circle: \\((x-h)^2+(y-k)^2=r^2\\).\nIn this example, \\((x_1-2)+(x_2-2)^2=1\\). So, \\(7-4x_1-4x_2+x_1^2+x_2^2=0\\).\nTherfore, \\(\\phi(\\va x)=\\mqty[1&x_1&x_2&x_1x_2&x_1^2&x_2^2]^\\top\\) and \\(\\va\\theta=\\mqty[7&-4&-4&0&1&1]^\\top\\).\nBut, we don’t want to use this method. Why?\n\nBecause dimension will blow-up. Suppose \\(\\va x\\in\\R^d\\) and \\(\\phi(\\va x)\\in\\R^p\\). Then, \\[p=\\text{number of first and second order terms}=(d+1)+\\dfrac{d(d+1)}{2}\\sim\\bigO(d^2).\\]\nWe have \\(p\\gg d\\). It’s also likely that \\(p\\gg N\\).\nIn perceptron algorithm, our update complexity will then be \\(\\sim\\bigO(p)=\\bigO(d^2)\\), which is inefficient.\n\n\nKey Observation:\n\nIf we initialize \\(\\va\\theta^{(k)}=\\va 0\\), then \\(\\va\\theta^{(k)}\\) is always in the span of feature vectors and can be expressed to linear combination of feature vectors: \\[\\va\\theta^{(k)}=\\sum_{i=1}^N\\alpha_i\\va x^{(i)}\\quad\\text{for some }\\alpha_1,\\alpha_2,\\dots,\\alpha_N.\\]\nThen, we can rewrite the classifier to get \\[\\begin{aligned}h(\\va x;\\va\\theta)&=\\operatorname{sign}(\\va\\theta\\cdot\\va x)\\\\&=\\operatorname{sign}\\qty(\\qty(\\sum_{i=1}^N\\alpha_i\\va x^{(i)})\\cdot\\va x)\\\\&=\\operatorname{sign}\\qty(\\sum_{i=1}^N\\alpha_i\\va x^{(i)}\\cdot\\va x).\\end{aligned}\\]\nEven if we are in higher dimensional feature space: \\[h(\\phi(\\va x);\\va\\theta)=\\operatorname{sign}\\qty(\\sum_{i=1}^N\\alpha_i\\qty(\\phi(\\va x^{(i)})\\cdot\\phi(\\va x))).\\]\nClassifiers now is written in \\(\\alpha_i\\) (\\(N\\)-dimensional) space, not in \\(\\va x\\) (\\(d\\)-dimensional) space.\nThen, the update rule is \\[\\alpha_i^{(k+1)}=\\alpha_i^{(k)}+y^{(i)}.\\]\n\n\n\n\n\\begin{algorithm} \\caption{New Perceptron} \\begin{algorithmic} \\State $\\va\\alpha^{(0)}=\\va 0\\in\\R^N$, $k=0$ \\While{not converged} \\For{$i=1,\\dots,N$} \\If{$y^{(i)}\\qty(\\sum_{j=1}^N\\alpha_j\\phi(\\va x^{(i)})\\cdot\\phi(\\va x^{(j)}))\\leq 0$} \\State $\\alpha_{i}^{(k+1)}=\\alpha_i^{(k)}+y^{(i)}$ \\State $\\alpha_j^{(k+1)}=\\alpha_j^{(k)}$ when $j\\neq i$ \\State $k++$ \\EndIf \\EndFor \\EndWhile \\end{algorithmic} \\end{algorithm}\n\n\n\n\\(\\phi(\\va x^{(i)})\\cdot\\phi(\\va x^{(j)})\\) can be pre-computed. Complexity is \\(\\bigO(N^2p)\\)\nThe update complexity \\(\\bigO(1)\\).\nHow can we speed up dot product \\(\\phi(\\va x)\\cdot\\phi(\\va x')\\)? We can calculate the dot producrt \\(\\phi(\\va x)\\cdot\\phi(\\va x')\\) withtout calculating \\(\\phi(\\va x)\\) and \\(\\phi(\\va x')\\).\n\n\n\n\n\n\n\n\nExample 1 Suppose \\(\\phi(\\va x)=\\mqty[x_1^2&x_2^2&\\sqrt{2}x_1x_2]^\\top\\). Then, \\[\n\\begin{aligned}\n\\phi(\\va u)\\cdot\\phi(\\va v)&=\\mqty[u_1^2&u_2^2&\\sqrt{2}u_1u_2]^\\top\\cdot\\mqty[v_1^2&v_2^2&\\sqrt{2}v_1v_2]^\\top\\\\\n&=u_1^2v_1^2+u_2v_2^2+2u_1u_2v_1v_2\\\\\n&=(u_1v_1+u_2v_2)^2\\\\\n&=\\qty(\\va u\\cdot\\va v)^2.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1 (Kernel Function) Kernel function is an implicit feature mapping that allows us to compute the dot product in the feature space without explicitly computing the feature vectors. \\[K:\\underbrace{\\R^d\\times\\R^d}_\\text{feature vectors}\\to\\R\\]\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1 (Common Kernels:)  \n\nLinear Kernel: \\[K\\qty(\\va x^{(i)},\\va x^{(j)})=\\va x^{(i)}\\cdot\\va x^{(j)}.\\]\nPolynomial Kernel: \\[K\\qty(\\va x^{(i)},\\va x^{(j)})=\\qty(\\va x^{(i)}\\cdot\\va x^{(j)}+1)^d.\\]\nRadial Basis Function (RBF) Kernel: infinite-dimensional feature space \\[\\begin{aligned}K_\\text{RBF}(\\va u,\\va v)&=\\exp\\qty(-\\gamma\\norm{\\va u-\\va v}^2)\\\\&=C\\sum_{n=0}^\\infty\\dfrac{K_{\\operatorname{poly}(n)}(\\va u,\\va v)}{n!}\\end{aligned}\\]\n\n\n\n\n\n\n\n\\begin{algorithm} \\caption{Kernelized Perceptron} \\begin{algorithmic} \\State $\\alpha_j^{(0)}=0$, $k=0$ \\While{not converged} \\For{$i=1,\\dots,N$} \\If{$y^{(i)}\\qty(\\sum_{j=1}^N\\alpha_j^{(k)}K(\\va x^{(j)}, \\va x^{(i)}))\\leq 0$} \\State $\\alpha_{i}^{(k+1)}=\\alpha_i^{(k)}+y^{(i)}$ \\State $\\alpha_j^{(k+1)}=\\alpha_j^{(k)}$ when $j\\neq i$ \\State $k++$ \\EndIf \\EndFor \\EndWhile \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "notes/cs334/08-Decision-Trees-and-Random-Forest/08-Decision-Trees-and-Random-Forest.html",
    "href": "notes/cs334/08-Decision-Trees-and-Random-Forest/08-Decision-Trees-and-Random-Forest.html",
    "title": "8 Decision Trees and Random Forest",
    "section": "",
    "text": "Interpretability: Given a linear classifier dinfed by \\(\\va\\theta=[\\theta_1,\\theta_2,\\dots,\\theta_d]\\). How can we interpret the meaning of each parameter/coefficient? \\[\\norm{\\theta_i}\\text{: how this feature contributes to the decision.}\\] To fit nonlinear model, we can use kernels. However, with kernels, the fitted parameter \\(\\va\\theta\\) becomes a blackbox, and we lose interpretability (especially when we use RBF kernels, \\(\\va\\theta\\in\\R^\\infty\\)).\nA different approach: Decision Tree \\[f:\\mathcal{X}\\to\\mathcal{Y}\\]\n\nBoth features (\\(\\mathcal{X}\\)) and labels (\\(\\mathcal{Y}\\)) can be a continuous, descrete, or binary value.\n\n\n\n\n\n\n\n\n\nExample 1 (Forming a Decision Tree) Suppose \\(\\va x\\in\\qty{0,1}^2\\)\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y\\)\n\n\n\n\n1\n1\n0\n\n\n1\n0\n1\n\n\n0\n1\n1\n\n\n0\n0\n0\n\n\n\n\n\n\n\n\n\nFigure 1: Decision Tree\n\n\n\n\n\n\n\n\nWe can use a boolean function of input feature to represent a decision tree.\n\n\n\n\n\n\n\n\nRemark 1. \n\nAs the number of nodes increases, the hypothesis space grows.\nTrivial solution: each examples has its own leaf node. If \\(\\va x\\in\\qty{0,1}^d\\), then we have \\(2^d\\) leaf nodes.\nProblem: overffing, unlikely to generalize to unseen examples.\n\n\n\n\n\n\nGoal: Find the smallest tree that performs well on training data.\n\nHowever, finding the optimal partition of the data is NP-complete (hard).\nInstead, we can use a greedy appraoch:\n\nStart with empty tree.\nFind best feature to split on.\nRecursively build branches into subtree.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2 (How to Find the Best Split) We want to predict whether ot not a flight will get delayed: \\[\\text{training data: }\\begin{cases}29\\text{ positive}\\\\35\\text{ negative}\\end{cases}\\implies[29^+, 35^-].\\] Suppose \\(\\va x=[x_1,x_2]\\) are two binary features:\n\n\n\n\n\n\nFigure 2: First Slipt\n\n\n\nSplitting by snow is better because it produces more certain labels. But, how do we measure uncertainty?\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1 (Shannon’s Entropy) Let \\(D_N\\) be the training data, \\(y\\in\\qty{-1,+1}\\) be the binary outcome/label.\n\n\\(\\P_\\oplus\\): fraction of positive examples\n\\(\\P_\\ominus\\): fraction of negative examples \\[\\text{Entropy of }D_N=-\\qty\\Big(\\P_\\oplus\\log_2\\P_\\oplus+\\P_\\ominus\\log_2\\P_\\ominus)\\]\nThe definition uses \\(\\log_2\\) because entropy is measured in bits.\nIt measures the expected number of bits needed to encode a randomly drawn value of \\(y\\).\n\n\n\n\n\n\n\nFigure 3: Plot of Entropy\n\n\n\n\nMore generally, for categorical outcome \\(y\\in\\qty{y_1,y_2,\\dots,y_k}\\), \\[\n\\begin{aligned}\nH(y)&=-\\qty\\Big(\\P(Y=y_1)\\log_2\\P(Y=y_1)+\\P(Y=y_2)\\log_2\\P(Y=y_2)+\\cdots+\\P(Y=y_k)\\log_2\\P(Y=y_k))\\\\\n&=-\\sum_{i=1}^k\\P(Y=y_i)\\log_2\\P(Y=y_i)\n\\end{aligned}\n\\]\nNote: entropy is usually positive as \\(\\log_2\\P(Y=y_i)\\) is negative.\n\n\n\n\n\n\nEntropy and Peakyness:\n\nImagine rolling a die and plotting the empirical distribution: \n\nOn the left: high entropy, more uncertain about the label, less peaky distribution\nOn the right: low entropy, less uncertain about the label, more peaky distribution\n\n\n\n\n\n\n\n\n\n\nDefinition 2 (Conditional Entropy) \\[H(Y\\mid X=x)=-\\qty(\\sum_{i=1}^k\\P(Y=y_i\\mid X=x)\\log_2\\P(Y=y_i\\mid X=x))\\] \\[H(Y\\mid X)=\\sum_{x\\in X}\\P(X=x)\\cdot H(Y\\mid X=x)\\]\n\n\\(H(Y\\mid X)\\) shows the average surprise of \\(Y\\) when \\(X\\) is known.\n\n\n\n\n\n\n\n\n\n\n\n\nRemark 2. \n\nWhen does \\(H(Y\\mid X)=0\\)? When \\(Y\\) is completely determined by \\(X\\).\nWhen does \\(H(Y\\mid X)=H(Y)\\)? When \\(Y\\independ X\\).\nWe can use conditional entropy to measure the quality of a split:\n\nIdea: if knowing \\(x_1\\) reduces uncertainty more than knowing \\(x_2\\), we should split by \\(x_1\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3 (Information Gain (IG) / Mutual Information) \\[\nIG(X;Y)=H(Y)-H(Y\\mid X),\n\\]\n\nwhere \\(H(Y)\\) is the entropy of parent node, and \\(H(Y\\mid X)\\) is the average entropy of the children note.\n\\(IG\\) measures the amount of information we learn about \\(Y\\) by knowing the value of \\(C\\) (and vice versa \\(\\implies\\) symmetric).\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3 (Back to the Example) We want to predict whether ot not a flight will get delayed: \\[\\text{training data: }\\begin{cases}29\\text{ positive}\\\\35\\text{ negative}\\end{cases}\\implies[29^+, 35^-].\\] Suppose \\(\\va x=[x_1,x_2]\\) are two binary features:\n\n\n\n\n\n\nFigure 4: First Slipt\n\n\n\n\nCalculate entropy of \\(y\\): \\[H(y)=-\\qty(\\dfrac{29}{64}\\log_2\\dfrac{29}{64}+\\dfrac{35}{64}\\log_2\\dfrac{35}{64})\\approx0.9937\\]\nCalcualte the conditional entropy for each feature: \\[\n\\begin{aligned}\nH(y\\mid x_1)&=\\dfrac{26}{64}\\qty(-\\dfrac{21}{26}\\log_2\\dfrac{21}{26}-\\dfrac{5}{26}\\log_2\\dfrac{5}{26})+\\dfrac{38}{64}\\qty(-\\dfrac{8}{38}\\log_2\\dfrac{8}{38}-\\dfrac{30}{38}\\log_2\\dfrac{30}{38})\\\\\n&=0.7278\\\\\\\\\nH(y\\mid x_2)&=\\dfrac{45}{64}\\qty(-\\dfrac{18}{45}\\log_2\\dfrac{18}{45}-\\dfrac{27}{45}\\log_2\\dfrac{27}{45})+\\dfrac{19}{64}\\qty(-\\dfrac{11}{19}\\log_2\\dfrac{11}{19}-\\dfrac{8}{19}\\log_2\\dfrac{8}{19})\\\\\n&=0.9742.\n\\end{aligned}\n\\]\nCalcualte the information gain: \\[\n\\begin{aligned}\nIG(x_1;y)&=0.9937-0.7278=0.2659\\\\\nIG(x_2;y)&=0.9937-0.9742=0.0195.\n\\end{aligned}\n\\] So, \\(IG(x_1;y)&gt;IG(x_2;y)\\), \\(x_1\\) is a better split.\n\n\n\n\n\n\nAnother Measure of Uncertainty: Gini Index and Gini Gain: \\[\n\\begin{aligned}\n\\text{Gini}(Y)&=\\sum_{k=1}^k\\P(Y=y_k)\\qty(1-\\P(Y=y_k))=1-\\sum_{k=1}^k\\P(Y=y_k)^2\\\\\n\\text{GiniGain}(X;Y)&=\\text{Gini}(Y)-\\sum_{x\\in X}\\P(X=x)\\text{Gini}(Y\\mid X=x).\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nRemark 3. In practice, using \\(IG\\) or \\(\\text{GiniGain}\\) may lead to different results, but it is unclear how different it can be.\n\n\n\n\n\n\nFigure 5: Gini Index and Entropy Plot\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\arg\\max_{j=1,\\dots,d}IG(x_j;y)&=\\arg\\max_j H(y)-H(y\\mid x_j)\\\\\n&=\\arg\\min_j H(y\\mid x_j).\n\\end{aligned}\n\\]\n\nWhen to stop growing a tree? (Stopping Criteria)\n\nWhen all record have the same label (assume no noise).\nIf all record have identical features (no further splits possible).\n\n\n\n\n\n\n\n\n\nRemark 4. We should not stop when all stributtes have \\(0\\ IG\\). See Example 1.\n\n\n\n\n\n\n\\begin{algorithm} \\caption{Building Decision Tree} \\begin{algorithmic} \\Procedure{BuildTree}{$DS$} \\IF{$y^{(i)}==y$ for all examples in $DS$} \\Return{$y$} \\ElseIf{$x^{(i)}==x$ for all examples in $DS$} \\Return{majority label} \\Else \\State $x_s=\\argmin_x H(y\\mid x)$ \\For{each value $v$ of $x_s$} \\State $DS_y=\\qty{\\text{examples in }DS\\text{ where }x_s=v}$ \\State BuildTree($DS_y$)$\\qquad$\\Comment{recursive function} \\EndFor \\EndIf \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\nHow do we avoid overfitting? We want simpler trees.\n\nSet a maximum depth\nMeasure performance on validation data. If growing tree results in worse performance, stop.\nPost-prune: grow entire/full tree and then greedily remove nodes that affect validation error the least."
  },
  {
    "objectID": "notes/cs334/08-Decision-Trees-and-Random-Forest/08-Decision-Trees-and-Random-Forest.html#decision-tree",
    "href": "notes/cs334/08-Decision-Trees-and-Random-Forest/08-Decision-Trees-and-Random-Forest.html#decision-tree",
    "title": "8 Decision Trees and Random Forest",
    "section": "",
    "text": "Interpretability: Given a linear classifier dinfed by \\(\\va\\theta=[\\theta_1,\\theta_2,\\dots,\\theta_d]\\). How can we interpret the meaning of each parameter/coefficient? \\[\\norm{\\theta_i}\\text{: how this feature contributes to the decision.}\\] To fit nonlinear model, we can use kernels. However, with kernels, the fitted parameter \\(\\va\\theta\\) becomes a blackbox, and we lose interpretability (especially when we use RBF kernels, \\(\\va\\theta\\in\\R^\\infty\\)).\nA different approach: Decision Tree \\[f:\\mathcal{X}\\to\\mathcal{Y}\\]\n\nBoth features (\\(\\mathcal{X}\\)) and labels (\\(\\mathcal{Y}\\)) can be a continuous, descrete, or binary value.\n\n\n\n\n\n\n\n\n\nExample 1 (Forming a Decision Tree) Suppose \\(\\va x\\in\\qty{0,1}^2\\)\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y\\)\n\n\n\n\n1\n1\n0\n\n\n1\n0\n1\n\n\n0\n1\n1\n\n\n0\n0\n0\n\n\n\n\n\n\n\n\n\nFigure 1: Decision Tree\n\n\n\n\n\n\n\n\nWe can use a boolean function of input feature to represent a decision tree.\n\n\n\n\n\n\n\n\nRemark 1. \n\nAs the number of nodes increases, the hypothesis space grows.\nTrivial solution: each examples has its own leaf node. If \\(\\va x\\in\\qty{0,1}^d\\), then we have \\(2^d\\) leaf nodes.\nProblem: overffing, unlikely to generalize to unseen examples.\n\n\n\n\n\n\nGoal: Find the smallest tree that performs well on training data.\n\nHowever, finding the optimal partition of the data is NP-complete (hard).\nInstead, we can use a greedy appraoch:\n\nStart with empty tree.\nFind best feature to split on.\nRecursively build branches into subtree.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2 (How to Find the Best Split) We want to predict whether ot not a flight will get delayed: \\[\\text{training data: }\\begin{cases}29\\text{ positive}\\\\35\\text{ negative}\\end{cases}\\implies[29^+, 35^-].\\] Suppose \\(\\va x=[x_1,x_2]\\) are two binary features:\n\n\n\n\n\n\nFigure 2: First Slipt\n\n\n\nSplitting by snow is better because it produces more certain labels. But, how do we measure uncertainty?\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1 (Shannon’s Entropy) Let \\(D_N\\) be the training data, \\(y\\in\\qty{-1,+1}\\) be the binary outcome/label.\n\n\\(\\P_\\oplus\\): fraction of positive examples\n\\(\\P_\\ominus\\): fraction of negative examples \\[\\text{Entropy of }D_N=-\\qty\\Big(\\P_\\oplus\\log_2\\P_\\oplus+\\P_\\ominus\\log_2\\P_\\ominus)\\]\nThe definition uses \\(\\log_2\\) because entropy is measured in bits.\nIt measures the expected number of bits needed to encode a randomly drawn value of \\(y\\).\n\n\n\n\n\n\n\nFigure 3: Plot of Entropy\n\n\n\n\nMore generally, for categorical outcome \\(y\\in\\qty{y_1,y_2,\\dots,y_k}\\), \\[\n\\begin{aligned}\nH(y)&=-\\qty\\Big(\\P(Y=y_1)\\log_2\\P(Y=y_1)+\\P(Y=y_2)\\log_2\\P(Y=y_2)+\\cdots+\\P(Y=y_k)\\log_2\\P(Y=y_k))\\\\\n&=-\\sum_{i=1}^k\\P(Y=y_i)\\log_2\\P(Y=y_i)\n\\end{aligned}\n\\]\nNote: entropy is usually positive as \\(\\log_2\\P(Y=y_i)\\) is negative.\n\n\n\n\n\n\nEntropy and Peakyness:\n\nImagine rolling a die and plotting the empirical distribution: \n\nOn the left: high entropy, more uncertain about the label, less peaky distribution\nOn the right: low entropy, less uncertain about the label, more peaky distribution\n\n\n\n\n\n\n\n\n\n\nDefinition 2 (Conditional Entropy) \\[H(Y\\mid X=x)=-\\qty(\\sum_{i=1}^k\\P(Y=y_i\\mid X=x)\\log_2\\P(Y=y_i\\mid X=x))\\] \\[H(Y\\mid X)=\\sum_{x\\in X}\\P(X=x)\\cdot H(Y\\mid X=x)\\]\n\n\\(H(Y\\mid X)\\) shows the average surprise of \\(Y\\) when \\(X\\) is known.\n\n\n\n\n\n\n\n\n\n\n\n\nRemark 2. \n\nWhen does \\(H(Y\\mid X)=0\\)? When \\(Y\\) is completely determined by \\(X\\).\nWhen does \\(H(Y\\mid X)=H(Y)\\)? When \\(Y\\independ X\\).\nWe can use conditional entropy to measure the quality of a split:\n\nIdea: if knowing \\(x_1\\) reduces uncertainty more than knowing \\(x_2\\), we should split by \\(x_1\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3 (Information Gain (IG) / Mutual Information) \\[\nIG(X;Y)=H(Y)-H(Y\\mid X),\n\\]\n\nwhere \\(H(Y)\\) is the entropy of parent node, and \\(H(Y\\mid X)\\) is the average entropy of the children note.\n\\(IG\\) measures the amount of information we learn about \\(Y\\) by knowing the value of \\(C\\) (and vice versa \\(\\implies\\) symmetric).\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3 (Back to the Example) We want to predict whether ot not a flight will get delayed: \\[\\text{training data: }\\begin{cases}29\\text{ positive}\\\\35\\text{ negative}\\end{cases}\\implies[29^+, 35^-].\\] Suppose \\(\\va x=[x_1,x_2]\\) are two binary features:\n\n\n\n\n\n\nFigure 4: First Slipt\n\n\n\n\nCalculate entropy of \\(y\\): \\[H(y)=-\\qty(\\dfrac{29}{64}\\log_2\\dfrac{29}{64}+\\dfrac{35}{64}\\log_2\\dfrac{35}{64})\\approx0.9937\\]\nCalcualte the conditional entropy for each feature: \\[\n\\begin{aligned}\nH(y\\mid x_1)&=\\dfrac{26}{64}\\qty(-\\dfrac{21}{26}\\log_2\\dfrac{21}{26}-\\dfrac{5}{26}\\log_2\\dfrac{5}{26})+\\dfrac{38}{64}\\qty(-\\dfrac{8}{38}\\log_2\\dfrac{8}{38}-\\dfrac{30}{38}\\log_2\\dfrac{30}{38})\\\\\n&=0.7278\\\\\\\\\nH(y\\mid x_2)&=\\dfrac{45}{64}\\qty(-\\dfrac{18}{45}\\log_2\\dfrac{18}{45}-\\dfrac{27}{45}\\log_2\\dfrac{27}{45})+\\dfrac{19}{64}\\qty(-\\dfrac{11}{19}\\log_2\\dfrac{11}{19}-\\dfrac{8}{19}\\log_2\\dfrac{8}{19})\\\\\n&=0.9742.\n\\end{aligned}\n\\]\nCalcualte the information gain: \\[\n\\begin{aligned}\nIG(x_1;y)&=0.9937-0.7278=0.2659\\\\\nIG(x_2;y)&=0.9937-0.9742=0.0195.\n\\end{aligned}\n\\] So, \\(IG(x_1;y)&gt;IG(x_2;y)\\), \\(x_1\\) is a better split.\n\n\n\n\n\n\nAnother Measure of Uncertainty: Gini Index and Gini Gain: \\[\n\\begin{aligned}\n\\text{Gini}(Y)&=\\sum_{k=1}^k\\P(Y=y_k)\\qty(1-\\P(Y=y_k))=1-\\sum_{k=1}^k\\P(Y=y_k)^2\\\\\n\\text{GiniGain}(X;Y)&=\\text{Gini}(Y)-\\sum_{x\\in X}\\P(X=x)\\text{Gini}(Y\\mid X=x).\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nRemark 3. In practice, using \\(IG\\) or \\(\\text{GiniGain}\\) may lead to different results, but it is unclear how different it can be.\n\n\n\n\n\n\nFigure 5: Gini Index and Entropy Plot\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\arg\\max_{j=1,\\dots,d}IG(x_j;y)&=\\arg\\max_j H(y)-H(y\\mid x_j)\\\\\n&=\\arg\\min_j H(y\\mid x_j).\n\\end{aligned}\n\\]\n\nWhen to stop growing a tree? (Stopping Criteria)\n\nWhen all record have the same label (assume no noise).\nIf all record have identical features (no further splits possible).\n\n\n\n\n\n\n\n\n\nRemark 4. We should not stop when all stributtes have \\(0\\ IG\\). See Example 1.\n\n\n\n\n\n\n\\begin{algorithm} \\caption{Building Decision Tree} \\begin{algorithmic} \\Procedure{BuildTree}{$DS$} \\IF{$y^{(i)}==y$ for all examples in $DS$} \\Return{$y$} \\ElseIf{$x^{(i)}==x$ for all examples in $DS$} \\Return{majority label} \\Else \\State $x_s=\\argmin_x H(y\\mid x)$ \\For{each value $v$ of $x_s$} \\State $DS_y=\\qty{\\text{examples in }DS\\text{ where }x_s=v}$ \\State BuildTree($DS_y$)$\\qquad$\\Comment{recursive function} \\EndFor \\EndIf \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\nHow do we avoid overfitting? We want simpler trees.\n\nSet a maximum depth\nMeasure performance on validation data. If growing tree results in worse performance, stop.\nPost-prune: grow entire/full tree and then greedily remove nodes that affect validation error the least."
  },
  {
    "objectID": "notes/cs334/08-Decision-Trees-and-Random-Forest/08-Decision-Trees-and-Random-Forest.html#forming-a-decision-tree",
    "href": "notes/cs334/08-Decision-Trees-and-Random-Forest/08-Decision-Trees-and-Random-Forest.html#forming-a-decision-tree",
    "title": "8 Decision Trees and Random Forest",
    "section": "",
    "text": "Suppose \\(\\va x\\in\\qty{0,1}^2\\)\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y\\)\n\n\n\n\n1\n1\n0\n\n\n1\n0\n1\n\n\n0\n1\n1\n\n\n0\n0\n0\n\n\n\n\n\n\n\n\n\nFigure 1: Decision Tree"
  },
  {
    "objectID": "notes/cs334/08-Decision-Trees-and-Random-Forest/08-Decision-Trees-and-Random-Forest.html#ensemble-methods-and-random-forest",
    "href": "notes/cs334/08-Decision-Trees-and-Random-Forest/08-Decision-Trees-and-Random-Forest.html#ensemble-methods-and-random-forest",
    "title": "8 Decision Trees and Random Forest",
    "section": "Ensemble Methods and Random Forest",
    "text": "Ensemble Methods and Random Forest\n\nGoal: Descreae variance without increasing bias (recall bias-variance trade-off)\nIdea: Average across multiple models to reduce estimation error. But we only have one single training set, how can we learn multiple models? BAGGING\n\n\nBootstrap Aggregatting (BAGGING)\n\nGeneral procedure:\n\nCreate \\(B\\) bootstrap samples: \\(D_N^{(1)},\\dots,D_N^{(B)}\\)\nTrain decision tree on each \\(D_N^{(b)}\\)\nClassify new examples by majority vote (i.e., mode)\n\n\n\n\n\n\n\n\nFigure 6: BAGGING\n\n\n\n\nWhy does bagging work? (Assume \\(y\\in\\qty{-1,+1}\\))\n\nSuppose we have \\(B\\) independent classifers: \\[\\hat f^{(b)}:\\R^d\\to\\qty{-1,+1},\\] and each \\(\\hat f^{(b)}\\) has a misclassification rate of \\(0.4\\).\n\nThat is, if \\(y^{(i)}=+1\\), then \\(\\P\\qty(\\hat f^{(b)}(\\va x^{(i)})=-1)=0.4\\quad\\forall b=1,\\dots,B\\) and \\(\\forall\\ i,\\dots,d\\)\n\nNow, applay baaged classifier: \\[\\hat f^{(\\text{bag})}(\\va x)=\\arg\\max_{y\\in\\qty{-1,+1}}\\sum_{b=1}^B\\1\\qty(\\hat f^{(b)}(\\va x)=\\hat y)=\\arg\\max\\qty{B_{-1},B_{+1}},\\] where \\(B_{-1}\\) is the number of votes for \\(-1\\), and \\(B_{+1}\\) is the number of votes for \\(+1\\). Then, \\[B_{-1}\\sim\\text{Binomial}(B,0.4).\\] Recall: if \\(X\\sim\\text{Binomial}(n,p)\\), then the probability of getting exactkly \\(k\\) successes in \\(n\\) trails is given by \\[P(X=k)=\\binom{n}{k}p^k(1-p)^{n-k}.\\] Thus, misclassification rate of bagged classifer is \\[\\begin{aligned}\\P\\qty(\\hat f^{(\\text{bag})}(\\va x^{(i)})=-1)&=\\P\\qty(B_{-1}\\geq\\dfrac{B}{2})\\\\&=1-\\P\\qty(B_{-1}&lt;\\dfrac{B}{2})\\\\&=1-\\sum_{k=1}^{\\lfloor B/2\\rfloor}\\binom{B}{k}(0.4)^k(1-0.4)^{B-k}.\\end{aligned}\\] Note: \\[\\lim_{B\\to+\\infty}\\sum_{k=1}^{\\lfloor B/2\\rfloor}\\binom{B}{k}p^k(1-p)^{B-k}=1\\] as long as misclassification rate \\(p&lt;0.5\\). So, as \\(B\\to\\infty\\), misclassification \\(\\to0\\).\n\nReality: Predcition error rarely goes to \\(0\\).\n\nBagging only reduces estimation error (varaince).\nWe don’t ahve independence assumption: classifiers trained on bootstrapped dataset are NOT independent.\n\n\n\n\n\n\n\n\n\nRemark 5. \n\nOn average, each bootstrap contain \\(63.2\\%\\) of original data.\nHow similar are boostrap samples?\n\nProbability of example \\((i)\\) is not selected once: \\(1-\\dfrac{1}{n}\\).\nProbability of example \\((i)\\) is not selected at all: \\(\\qty(1-\\dfrac{1}{n})^n\\).\n\nThen, \\[\\lim_{n\\to\\infty}\\qty(1-\\dfrac{1}{n})^n=\\dfrac{1}{e}\\approx36.8\\%.\\] So, when \\(n\\to\\infty\\), the probability of example \\((i)\\) is not selected at all is \\(36.8\\%\\).\n\n\n\n\n\n\n\nFurther Decorrelate Trees: Random Forest\n\nRandom forest is an ensemble method designed specifically for trees (bagging applies more boardly).\nTwo sources of randomness:\n\nBagging\nRandom feature subsets:at erach node, best split chosen from subset of \\(m\\) features instead of all \\(d\\) features.\n\n\n\n\n\\begin{algorithm} \\caption{Random Forest} \\begin{algorithmic} \\State{\\# Bagging} \\For{$b=1,\\dots, B$} \\State draw bootstrap sample $D_N^{(b)}$ of size $N$ from $D_N$ \\State grow decision tree $DT^{(b)}$$\\qquad$\\Comment{See below code for growing $DT^{(b)}$} \\EndFor \\Return{ensemble $\\qty{DT^{(1)}, \\dots, DT^{(B)}}$} \\State{\\# Subprocedure for growing $DT^{(b)}$; random feature subset} \\While{stopping criteria not met} \\State Recursively repeat following steps for each node of tree: \\State 1. select $m$ features at random from $d$ features \\State 2. pick best feature to split on (using $IG$ or $\\text{Gini}$) \\State 3. split node into childre. \\EndWhile \\State{\\# Another option to do Steps 1 and 2: } \\State{1. compute $IG$ for all $d$ features} \\State{2. randomly pick from top $m$} \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "notes/RealAnalysis.html",
    "href": "notes/RealAnalysis.html",
    "title": "Real Analysis",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jiuru Lyu",
    "section": "Education",
    "text": "Education\nEmory University (2022 - 2026) | Atlanta, GA  BS in Applied Mathematics and Statistics; Minor in Computer Science\nUnited World College Changshu China (2019 - 2022) | Suzhou, China  International Baccalaureate Diploma Programme"
  },
  {
    "objectID": "index.html#research-experience",
    "href": "index.html#research-experience",
    "title": "Jiuru Lyu",
    "section": "Research Experience",
    "text": "Research Experience\nResearch Assistant @ Emory University (2025 - Present) | Atlanta, GA Topic: Numerical Methods for Mean-Field Games\nResearch Assistant @ Emory University (2025 - Present) | Atlanta, GA Topic: Spectral Graph Theory and Differential Equations\nResearch Assistant @ Emory University (2023 - Present) | Atlanta, GA Topic: Global Optimization for Morse Potential"
  },
  {
    "objectID": "index.html#work-experience",
    "href": "index.html#work-experience",
    "title": "Jiuru Lyu",
    "section": "Work Experience",
    "text": "Work Experience\nSummer Digital Intern @ Baker Hughes (2024) | Shanghai, China"
  },
  {
    "objectID": "notes/NumericalODEsPDEs.html",
    "href": "notes/NumericalODEsPDEs.html",
    "title": "Numerical ODEs and PDEs",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes/cs334/09-Boosting/09-Boosting.html",
    "href": "notes/cs334/09-Boosting/09-Boosting.html",
    "title": "9 Boosting",
    "section": "",
    "text": "Goal: Reduce both approximation and estimation error at the same time (bias-variance tradeoff)."
  },
  {
    "objectID": "notes/cs334/09-Boosting/09-Boosting.html#bosting-introduction",
    "href": "notes/cs334/09-Boosting/09-Boosting.html#bosting-introduction",
    "title": "9 Boosting",
    "section": "Bosting Introduction",
    "text": "Bosting Introduction\n\n\n\n\n\n\n\nDefinition 1 (Boosting) Combine simple “weak” classifiers into a more complex “stronger” model.\n\nType of boosting methos: AdaBoost and Gradient Boosting\nWeak classifier: Decision stump (a one-level decision tree)\n\n\n\n\n\n\n\nFigure 1: Decision Stump\n\n\n\n\n\n\n\n\nNotation: \\[\nh\\qty(\\va x;\\va\\theta)=\\operatorname{sign}(\\theta_1(x_k-\\theta_0))=\\pm1,\n\\] where \\(\\va\\theta=(k,\\theta_0,\\theta_1)\\) such that \\(k\\) denotes the coordinate, \\(\\theta_0\\) is the location, and \\(\\theta_1\\) is the direction.\nEnsemble as a weighted combination of decision stumps: \\[\n\\begin{aligned}\nh_M(\\va x)&=\\sum_{m=1}^M\\alpha_m h\\qty(\\va x;\\va\\theta^{(m)})&\\text{for}\\quad \\alpha_m\\geq0.\\\\\n&=\\alpha_1 h\\qty(\\va x;\\va\\theta^{(1)})+\\alpha_2 h\\qty(\\va x;\\va\\theta^{(2)})+\\cdots+\\alpha_M h\\qty(\\va x;\\va\\theta^{(M)})\n\\end{aligned}\n\\]\nThis can be viewed as a linear classifier: \\(h_M(\\va x)=\\va\\alpha\\cdot\\phi(\\va x)\\), where\n\n\\(\\phi(\\va x)=\\mqty[h(\\va x;\\va\\theta^{(1)},\\dots,h(\\va x;\\va\\theta^{(M)})]\\), the feature mapping\n\\(\\va\\alpha=\\mqty[\\alpha_1,\\dots,\\alpha_m]\\), the parameter vector \\(\\va\\alpha\\in\\R^M\\).\n\n\\(\\phi(\\va x)\\in\\R^M\\):\n\nintead of specifying \\(\\phi(\\va x)\\), we learn \\(\\phi(\\cdot)\\) in addition to \\(\\va\\alpha\\).\n\\(M\\), the number of weak learners, is a hyperparameter."
  },
  {
    "objectID": "notes/cs334/09-Boosting/09-Boosting.html#adaboost-adaptive-boosting",
    "href": "notes/cs334/09-Boosting/09-Boosting.html#adaboost-adaptive-boosting",
    "title": "9 Boosting",
    "section": "AdaBoost: Adaptive Boosting",
    "text": "AdaBoost: Adaptive Boosting\n\nOne of the simplies and most popular boosting algorithms.\nAlgorithm Overview:\n\nSet example weights uniformly.\nFor each weak learner \\(m=1,\\dots, M\\):\n\nTrain decision stump on weighted examples\nApply decision stump to all examples\nSet decision stump weight based on weighted error\nSet example weights based on ensemble predictions.\n\n\nA greedy approach: Given the current ensemble, find the next weak learner that, if added, makes a better new ensemble.\n\nSuppose we have the frist \\((m-1)\\) decision stumps, which make up an ensemble classifier \\[h(\\va x;\\va\\theta^{(1)}),\\ h(\\va x;\\va\\theta^{(2)}),\\dots, h(\\va x;\\va\\theta^{(m-1)})\\implies h_{m-1}(\\va x).\\]\nGoal: Find the next decision stump \\(h(\\va x;\\va\\theta^{(m)})\\) and its weight \\(\\alpha_m\\) in order to minimize the same training loss (empirical risk) of the new ensemble \\(h_m(\\va x)\\): \\[\n\\begin{aligned}\n\\argmin_{\\va\\theta^{(m)},\\alpha_m} J(\\alpha_m,\\va\\theta^{(m)})&=\\argmin_{\\va\\theta^{(m)},\\alpha_m}\\sum_{i=1}^N \\operatorname{loss}\\qty(y^{(i)}\\cdot h_m(\\va x^{(i)})) &\\text{exponential }\\operatorname{loss}(\\cdot)=e^{-z}=\\exp(-z).\\\\\nJ(\\alpha_m,\\va\\theta^{(m)})&=\\sum_{i=1}^N \\exp\\qty(-y^{(i)}\\cdot h_m(\\va x^{(i)}))\\\\\n&=\\sum_{i=1}^N \\exp\\bigg(-y^{(i)}\\bigg[\\underbrace{h_{m-1}(\\va x^{(i)})}_{\\substack{\\text{previous}\\\\\\text{ensemble}}}+\\underbrace{\\alpha_mh(\\va x;\\va\\theta^{(m)})}_{\\text{update}}\\bigg]\\bigg)\\\\\n&=\\sum_{i=1}^N\\underbrace{\\exp\\qty(-y^{(i)}h_{m-1}(\\va x^{(i)}))}_{\\substack{\\text{independent of }\\alpha_m,\\va\\theta^{(m)}\\\\\\text{denote it as }w_{m-1}(i)}}\\exp\\qty(-y^{(i)}\\alpha_mh(\\va x;\\va\\theta^{(m)}))\\\\\n&=\\sum_{i=1}^N w_{m-1}(i)\\exp\\qty(-y^{(i)}\\alpha_mh(\\va x;\\va\\theta^{(m)}))\\\\\n\\end{aligned}\n\\]\nLet \\(w_{m-1}(i)=\\exp\\qty(-y^{(i)}h_{m-1}(\\va x^{(i)}))\\):\n\nLoss associated with the previous ensemble with respect to the \\((i)\\)-th point.\nUsed as weight for each training example: \\(i=1,\\dots, N\\).\nNew decision stump \\(\\va\\theta^{(m)}\\) will be more heavily influenced by examples that were misclassified by the previous ensemble.\n\nNote: weighted loss associated with point \\(\\va x^{(i)}\\) in the objective: \\[\nJ(\\alpha_m,\\va\\theta{(m)})=\\begin{cases}w_{m-1}(i)\\exp(-\\alpha_m)\\quad&\\text{if }\\va x^{(i)}\\text{ is correctly classified}\\\\w_{m-1}(i)\\exp(\\alpha_m)\\quad&\\text{o/w}.\\end{cases}\n\\] If \\(\\va x^{(i)}\\) is correctly classified: \\(y^{(i)}=h(\\va x;\\va\\theta^{(m)})\\). Let’s normalize example weigths so that they sum to \\(1\\): \\[\n\\tilde w_{m-1}(i)=\\dfrac{w_{m-1}(i)}{\\dsst\\sum_{j} w_{m-1}(j)}.\n\\] Then, \\[\n\\begin{aligned}\n\\tilde J(\\alpha_m,\\va\\theta^{(m)})&=\\underbrace{\\exp(-\\alpha_m)\\sum_{i:y^{(i)}=h(\\va x^{(i)};\\va\\theta^{(m)})}\\tilde w_{m-1}(i)}_\\text{correctly classified}+\\underbrace{\\exp(\\alpha_m)\\sum_{i:y^{(i)}\\neq h(\\va x^{(i)};\\va\\theta^{(m)})}\\tilde w_{m-1}(i)}_\\text{incorrect points}\\\\\n&=\\exp(-\\alpha_m)\\underbrace{\\sum_{i=1}^N\\tilde w_{m-1}(i)}_{=1}-\\exp(-\\alpha_m)\\sum_{i=1}^N\\tilde{w}_{m-1}(i)\\1\\qty{y^{(i)}\\neq h(\\va x^{(i)}\\neq h(\\va x^{(i);\\va\\theta^{(m)}}))}\\\\\n&\\qquad\\qquad+\\exp(\\alpha_m)\\sum_{i=1}^N\\tilde{w}_{m-1}(i)\\1\\qty{y^{(i)}\\neq h(\\va x^{(i)};\\va\\theta^{(m)})}\\\\\n&=\\exp(-\\alpha_m)+\\underbrace{\\qty\\bigg(\\exp(\\alpha_m)-\\exp(-\\alpha_m))}_{\\geq0}\\sum_{i=1}^N\\tilde{w}_{m-1}(i)\\1\\qty{y^{(i)}\\neq h(\\va x^{(i)};\\va\\theta^{(m)})}\\\\\n\\end{aligned}\n\\] So, our objective is \\[\n\\argmin_{\\va\\theta^{(m)},\\alpha_m}\\exp(-\\alpha_m)+\\qty\\bigg(\\exp(\\alpha_m)-\\exp(-\\alpha_m))\\sum_{i=1}^N\\tilde{w}_{m-1}(i)\\1\\qty{y^{(i)}\\neq h(\\va x^{(i)};\\va\\theta^{(m)})}\n\\]\nTo solve:\n\nFirst solve for best \\(\\va\\theta^{(m)*}\\), then \\[\n\\va\\theta^{(m)*}=\\argmin_{\\va\\theta^{(m)}}\\underbrace{\\sum_{i=1}^N\\tilde{w}_{m-1}(i)\\1\\qty{y^{(i)}\\neq h(\\va x^{(i)};\\va\\theta^{(m)})}}_{\\substack{\\epsilon_m\\text{: weighted classification error}\\\\\\text{of decision stumps}}}=\\argmin_{\\va\\theta^{(m)}}\\,\\epsilon_m\n\\] It is easy to optimize \\(\\va\\theta^{(m)}\\) via an exhaustive search over all possible decision stumps. Denote \\[\n\\tilde\\epsilon_m=\\sum_{i=1}^N\\tilde{w}_{m-1}(i)\\1\\qty{y^{(i)}\\neq h(\\va x^{(i)};\\va\\theta^{(m)*})}\n\\]\nFigure out how much weight to give it. \\[\n\\alpha_m^*=\\argmin_{\\alpha_m}\\underbrace{\\exp(-\\alpha_m)+\\qty[\\exp(\\alpha_m)-\\exp(-\\alpha_m)]\\tilde\\epsilon_m}_{L(\\alpha_m).}\n\\] By FOC: \\[\n\\begin{aligned}\n\\dfrac{\\partial L(\\alpha_m)}{\\partial\\alpha_m}=-\\exp(-\\alpha_m)+\\qty[\\exp(\\alpha_m)-\\exp(-\\alpha_m)]\\tilde\\epsilon_m&\\overset{\\text{set}}{=}0\\\\\n(\\epsilon_m-1)\\exp(-\\alpha_m)+\\epsilon_m\\exp(\\alpha_m)&=0\\\\\n\\implies\\alpha_m^*&=\\dfrac{1}{2}\\ln\\dfrac{1-\\tilde\\epsilon_m}{\\tilde\\epsilon_m}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\\begin{algorithm} \\caption{AdaBoost} \\begin{algorithmic} \\State{Set $\\tilde w_0(i)=\\dfrac{1}{N}$ for $i=1,\\dots, N$} \\# Set example weights uniformly \\For{$m=1,\\dots,M$} \\State{\\# for each weak leaner} \\State{Find $\\va\\theta^{(m)}$ that minimizes $\\epsilon_m$} \\# Train decision stump on weighted examples \\State{Set $\\alpha_m=\\dfrac{1}{2}\\ln\\qty(\\dfrac{1-\\tilde\\epsilon_m}{\\tilde\\epsilon_m})$} \\# Set decision stump weight based on weighted error \\State{\\# Update weights based on the new ensemble} \\# Set example weights based on ensemble predictions \\For{$i=1,\\dots,N$} \\State{$\\tilde w_m(i)=\\dfrac{\\tilde w_{m-1}(i)\\exp\\qty(-y^{(i)}\\alpha_m^*h_m(\\va x^{(i)};\\theta^{(m)*}))}{Z_m}$} \\# Update weights \\State{where $Z_m$ is the normalization factor to ensure weigths sum to $1$} \\EndFor \\EndFor \\State{Find ensemble classifier $\\dsst h_M(\\va x)=\\sum_{m=1}^M\\alpha_mh(\\va x;\\va\\theta^{(m)})$} \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\n\nExample 1 (How AdaBoost Works in Action)  \n\n\n\n\n\n\nFigure 2: Boosting Example\n\n\n\n\n\n\n\n\n\nFigure 3: Graphical Representation of Boosting Results\n\n\n\n\n\n\n\n\nProperties of Boosting:\n\nWeighted training error \\(\\hat\\epsilon_m\\) (for each iteration’s new weak error): tends to increase with each boosting iteration.\nEnsemble training error does not necessarily decrease monotonically. Ensemble exponential loss decreases monotonically (as this is what we are minimizing).\nEnsemble test error (generalization error) does not increase even after a large number of boosting iterations (more robust to overfitting)."
  },
  {
    "objectID": "notes/cs334/10-Introduction-to-Neural-Networks/10-Introduction-to-Neural-Networks.html",
    "href": "notes/cs334/10-Introduction-to-Neural-Networks/10-Introduction-to-Neural-Networks.html",
    "title": "10 Introduction to Neural Networks",
    "section": "",
    "text": "Successful machine learning application relies on successful representation of data. \\[\n\\text{Input}\\longrightarrow\\text{Feature Representation}\\longrightarrow\\text{Learning Algorithm}\n\\]\nMachine learning practitioners put a lot of effort into feature engineering:\n\nHow can we get good representations automatically?\nHow can we learn good features?\n\nMethods for non-linear classification:\n\nExplicit feature mapping \\(\\phi(\\va x)\\): \\[\\hat y=\\operatorname{sign}(\\va\\theta\\cdot\\phi(\\va x)).\\]\n\nHand-crafted, labor-intensive, could become very high dimensional.\n\\(\\phi(\\va x)\\) depends on \\(\\va x\\) alone and is not learned.\n\nImplicit feature mapping via a kernel: \\[\\hat y=\\operatorname{sign}\\qty(\\sum_{j=1}^N\\alpha_j y^{(j)}K(\\va x^{(j)},\\va x))=\\operatorname{sign}(\\va\\alpha\\cdot\\phi(\\va x)),\\] where \\(\\phi(\\va x)=\\mqty[y^{(1)}K(\\va x^{(1)},\\va x),\\dots,y^{(N)}K(\\va x^{(N)},\\va x)]\\).\n\n\\(\\phi(\\va x)\\) depends on \\(\\va x\\) and the training set \\(\\qty{\\va x^{(i)},y^{(i)}}_{i=1}^N\\), but is not learned.\n\nBoosting ensemble: \\[\\hat y=\\operatorname{sign}\\qty(\\sum_{m=1}^M\\alpha_mh(\\va x;\\va\\theta^{(m)}))=\\operatorname{sign}(\\va\\alpha\\cdot\\phi(\\va x)),\\] where \\(\\phi(\\va x)=\\mqty[h(\\va x;\\va\\theta^{(1)}),\\dots,h(\\va x;\\va\\theta^{(M)})]\\).\n\n\\(\\phi(\\va x)\\) is learned via a greedy approach in boosting.\nOptimization is performed sequentially choosing one weak classifier at a time.\n\nNeural networks:\n\nJointly Learning\nModel parameters and feature representations are learned at the same time."
  },
  {
    "objectID": "notes/cs334/10-Introduction-to-Neural-Networks/10-Introduction-to-Neural-Networks.html#representing-data",
    "href": "notes/cs334/10-Introduction-to-Neural-Networks/10-Introduction-to-Neural-Networks.html#representing-data",
    "title": "10 Introduction to Neural Networks",
    "section": "",
    "text": "Successful machine learning application relies on successful representation of data. \\[\n\\text{Input}\\longrightarrow\\text{Feature Representation}\\longrightarrow\\text{Learning Algorithm}\n\\]\nMachine learning practitioners put a lot of effort into feature engineering:\n\nHow can we get good representations automatically?\nHow can we learn good features?\n\nMethods for non-linear classification:\n\nExplicit feature mapping \\(\\phi(\\va x)\\): \\[\\hat y=\\operatorname{sign}(\\va\\theta\\cdot\\phi(\\va x)).\\]\n\nHand-crafted, labor-intensive, could become very high dimensional.\n\\(\\phi(\\va x)\\) depends on \\(\\va x\\) alone and is not learned.\n\nImplicit feature mapping via a kernel: \\[\\hat y=\\operatorname{sign}\\qty(\\sum_{j=1}^N\\alpha_j y^{(j)}K(\\va x^{(j)},\\va x))=\\operatorname{sign}(\\va\\alpha\\cdot\\phi(\\va x)),\\] where \\(\\phi(\\va x)=\\mqty[y^{(1)}K(\\va x^{(1)},\\va x),\\dots,y^{(N)}K(\\va x^{(N)},\\va x)]\\).\n\n\\(\\phi(\\va x)\\) depends on \\(\\va x\\) and the training set \\(\\qty{\\va x^{(i)},y^{(i)}}_{i=1}^N\\), but is not learned.\n\nBoosting ensemble: \\[\\hat y=\\operatorname{sign}\\qty(\\sum_{m=1}^M\\alpha_mh(\\va x;\\va\\theta^{(m)}))=\\operatorname{sign}(\\va\\alpha\\cdot\\phi(\\va x)),\\] where \\(\\phi(\\va x)=\\mqty[h(\\va x;\\va\\theta^{(1)}),\\dots,h(\\va x;\\va\\theta^{(M)})]\\).\n\n\\(\\phi(\\va x)\\) is learned via a greedy approach in boosting.\nOptimization is performed sequentially choosing one weak classifier at a time.\n\nNeural networks:\n\nJointly Learning\nModel parameters and feature representations are learned at the same time."
  },
  {
    "objectID": "notes/cs334/10-Introduction-to-Neural-Networks/10-Introduction-to-Neural-Networks.html#introduction-to-neural-networks",
    "href": "notes/cs334/10-Introduction-to-Neural-Networks/10-Introduction-to-Neural-Networks.html#introduction-to-neural-networks",
    "title": "10 Introduction to Neural Networks",
    "section": "Introduction to Neural Networks",
    "text": "Introduction to Neural Networks\n\nOther names:\n\nFeedforward neural networks (FNN)\nMultilayer perceptrons (MLP)\nFully-connected/Dense networks\n\n\n\n\n\n\n\n\n\nDefinition 1 (Neural Networks) Neural networks are composed of simple computational units called neurons/units.\n\n\n\n\n\n\nFigure 1: Example of a Neural Network\n\n\n\n\n\\(z=z(\\va x;\\va w)=w_1x_1+w_2x_2+w_3x_3\\), and \\(h=g(z)\\) is the activation function.\nExample of activation function: sigmoid function: \\[h=g(z)=\\dfrac{1}{1+e^{-z}}.\\]\n\nNeurons are arranged in a network composed of layers. The specific arrangement corresponds to the architecture. - Input layer: The first layer of the network, which receives the input data. - Hidden layers: Intermediate layers between the input and output layers. Each hidden layer consists of multiple neurons that process the input data. - Output layer: The final layer of the network, which produces the output predictions.\n\n\n\n\n\n\nFigure 2: Example of Layers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemark 1 (Single-Layer Neural Network). A single-layer neural network is simply logistic regression. \\[\n\\va x=\\mqty[x_1\\\\x_2\\\\x_3],\\quad \\va w=\\mqty[w_1\\\\w_2\\\\w_3],\\quad \\implies\\quad\\hat y=h(\\va x;\\va w)=\\dfrac{1}{1+\\exp(-\\va w\\cdot\\va x)}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nRemark 2 (Offset/Bias Term). We can always include an offset/intercept/bias term by adding a constant input.\n\n\n\n\n\n\nFigure 3: Adding Bias\n\n\n\n\\[\nz=z(\\va x;\\va w)=w_0+w_1x_1+w_2x_2+w_3x_3+w_4.\n\\]\n\n\n\n\n\nAdding one hidden layer:\n\nNotation: the weight \\(W_{ik}^{(j)}\\), where \\(j\\)-th layer, \\(i\\)-th input, and \\(k\\)-th unit. \nTwo-layer network since ther eare two sets of weights.\nForward propagation: \nCommon activation functions:\n\nReLU: \\(g(z)=\\max\\qty{0,z}\\).\nThreshold: \\(g(z)=\\operatorname{sign}(z)\\).\nSoftmax: \\(g(z)=\\dfrac{e^z}{\\dsst\\sum_ke^{z_k}}\\) for multiclass classification.\nSigmoid: \\(g(z)=\\dfrac{1}{(1+e^{-z})}\\).\nHyperbolic tangent: \\(g(z)=\\tanh(z)\\).\n\n\n\n\n\n\n\n\n\n\nExample 1 (What is neural network doing?)  \n\n\n\nNeural Network Effects"
  },
  {
    "objectID": "notes/cs334/10-Introduction-to-Neural-Networks/10-Introduction-to-Neural-Networks.html#what-is-neural-network-doing",
    "href": "notes/cs334/10-Introduction-to-Neural-Networks/10-Introduction-to-Neural-Networks.html#what-is-neural-network-doing",
    "title": "10 Introduction to Neural Networks",
    "section": "What is neural network doing?",
    "text": "What is neural network doing?\n\n\n\nNeural Network Effects"
  },
  {
    "objectID": "notes/cs334/10-Introduction-to-Neural-Networks/10-Introduction-to-Neural-Networks.html#training-neural-networks",
    "href": "notes/cs334/10-Introduction-to-Neural-Networks/10-Introduction-to-Neural-Networks.html#training-neural-networks",
    "title": "10 Introduction to Neural Networks",
    "section": "Training Neural Networks",
    "text": "Training Neural Networks\nGaol: Formulate an optimization problem to find the weifghts. Apply SGD.\n\nSet-up: \\(D=\\qty{\\va x^{(i)},y^{(i)}}_{i=1}^N\\), \\(\\va x\\in\\R^d\\), \\(y\\in\\qty{-1,+1}\\).\nWe want to learn \\(\\va\\theta=\\mqty[w_{10}^{(1)},w_{11}^{(1)},\\dots,w_{km}^{(L)}]\\) parameter of the neural network in order to minimize loss over training examples: \\[\nJ(\\va\\theta)=\\dfrac{1}{N}\\sum_{i=1}^N\\operatorname{loss}(y^{(i)}\\cdot h(\\va x^{(i)};\\va\\theta)),\n\\] where \\(y^{(i)}\\) is the true label and \\(h(\\va x^{(i)};\\va\\theta)\\) is the neural network output.\n\n\n\n\n\n\n\n\nRemark 3 (Choice of Loss Function). \\(\\operatorname{loss}(\\cdot)\\) can be any (sub)differentiable loss function. For example, hinge loss.\n\n\n\n\n\nOverview of Optimization Procedure: SGD\n\nInitialize \\(\\va\\theta\\) to small random values (to prevent learning the same features).\nSelect \\(i=\\qty{1,\\dots,N}\\) at random (or mini-batch).\nUpdate \\[\\va\\theta^{(k+1)}=\\va\\theta^{(k)}-\\eval{\\eta_k\\grad_{\\va\\theta}\\operatorname{loss}\\qty(y^{(i)}\\cdot h(\\va x^{(i)};\\va\\theta))}_{\\va\\theta=\\va\\theta^{(k)}}.\\]\n\n\n\n\n\n\n\n\n\nRemark 4 (Number of Parameters/Dimensions). Gradient vector has the same dimensionality as the number of parameters. \\[\nd'=\\sum_{\\l=1}^{\\text{number of layers}}\\qty(\\text{number of inputs for layer }\\l)\\times\\qty(\\text{number of ouputs for layer }\\l).\n\\]"
  },
  {
    "objectID": "notes/cs334/10-Introduction-to-Neural-Networks/10-Introduction-to-Neural-Networks.html#backpropagation",
    "href": "notes/cs334/10-Introduction-to-Neural-Networks/10-Introduction-to-Neural-Networks.html#backpropagation",
    "title": "10 Introduction to Neural Networks",
    "section": "Backpropagation",
    "text": "Backpropagation\n\nSimple Single-Layer NN\n\n\n\n\n\n\nFigure 4: single-layer-nn\n\n\n\n\nWe consider no activation function: \\[h(\\va x;\\theta)=w_0+\\sum_{j=1}^dw_jx_j=z.\\]\nConsider Hinge loss: \\[L=\\max\\qty{0, 1-yz}.\\]\nFor \\(j=1,\\dots, d\\): \\[\\pdv{L}{w_j}=\\pdv{L}{z}\\cdot\\pdv{z}{w_j}=-y\\cdot\\1\\qty{L&gt;0}\\cdot x_j,\\quad\\text{Chain Rule: }L(z(w)).\\]\nThe update rule is: if \\(\\operatorname{loss}(yz)&gt;0\\) (equivalent to \\(y\\cdot h(\\va x;\\va\\theta^{(k)})&lt;1\\).), then \\(w_j^{(k+1)}=w_j^{(k)}+\\eta_kyx_j\\).\n\\(j=0\\): \\[\\pdv{L}{w_0}=\\pdv{L}{z}\\cdot\\pdv{z}{w_0}=-y\\cdot\\1\\qty{L&gt;0}\\cdot 1=-y\\cdot\\1{K&gt;0}.\\]\nThe update rule is if \\(\\operatorname{loss}(yz)&gt;0\\), then \\(w_0^{(k+1)}=w_0^{(k)}+\\eta_ky\\).\n\n\n\nTwo-Layer NN (One Hidden Layer)\n\n\n\n\n\n\nFigure 5: Two-Layer NN\n\n\n\n\\[\n\\begin{aligned}\nz_j^{(2)}&=w_{0j}^{(1)}+\\sum_{i=1}^d x_iw_{ij}^{(1)}\\\\\nh_j^{(2)}&=\\max\\qty{0,z_j^{(2)}}\\\\\nz^{(3)}&=w_{0}^{(2)}+\\sum_{j=1}^m h_j^{(2)}w_j^{(2)}\\\\\nh(\\va x;\\va\\theta)&=z^{(3)}&\\text{(no activation function)}\n\\end{aligned}\n\\]\n\nNotation change: \\(i=0,\\dots, d\\) and \\(j=1,\\dots, m\\)\n\n\\(w_{ij}=w_{ij}^{(1)}\\)\n\\(v_j=w_j^{(2)}\\)\n\\(z_j=z_j^{(2)}\\)\n\\(h_j=h_j^{(2)}\\)\n\\(z=z^{(3)}\\)\nThen, \\[\n\\begin{aligned}\nz_j&=w_{0j}+\\sum_{i=1}^d x_iw_{ij}\\\\\nh_j&=\\max\\qty{0,z_j}\\\\\nz&=v_0+\\sum_{j=1}^m h_jv_j\\\\\nh(\\va x;\\va\\theta)&=z\\\\\nL&=\\max\\qty{0,1-yz} &\\text{(loss function)}.\n\\end{aligned}\n\\]\n\nTo work out the update rule, let’s go backwards\n\nOutput layer is simple linear classifier based on \\(h_j\\)’s: \\[\n\\begin{aligned}\nh(\\va x;\\va\\theta)&=z=\\va v\\cdot\\phi(\\va x)\\\\\n\\phi(\\va x)&=\\mqty[1, h_1,\\dots,h_m]\\\\\n\\va v&=\\mqty[v_0,v_1,\\dots,v_m]\\\\\n\\pdv{L}{v_j}&=\\pdv{L}{z}\\cdot\\pdv{z}{v_j}=-y\\1\\qty{L&gt;0}h_j\n\\end{aligned}\n\\] So, the update rule is \\(v_j^{(k+1)}=v_j^{(k)}+\\eta_ky\\1\\qty{L&gt;0}h_j\\).\nHidden layer: each unit is a linear combination of input feature \\(x_i\\)’s followed by activation function \\(g\\), but its update \\(h_j\\) is used as input to the ouput layer, so changing \\(w_{ij}\\) will affect the loss through \\(z_j,h_j\\), and \\(z\\). \\[\n\\begin{aligned}\n\\pdv{L}{w_{ij}}&=\\pdv{L}{z}\\cdot\\pdv{z}{h_j}\\cdot\\pdv{h_j}{z_j}\\cdot\\pdv{z_j}{w_{ij}}\\\\\n&=-y\\1\\qty{L&gt;0}v_j\\cdot\\1\\qty{z_j&gt;0}\\cdot x_i\\\\\n\\end{aligned}\n\\] Then, the update rule is \\(w_{ij}^{(k+1)}=w_{ij}^{(k)}+\\eta_ky\\1\\qty{L&gt;0}v_j\\1\\qty{z_j&gt;0}x_i\\) for \\(i=0,\\dots,d\\) and \\(j=0,\\dots,m\\).\n\nThis is the backpropagation: \\[\n\\begin{aligned}\n\\pdv{L}{v_1}&=\\pdv{L}{z}\\cdot\\pdv{z}{v_1}\\\\\n\\pdv{L}{w_{11}}&=\\pdv{L}{z}\\cdot\\pdv{z}{h_1}\\cdot\\pdv{h_1}{z_1}\\cdot\\pdv{z_1}{w_{11}}\\\\\n\\pdv{L}{w_{01}}&=\\pdv{L}{z}\\cdot\\pdv{z}{h_1}\\cdot\\pdv{h_1}{z_1}\\cdot\\pdv{z_1}{w_{01}}\\\\\n\\end{aligned}\n\\]\n\nObservation: intermediate partial derivatives are shared. Overlapping subproblems, implemented via dynamic programming.\nBackpropagation: Chain Rule + Dynamic Programming.\nSubroutine for effeciently computing the gradient (i.e., all partial derivatives) of a neural network:\n\nfor each training example:\n\nmake a forward pass, go through the layers and make a prediction\ncalculate loss\nmake a backward pass, go through the layers in reverse and calculate the partial derivatives.\n\nUse the gradient information in an optimiztioon procedure (e.g. SGD).\n\n\n\n\n\nThree-Layer NN (Two Hidden Layers)\n\n\n\n\n\n\nFigure 6: Three-Layer NN"
  },
  {
    "objectID": "notes/cs334/11-Convolutional-Neural-Networks/11-Convolutional-Neural-Networks.html",
    "href": "notes/cs334/11-Convolutional-Neural-Networks/11-Convolutional-Neural-Networks.html",
    "title": "11 Convolutional Neural Networks",
    "section": "",
    "text": "Each image is a \\(32\\times32\\) grayscale image (\\(0-255\\)).\nA flat representation of the image: \\[\\va x=\\mqty[x_1,x_2,\\dots,x_{1024}]\\]\nProblem with flat representation:\n\nIgnore spatial structure\nSubsceptible to translational error\n\nGoal: preserve the spatial structure of the task by capturing relationships among neighboring pixels.\nIdeas:\n\nLaern feature representations based on small patches.\nApply patch-based feature representations across the entire image.\n\nBuilding blocks of CNN:\n\nConvolutional layers\nActivation layers\nPooling layers\nFully connected layers"
  },
  {
    "objectID": "notes/cs334/11-Convolutional-Neural-Networks/11-Convolutional-Neural-Networks.html#introduction-to-the-mnist-dataset-and-cnns",
    "href": "notes/cs334/11-Convolutional-Neural-Networks/11-Convolutional-Neural-Networks.html#introduction-to-the-mnist-dataset-and-cnns",
    "title": "11 Convolutional Neural Networks",
    "section": "",
    "text": "Each image is a \\(32\\times32\\) grayscale image (\\(0-255\\)).\nA flat representation of the image: \\[\\va x=\\mqty[x_1,x_2,\\dots,x_{1024}]\\]\nProblem with flat representation:\n\nIgnore spatial structure\nSubsceptible to translational error\n\nGoal: preserve the spatial structure of the task by capturing relationships among neighboring pixels.\nIdeas:\n\nLaern feature representations based on small patches.\nApply patch-based feature representations across the entire image.\n\nBuilding blocks of CNN:\n\nConvolutional layers\nActivation layers\nPooling layers\nFully connected layers"
  },
  {
    "objectID": "notes/cs334/11-Convolutional-Neural-Networks/11-Convolutional-Neural-Networks.html#convolutional-layer",
    "href": "notes/cs334/11-Convolutional-Neural-Networks/11-Convolutional-Neural-Networks.html#convolutional-layer",
    "title": "11 Convolutional Neural Networks",
    "section": "Convolutional Layer",
    "text": "Convolutional Layer\n\n\n\n\n\n\nFigure 1: Covolutional Layer\n\n\n\n\nNeurons that maps a \\(3\\times 3\\) patch to a scalar value. \\[\\va x\\cdot\\va w+b,\\] where \\(\\va x\\) is the image patch, \\(\\va w\\) and \\(b\\) are filter parameters.\nConvolution operation:\n\nSlide the filter over the image spatially\nCompute the dot product with different patches of the image.\n\n\n\n\n\n\n\n\n\nExample 1 (Padding)  \n\n\n\n\n\n\nFigure 2: filter\n\n\n\nOften times, it is beneficial to preserve the original image size. This can be done with padding: allow filter to overlap with boundary (zero padding/copy-paste).\n\n\n\n\n\n\nFigure 3: padding"
  },
  {
    "objectID": "notes/cs334/12-Recurrent-Neural-Networks/12-Recurrent-Neural-Networks.html",
    "href": "notes/cs334/12-Recurrent-Neural-Networks/12-Recurrent-Neural-Networks.html",
    "title": "12 Recurrent Neural Networks",
    "section": "",
    "text": "Application:\n\nLanguage modeling\nSequence tagging\nText classification\n\nRNN is a family of neural networks for processing sequential data of arbitrary length.\n\nOutput of the layer can connect back to the neuron itself or a layer before it.\nShare same weights across several time steps.\n\nA recurrence function is applied at each step: \\[h_t=f_W(h_{t-1},x_t),\\] where\n\n\\(h_t\\) is the new state\n\\(f_W\\) is a neural network with parameter \\(W\\)\n\\(h_{t-1}\\) is the old state\n\\(x-t\\) is the input feature vector at time step \\(t\\)\n\nVanilla RNN: connect the output of the last layer to the input of the next layer.\n\nThe problem of long-term dependencies:\n\nAppeal of RNN is to connect previous information to current task.\nGap between relevant information and where we need it can be large.\nLong-range dependencies are difficult to learn because of vanishing gradients or exploding gradients.\n\n\nTo solve the problem, we introduce LSTM networks and GRU networks.\nThere are other more advanced architectures, such as Attention and Transformer networks."
  },
  {
    "objectID": "notes/cs334/14-Recommender-Systems/14-Recommender-Systems.html",
    "href": "notes/cs334/14-Recommender-Systems/14-Recommender-Systems.html",
    "title": "14 Recommender Systems",
    "section": "",
    "text": "Collaborative filtering:\n\nRecommender problems that can be reduced to a matrix completion problem.\nWe have a \\(n\\times m\\) matrix, where \\(n\\) is the number of users and \\(m\\) is the number of movies. Each entry \\(Y_{a,i}\\in\\qty{1,\\dots,5}\\) is the rating of user \\(a\\) for movie \\(i\\).\nThe key idea is to borrow experience from other similar users."
  },
  {
    "objectID": "notes/cs334/15-Clustering/15-Clustering.html",
    "href": "notes/cs334/15-Clustering/15-Clustering.html",
    "title": "15 Clustering",
    "section": "",
    "text": "Input: dataset of feature vectors \\(D=\\qty{\\va x^{(i)}}_{i=1}^N\\), where \\(\\va x^{(i)}\\in\\R^d\\). We don’t have labels!\nOutput: a set of clusters \\(C_1,\\dots,C_k\\)\nTypes of clusters: partitional vs. hierarchical"
  },
  {
    "objectID": "notes/cs334/13-Reinforcement-Learning/13-Reinforcement-Learning.html",
    "href": "notes/cs334/13-Reinforcement-Learning/13-Reinforcement-Learning.html",
    "title": "13 Reinforcement Learning",
    "section": "",
    "text": "Reinforcement learning is agent-oriented learning."
  },
  {
    "objectID": "notes/cs334/11-Convolutional-Neural-Networks/11-Convolutional-Neural-Networks.html#activation-layers",
    "href": "notes/cs334/11-Convolutional-Neural-Networks/11-Convolutional-Neural-Networks.html#activation-layers",
    "title": "11 Convolutional Neural Networks",
    "section": "Activation Layers",
    "text": "Activation Layers\n\n\n\n\n\n\nFigure 4: Activation Layer\n\n\n\n\nEach filter produces a feature map and a activation map.\nMultiple filters \\(\\longrightarrow\\) multiple feature maps and activation maps (or channels)."
  },
  {
    "objectID": "notes/cs334/11-Convolutional-Neural-Networks/11-Convolutional-Neural-Networks.html#pooling-layers",
    "href": "notes/cs334/11-Convolutional-Neural-Networks/11-Convolutional-Neural-Networks.html#pooling-layers",
    "title": "11 Convolutional Neural Networks",
    "section": "Pooling Layers",
    "text": "Pooling Layers\n\n\n\n\n\n\nFigure 5: Pooling Layer\n\n\n\n\nDownsamples previous layers activation map\nCosolidate feature learned at previous stage.\nWhy?\n\nCompress/Smooth\nSpatial invariance\nPrevent overfitting\n\nPooling often uses simple functions: max or average.\nPooling operates over each activation map independently."
  },
  {
    "objectID": "notes/cs334/11-Convolutional-Neural-Networks/11-Convolutional-Neural-Networks.html#fully-connected-layers",
    "href": "notes/cs334/11-Convolutional-Neural-Networks/11-Convolutional-Neural-Networks.html#fully-connected-layers",
    "title": "11 Convolutional Neural Networks",
    "section": "Fully Connected Layers",
    "text": "Fully Connected Layers\n\n\n\n\n\n\nFigure 6: Fully Connected Layer\n\n\n\n\nFlatten the output from previous layer\nNormal dense fully connected layer"
  },
  {
    "objectID": "notes/cs334/11-Convolutional-Neural-Networks/11-Convolutional-Neural-Networks.html#architecture-details",
    "href": "notes/cs334/11-Convolutional-Neural-Networks/11-Convolutional-Neural-Networks.html#architecture-details",
    "title": "11 Convolutional Neural Networks",
    "section": "Architecture Details:",
    "text": "Architecture Details:\n\nInput to a covolutional layer: \\(C_\\text{in}\\times H\\times W\\)\n\n\\(C_\\text{in}\\): number of input channels\n\\(H\\): height of the input\n\\(W\\): width of the input\n\n\\(C_\\text{out}\\) (number of output channels) filters of \\(h\\times w\\), where \\(h&lt;H\\) and \\(w&lt;W\\) (\\(h=w\\)).\nOutput: \\(C_\\text{out}\\times H'\\times W'\\), where \\(H'\\) and \\(W'\\) depends on filter size, padding, and stride.\nParameter sharing: efficient:\n\nSuppose input image \\(100\\times100\\longrightarrow10,000\\) input pixels.\nFully-connected layer with \\(100\\) neurons (no bias): \\(10,000\\times100=1,000,000\\) parameters.\nConvolutional layer with \\(100\\) filters of size \\(3\\times 3\\) (no bias): \\(3\\times3\\times100=900\\) parameters."
  },
  {
    "objectID": "notes/cs334/12-Recurrent-Neural-Networks/12-Recurrent-Neural-Networks.html#recurrent-neural-networks-rnns",
    "href": "notes/cs334/12-Recurrent-Neural-Networks/12-Recurrent-Neural-Networks.html#recurrent-neural-networks-rnns",
    "title": "12 Recurrent Neural Networks",
    "section": "",
    "text": "Application:\n\nLanguage modeling\nSequence tagging\nText classification\n\nRNN is a family of neural networks for processing sequential data of arbitrary length.\n\nOutput of the layer can connect back to the neuron itself or a layer before it.\nShare same weights across several time steps.\n\nA recurrence function is applied at each step: \\[h_t=f_W(h_{t-1},x_t),\\] where\n\n\\(h_t\\) is the new state\n\\(f_W\\) is a neural network with parameter \\(W\\)\n\\(h_{t-1}\\) is the old state\n\\(x-t\\) is the input feature vector at time step \\(t\\)\n\nVanilla RNN: connect the output of the last layer to the input of the next layer.\n\nThe problem of long-term dependencies:\n\nAppeal of RNN is to connect previous information to current task.\nGap between relevant information and where we need it can be large.\nLong-range dependencies are difficult to learn because of vanishing gradients or exploding gradients.\n\n\nTo solve the problem, we introduce LSTM networks and GRU networks.\nThere are other more advanced architectures, such as Attention and Transformer networks."
  },
  {
    "objectID": "notes/cs334/13-Reinforcement-Learning/13-Reinforcement-Learning.html#the-rl-interface-sequential-decision-making",
    "href": "notes/cs334/13-Reinforcement-Learning/13-Reinforcement-Learning.html#the-rl-interface-sequential-decision-making",
    "title": "13 Reinforcement Learning",
    "section": "The RL Interface: Sequential Decision Making",
    "text": "The RL Interface: Sequential Decision Making\n\nOverfiew: At each time \\(t\\), the agent\n\nObserve state \\(S_t\\) (Environment)\nExecutes an action \\(a_t\\)\nReceives a scalar reward \\(r_t\\)\nTransitions to the next state \\(S_{t+1}\\).\n\nThe process is repeated and forms a trajectory of states, actions, and rewards: \\(s_1,a_1,r_1,s_2,a_2,r_2,\\dots\\).\n\n\n\n\n\n\n\n\nDefinition 1 (Markov Decision Process (MDP))  \n\n\\(\\mathcal{S}\\): set of states\n\\(\\mathcal{A}\\): set of actions\n\\(R\\): reward function \\[p(r_t\\mid s_t,a_t)\\]\n\\(P\\): transition function \\[p(s_{t+1}\\mid s_t,a_t)\\]\n\nPolicy: mapping from states to actions \\[\\pi:\\mathcal{S}\\to\\mathcal{A}\\] Goal: learn polyci that maximizes the cummulative reward: \\[G=\\sum_{t=1}^Tr_t.\\]"
  },
  {
    "objectID": "notes/cs334/13-Reinforcement-Learning/13-Reinforcement-Learning.html#multi-armed-bandit-problem",
    "href": "notes/cs334/13-Reinforcement-Learning/13-Reinforcement-Learning.html#multi-armed-bandit-problem",
    "title": "13 Reinforcement Learning",
    "section": "Multi-Armed Bandit Problem",
    "text": "Multi-Armed Bandit Problem\n\n\n\n\n\n\nFigure 1: Multi-Armed Bandit Problem\n\n\n\n\nSet-up:\n\n\\(\\mathcal{S}\\): single state\n\\(\\mathcal{A}\\): arms \\(a=1,a=2,a=3,\\dots,a=k\\).\n\\(R\\): \\(P(r\\mid a)\\) for \\(a\\in\\mathcal{A}\\), pobablistic mapping from action to reward.\nTask: Agent sequentially chooses which arm to pull next.\nfor steps \\(t=1,\\dots, T\\): The arm chosed to pull: \\(a_t\\in\\mathcal{A}\\). Observed reward: \\(r_t\\sim P(r\\mid a_t)\\).\nGoal: Decide a sequence of actions that maximizes cumulative reward \\[\\sum_{t=1}^T r_t\\]\nAssumption: Reward depends only on the action taken. i.e., it is i.i.d. with expectation \\(\\mu(a)\\) for \\(a=1,\\dots, k\\).\n\n\n\n\n\n\n\n\n\nExample 1 (Example: Best Arm to Pull)  \n\naction \\(1\\): \\(\\mu(a)=8\\)\naction \\(2\\): \\(\\mu(a)=12\\)\naction \\(3\\): \\(\\mu(a)=12.5\\)\naction \\(4\\): \\(\\mu(a)=11\\)\n\nAction \\(3\\) is the best since \\(\\mu(a)\\) is the highest. If we know \\(\\mu(a)\\) for \\(a\\in\\mathcal{A}\\), then always taking \\(a=3\\) givesn the highest cummulative reward.\n\n\n\n\n\nChallenge: \\(\\mu(a)\\) is unknown, the distribution is unknown.\n\n\nAction-Value Methods\n\nMain idea: learn \\(Q(a)\\approx\\mu(a)\\quad\\forall\\,a\\in\\mathcal{A}\\).\nSuppose we have \\(a_1,r_1,a_2,r_2,\\dots,a_{t-1},r_{t-1}\\).\nEstimate \\(\\mu(a)\\) for \\(a\\in\\mathcal{A}\\) as sample average: \\[\n\\begin{aligned}\nQ_t(a)&=\\dfrac{\\text{sum of rewards when action }a\\text{ is taken previously}}{\\text{number of times action }a\\text{ is taken previously}}\\\\\n&=\\dfrac{\\dsst\\sum_{i=1}^{t-1}r_1\\1\\qty{a_i=a}}{\\dsst\\underbrace{\\sum_{i=1}^{t-1}\\1\\qty{a_i=a}}_{N_t(a)}}\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nRemark 1 (Convergence of Sample Average). Sample average converges to the true value if action is taken infinitely number of times: \\[\n\\lim_{N_t(a)\\to\\infty}Q_t(a)=\\mu(a),\n\\] where \\(N_t(a)\\) is the number of times action \\(a\\) has been taken by time \\(t\\).\n\n\n\n\n\nGiven \\(Q_t(a)\\) for all \\(a\\in\\mathcal{A}\\), the greedy action \\(a_t^*=\\argmax_{a}Q_t(a)\\).\nFor the next action \\(a_t\\):\n\nif \\(a_t=a_t^*\\), we are exploiting, we kind of know which actions are good.\nif \\(a_t\\neq a_t^*\\), we are exploring, we want to collect more data to improve estimates.\n\nWe can only pick one \\(a_t\\), so we can’t do both.\nBUT, we have to do both.\nThis is the key dilemma in reinforcement learning: Exploration vs. Exploitation.\n\nExploration: gain more information about value of each action.\nExploitation: Revisit actions that are already known to given high rewards.\n\nFirst attempt: Naïve approach:\n\nExploration phase: Try each arm \\(100\\) times.\nEstimate their value as \\(Q(a)\\) for \\(a\\in\\mathcal{A}\\).\nExploitation phase: Always pick \\(a^*=\\argmax_a Q(a)\\).\nProblem:\n\nIs \\(100\\) times too much? e.g., negtive rewards \\(10\\) consecutive times\nIs \\(100\\) times enough? \\(Q(a)\\to\\mu(a)\\) in the limit of infinite samples. We should never stop exploring.\n\n\nSecond attempt: \\(\\epsilon\\)-Greedy Action Selection:\n\nThis ia simple, effective way to balance exploration and exploitation.\n\n\n\n\n\\begin{algorithm} \\caption{$\\epsilon$-Greedy Action Selection} \\begin{algorithmic} \\State{Initialize $\\begin{cases}Q(a)=0\\\\N(a)=0\\end{cases}$ for $a=1,\\dots,k$} \\For{$t=1,2,3,\\dots$} \\State{$a_t=\\begin{cases}\\dsst\\argmax_a Q(a)&\\text{with probability }1-\\epsilon\\longrightarrow\\text{exploitation}\\\\\\text{random action}&\\text{with probability }\\epsilon\\longrightarrow\\text{exploration}\\end{cases}\\qquad$} \\# breaks ties randomly \\State{Take action $a_t$, receive reward $r_t$} \\State{update: $\\begin{cases}N(a_t)\\gets N(a_t)+1\\\\Q(a_t)\\gets Q(a_t)+\\dfrac{1}{N(a_t)}\\qty\\Big[r_t-Q(a_t)]\\end{cases}\\qquad$} \\# incremental implementation of sample average \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\n\nRemark 2 (\\(\\epsilon\\) Controls the Exploration Rate). \n\nLarger \\(\\epsilon\\), explore more, learn faster, converge to suboptimal.\nSmaller \\(\\epsilon\\), explore less, learn slower, converge to near-optimal.\n\n\n\n\n\n\n\\(\\epsilon\\)-greedy explores forever, and we have constant exploration rate.\n\nShould we explore forever?\nYes! But perhaps explore less as time goes by.\nKey idea: “Optimism in the face of uncertainty”\n\nThe more uncertain we are about an action, the more important it is to explore that action.\n\n\nOptimistic initialization:\n\nInitialize \\(Q(a)\\) to some positive value\nEncourage exploration initially, naturally gets “washed out” as we collect data.\nThis method is somewhat hacky, but it works.\n\n\n\n\n\\begin{algorithm} \\caption{Optimistic Initialization (Somewhat Hacky)} \\begin{algorithmic} \\State{Initialize $\\begin{cases}Q(a)=\\textcolor{red}{Q_0},&\\text{where }Q_0&gt;0\\\\N(a)=0\\end{cases}$ for $a=1,\\dots,k\\qquad$} \\# Larger $Q_0$, more exploration \\For{$t=1,2,3,\\dots$} \\State{$a_t=\\dsst\\argmax_{a}Q(a)\\qquad$} \\# breaks ties randomly \\State{Take action $a_t$, receive reward $r_t$} \\State{update: $\\begin{cases}N(a_t)\\gets N(a_t)+1\\\\Q(a_t)\\gets Q(a_t)+\\dfrac{1}{N(a_t)}\\qty\\Big[r_t-Q(a_t)]\\end{cases}\\qquad$} \\# incremental implementation of sample average \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\nUpper Confidence Bound (UCB) Action Selection:\n\nSelect actions greedily based on potential of being the best\nEstimate \\(Q(a)+\\text{Uncertainty }N(a)\\).\n\nUCB Score: \\[Q_t(a)+c\\sqrt{\\dfrac{\\ln(t)}{N_t(a)}},\\quad c&gt;0,\\] where\n\n\\(Q_t(a)\\) is the sample average estimate\n\\(\\dsst c\\sqrt{\\dfrac{\\ln(t)}{N_t(a)}}\\) is the exploration bonus\n\\(c\\) is the degree of exploration. Larger \\(c\\), more exploration.\n\nAs \\(N_t(a)\\) increases, more certain, explore less that action.\nAs \\(t\\) increases, but \\(N_t(a)\\) doesn’t increase, explore more of that action.\n\n\n\n\n\\begin{algorithm} \\caption{Upper Confidence Bound (UCB) Action Selection} \\begin{algorithmic} \\State{Initialize $\\begin{cases}Q(a)=0\\\\N(a)=0\\end{cases}$ for $a=1,\\dots,k\\qquad$} \\For{$t=1,2,3,\\dots$} \\State{$a_t=\\dsst\\argmax_{a}\\mqty[\\textcolor{red}{Q_t(a)+c\\sqrt{\\dfrac{\\ln(t)}{N_t(a)}}}]\\qquad$} \\# breaks ties randomly \\State{Take action $a_t$, receive reward $r_t$} \\State{update: $\\begin{cases}N(a_t)\\gets N(a_t)+1\\\\Q(a_t)\\gets Q(a_t)+\\dfrac{1}{N(a_t)}\\qty\\Big[r_t-Q(a_t)]\\end{cases}\\qquad$} \\# incremental implementation of sample average \\EndFor \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "notes/cs334/13-Reinforcement-Learning/13-Reinforcement-Learning.html#example-best-arm-to-pull",
    "href": "notes/cs334/13-Reinforcement-Learning/13-Reinforcement-Learning.html#example-best-arm-to-pull",
    "title": "13 Reinforcement Learning",
    "section": "Example: Best Arm to Pull",
    "text": "Example: Best Arm to Pull\n\naction \\(1\\): \\(\\mu(a)=8\\)\naction \\(2\\): \\(\\mu(a)=12\\)\naction \\(3\\): \\(\\mu(a)=12.5\\)\naction \\(4\\): \\(\\mu(a)=11\\)\n\nAction \\(3\\) is the best since \\(\\mu(a)\\) is the highest. If we know \\(\\mu(a)\\) for \\(a\\in\\mathcal{A}\\), then always taking \\(a=3\\) givesn the highest cummulative reward."
  },
  {
    "objectID": "notes/cs334/14-Recommender-Systems/14-Recommender-Systems.html#nearest-neighbors-prediction",
    "href": "notes/cs334/14-Recommender-Systems/14-Recommender-Systems.html#nearest-neighbors-prediction",
    "title": "14 Recommender Systems",
    "section": "Nearest Neighbors Prediction",
    "text": "Nearest Neighbors Prediction\n\n\\(KNN(a,i)\\): \\(k\\) nearest neighbors\n\nThe \\(k\\) most similar users to \\(a\\), who has rated movie \\(i\\). \\[\\hat Y_{a,i}=\\dfrac{1}{\\abs{KNN(a,i)}}\\sum_{b\\in KNN(a,i)}Y_{b,i}\\]\nHow to find \\(KNN\\)? Correlation. Also, we can consider an average correction.\nPro: Interpretable, easy to implement.\nCon: Slow."
  },
  {
    "objectID": "notes/cs334/14-Recommender-Systems/14-Recommender-Systems.html#matrix-factorization",
    "href": "notes/cs334/14-Recommender-Systems/14-Recommender-Systems.html#matrix-factorization",
    "title": "14 Recommender Systems",
    "section": "Matrix Factorization",
    "text": "Matrix Factorization\n\nNotation: \\[Y:n\\times m,\\] where \\(Y\\) is the ratings, \\(n\\) is the number of users, and \\(m\\) is the number of items.\nProblem: missing entries: not all \\(Y_{a,i}\\)’s are observed. \\(\\implies\\) Matrix completion problem:\n\nFill out the missing entries\nPredict the unkonwn ratings.\n\n\n\nFirst Attempt\n\nLet \\(\\hat Y\\) represent the approximation of the true, unerlying rating matrix. Formulate a regression problem with regularization: \\[J(\\hat Y)=\\dfrac{1}{2}=\\sum_{a,i\\in D}\\qty(Y_{ai}-\\hat Y_{ai}^2+\\dfrac{\\lambda}{2}\\sum_{a,i}\\hat Y_{ai}^2),\\] where \\(D\\) is the set of observed data and the regularization term is added to all data.\nObjective: find \\(\\hat Y\\) that minimizes \\(J(\\hat Y)\\). FOC gives us \\[\\pdv{J}{\\hat Y_{ai}}=-\\1\\qty{(a,i)\\in D}\\qty(Y_{ai}-\\hat Y_{ai})+\\lambda\\hat Y_{ai}\\overset{\\text{set}}{=}0.\\]\n\nIf \\((a,i)\\notin D\\): unobserved entries: \\[\\hat Y_{ai}=0\\quad\\text{constantly predicting }0\\]\nIf \\((a,i)\\in D\\): observed data: \\[\n\\begin{aligned}\n-\\qty(Y_{ai}-\\hat Y_{ai})+\\lambda\\hat Y_{ai}&=0\\\\\n\\hat Y_{ai}&=\\dfrac{Y_{ai}}{1+\\lambda}\\quad\\longrightarrow\\text{shrinking real values}\n\\end{aligned}\n\\] So, we have a trivial solution that is not useful.\n\nProblem: without constraints, we can set each entry independently.\nIdea: impose a constraint such that row/column are linearly dependent.\n\nUse a low-rank approximation via matrix factorization (constraint on rank).\n\n\n\n\nLow-Rank Approximation\n\\[\nY\\text{ is }n\\times m\\qquad\\xrightarrow{\\quad\\text{approximation}\\quad} \\hat Y=UV^\\top,\\quad\\text{where }U\\in\\R^{n\\times k}, V\\in\\R^{m\\times d}.\n\\]\n\n\\(\\rank(\\hat Y)=\\min\\qty{\\rank(U),\\rank(V)}\\).  If we choose \\(d\\in\\min\\qty{m,n}\\), then \\(\\rank(\\hat Y)=d\\).  So, \\(\\hat Y\\) is not full rank, and entries of \\(\\hat Y\\) are not linearly independent.\n**Goal: Learn \\(U\\) and \\(V\\) such that \\(\\hat Y\\) is a good approximation of \\(Y\\).\nNotation:  Let \\(\\va u^{(a)}\\in\\R^d\\) be the \\(a\\)-th row of \\(U\\): \\[U=\\mqty[-&\\va u^{(1)\\top}&-\\\\&\\vdots\\\\-&\\va u^{(n)^\\top}&-].\\] Let \\(\\va v^{(i)}\\in\\R^d\\) be the \\(i\\)-th row of \\(V\\): \\[V=\\mqty[|&&|\\\\\\va v^{(1)}&\\cdots&\\va v^{(m)}\\\\|&&|].\\] Then, \\[\\mqty[UV^\\top]_{a,i}=\\va u^{(a)}\\cdot\\va v^{(i)}=\\hat Y_{a,i}.\\]\nThen, the new objective function is \\[\n\\begin{aligned}\nJ(U,V)&=\\dfrac{1}{2}\\sum_{(a,i)\\in D}\\qty(Y_{ai}-\\mqty[UV^\\top]_{a,i})^2+\\dfrac{\\lambda}{2}\\sum_{a=1}^n\\sum_{k=1}^dU_{ak}^2+\\dfrac{\\lambda}{2}\\sum_{i=1}^m\\sum_{k=1}^dV_{i,k}^2\\\\\n&=\\dfrac{1}{2}\\sum_{(a,i)\\in D}\\qty(Y_{ai}-\\va u^{(a)}\\cdot\\va v^{(i)})^2+\\dfrac{\\lambda}{2}\\sum_{a=1}^n\\norm{\\va u^{(a)}}^2+\\dfrac{\\lambda}{2}\\sum_{i=1}^m\\norm{\\va v^{(i)}}^2.\n\\end{aligned}\n\\] Optimization problem: \\[\\min_{U,V}J(U,V).\\]\n\nIf \\(d=\\min\\qty{m,n}\\), then \\(\\hat Y\\) is full rank.  We are reduced to the trivial solution since each element can be chosen independently.\nThe smaller \\(d\\), the more constrained the problem becomes.  Both \\(d\\) and \\(\\lambda\\) are hyperparameters.\n\nHow do we solve for this? Symmetry\n\nIf we know \\(U\\), we can solve \\(V\\). (\\(U\\) is feature and \\(V\\) is parameter).\nIf we know \\(V\\), we can solve \\(U\\).\nAlternative minimization.\n\n\n\n\n\\begin{algorithm} \\caption{Alternative Minimization} \\begin{algorithmic} \\State{(0) Initialize movie feature vectors randomly $\\va v^{(1)},\\va v^{(2)},\\dots,\\va v^{(m)}$.} \\While{not converged} \\State{\\# Stop when $U$ and $V$ does not change} \\State{(1) Fix $\\va v^{(1)},\\dots,\\va v^{(m)}$, solve for $\\va u^{(1)},\\dots,\\va u^{(n)}$:} \\State{$\\qquad\\dsst\\min_{\\va u^{(a)}}\\dfrac{1}{2}\\sum_{i\\in D_a}\\qty(Y_{ai}-\\va u^{(a)}\\cdot\\va v^{(i)})^2+\\dfrac{\\lambda}{2}\\norm{\\va u^{(a)}}^2$.} \\# $D_a$: all movies rated by user $a$; ridge regression \\State{Do this for each $a=1,\\dots,n$, we have} \\State{$\\qquad\\va u^{(a)},\\dots,\\va u^{(n)}$.} \\State{(2) Fix $\\va u^{(1)},\\dots,\\va u^{(n)}$, solve for $\\va v^{(1)},\\dots,\\va v^{(m)}$.} \\State{$\\qquad\\dsst\\min_{\\va v^{(i)}}\\dfrac{1}{2}\\sum_{a\\in D_i}\\qty(Y_{ai}-\\va u^{(a)}\\cdot\\va v^{(i)})^2+\\dfrac{\\lambda}{2}\\norm{\\va v^{(i)}}^2$.} \\# $D_i$: all users rated movie $i$; ridge regression \\State{Do this for each $i=1,\\dots,m$, we get} \\State{$\\qquad\\va v^{(1)},\\dots,\\va v^{(m)}$.} \\EndWhile \\end{algorithmic} \\end{algorithm}\n\n\n\nTheoretical guarantees:\n\n\\(J(U,V)\\) monotonically decreases as we iterate.\nPotentially many local minima. So, initialization is important.\n\n\n\n\n\n\n\n\n\nExample 1 (Alternating Minimization Example) \\[\nY=\\mqty[5&?&7\\\\?&2&?\\\\7&1&4\\\\4&?&?\\\\?&3&6]\n\\]\n\nHyperparameters: \\(d=1\\), \\(\\lambda=1\\).\nAfter some iterations, we have \\[U=\\mqty[6,2,3,3,5]^\\top\\quad\\text{and}\\quad V=\\mqty[4,1,5]^\\top.\\]\nThen, we will update \\(u^{(1)}\\) as follows\n\n\\(\\text{error}=\\qty(Y_{1,1}-\\hat Y_{1,1})^2=\\qty(5-6\\times4)^2=19^2\\).\nFix \\(V\\), update \\(U\\). Find \\(u^{(1)}\\). \\[\\tag{Objective}\\min_{u^{(1)}}\\dfrac{1}{2}\\sum_{i\\in D_1}\\qty(Y_{1i}-u^{(1)}v^{(i)})^2+\\dfrac{1}{2}\\qty(u^{(1)})^2\\equiv J\\qty(u^{(1)})\\] \\[\n\\begin{aligned}\nJ\\qty(u^{(1)})&=\\dfrac{1}{2}\\qty[\\qty(Y_{11}-u^{(1)}v^{(1)})^2+\\qty(Y_{13}-u^{(1)}v^{(3)})^2]+\\dfrac{1}{2}\\qty(u^{(1)})^2\\\\\n&=\\dfrac{1}{2}\\qty[\\qty(5-4u^{(1)})^2+\\qty(7-5u^{(1)})^2]+\\dfrac{1}{2}\\qty(u^{(1)})^2\n\\end{aligned}\n\\] So, the FOC gives us \\[\n\\begin{aligned}\n\\dv{u^{(1)}}J\\qty(u^{(1)})=-4\\qty(5-4u^{(1)})-5\\qty(7-5u^{(1)})+u^{(1)}&=0\\\\\n-20+16u^{(1)}-35+25u^{(1)}+u^{(1)}&=0\\\\\n42u^{(1)}&=55\\\\\nu^{(1)}&=\\dfrac{55}{42}\\approx 1.31\n\\end{aligned}\n\\]\nHence, the new \\(\\hat Y\\) is \\[\\hat Y_{11}=u^{(1)}\\cdot v^{(1)}=\\dfrac{55}{42}\\times4=\\dfrac{110}{21}\\approx5.24.\\] Then, the new error is \\[\\text{error}=\\qty(Y_{11}-\\hat Y_{11})=\\qty(5-5.24)^2\\approx0.057\\quad\\rightarrow\\text{better!}\\]"
  },
  {
    "objectID": "notes/cs334/15-Clustering/15-Clustering.html#partitional-clustering",
    "href": "notes/cs334/15-Clustering/15-Clustering.html#partitional-clustering",
    "title": "15 Clustering",
    "section": "Partitional Clustering",
    "text": "Partitional Clustering\n\nBasic idea: group similar points.\nGoal: Given a set of examples \\(D=\\qty{\\va x^{(i)}}_{i=0}^N\\), \\(\\va x^{(i)}\\in\\R^d\\).\n\nAssign similar points to the same clusters, and\nAssign dissimilar points to different clusters.\n\nMeasure dissmilarity: Euclidean distance: \\[\\norm{\\va x^{(i)}-\\va x^{(j)}}^2.\\]\nNotation:\n\n\\(C\\): cluster, a set of point indices\n\\(\\va z^{(j)}\\): representative example, cluster centroid.\n\nPartition \\(\\R^d\\) into \\(k\\) convex cells.\n\\(z_j=\\dfrac{1}{\\abs{C_j}}\\sum_{i\\in C_j}\\va x^{(i)}\\).\n\\(C_j=\\qty{i=1,\\dots,N,\\quad\\text{where the closest representative example to }\\va x^{(i)}\\text{ is }\\va z^{(j)}}\\)\n\nGiven \\(C_j\\), we can find \\(z_j\\), and vice versa.\n\n\n\n\n\n\n\n\nFigure 1: Cluster\n\n\n\n\nObjective: Intertia/Within-Cluster Sum of Square/Intra-Cluster Distance \\[\n\\min_{\\textcolor{blue}{\\substack{C_1,\\dots,C_k\\\\\\va z^{(1)},\\dots,\\va z^{(k)}}}}\\textcolor{red}{\\sum_{j=1}^k}\\textcolor{teal}{\\sum_{i\\in C_j}}\\norm{\\va x^{(i)}-\\va z^{(j)}}^2\n\\]\n\n\\(\\dsst\\min_{\\textcolor{blue}{\\substack{C_1,\\dots,C_k\\\\\\va z^{(1)},\\dots,\\va z^{(k)}}}}\\): Find clusters and their centroids.\n\\(\\dsst\\textcolor{red}{\\sum_{j=1}^k}\\): Sum over all clusters.\n\\(\\dsst\\textcolor{teal}{\\sum_{i\\in C_j}}\\): Sum over points in each cluster.\nThis problem is NP-hard.\nWe have efficient heuristics to find local optimum.\n\n\n\n\n\\begin{algorithm} \\caption{K-Means Clustering (Alternating Minimization)} \\begin{algorithmic} \\State{Initialize $\\va z^{(1)},\\dots,\\va z^{(k)}$ randomly.} \\While{not converged} \\For{$j=1,\\dots,k$} \\State{$C_j=\\qty{i=1,\\dots,N,\\text{ where the closest representative example for }\\va x^{(i)}\\text{ is }\\va z^{(j)}}$} \\EndFor \\For{$j=1,\\dots,k$} \\State{$\\dsst\\va z^{(j)}=\\dfrac{1}{\\abs{C_j}}\\sum_{i\\in C_j}\\va x^{(i)}$} \\EndFor \\EndWhile \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\n\nExample 1 (K-Means Example)  \n\n\n\n\n\n\nFigure 2: K Means\n\n\n\n\n\\(k=2\\)\nInitialization: \\(\\textcolor{blue}{\\va z^{(1)}=(3,5)}\\) and \\(\\textcolor{red}{\\va z^{(2)}=(1,1)}\\)\nFix \\(\\va z^{(j)}\\). Make assignment:\n\n\n\n\n\n\n\n\n\n\n\n\nPoint\n\\(L^2\\) Distance to \\(\\textcolor{blue}{\\va z^{(1)}}\\)\nto \\(\\textcolor{red}{\\va z^{(2)}}\\)\nAssignment\n\n\n\n\n1\n\\((0,5)\\)\n\\(3\\)\n\\(\\sqrt{17}\\)\n\\(\\textcolor{blue}{C_1}\\)\n\n\n2\n\\((2,5)\\)\n\\(1\\)\n\\(\\sqrt{17}\\)\n\\(\\textcolor{blue}{C_1}\\)\n\n\n3\n\\((1,4)\\)\n\\(\\sqrt{5}\\)\n\\(3\\)\n\\(\\textcolor{blue}{C_1}\\)\n\n\n4\n\\((2,2)\\)\n\\(\\sqrt{10}\\)\n\\(\\sqrt{2}\\)\n\\(\\textcolor{red}{C_2}\\)\n\n\n5\n\\((3,0)\\)\n\\(5\\)\n\\(\\sqrt{5}\\)\n\\(\\textcolor{red}{C_2}\\)\n\n\n6\n\\((3,2)\\)\n\\(3\\)\n\\(\\sqrt{5}\\)\n\\(\\textcolor{red}{C_2}\\)\n\n\n7\n\\((5,0)\\)\n\\(\\sqrt{29}\\)\n\\(\\sqrt{17}\\)\n\\(\\textcolor{red}{C_2}\\)\n\n\n\n\nUpdate centroid:\n\n\\(\\textcolor{blue}{\\va z^{(1)}=(1, 4.67)}\\)\n\\(\\textcolor{red}{\\va z^{(2)}=(3.25, 1)}\\)\n\nWe are converged already because redo the iteration will not change the assignment and clusters anymore.\n\n\n\n\n\n\nConvergence:\n\nClusters/centroids stop changing. If ties are not broken consistently, we may cycle. If it cycles, we know we converged.\n\\(k\\)-means is guaranteed to converge (to local optimum), but we may not necessarily converge to the global optimum.\n\nInitialization:\n\nRandomly\n\\(k\\)-means\\(++\\): accounts for distribution of \\(\\va x\\) in \\(\\R^d\\). This approach tries to “spread it out” more.\nIn general, we repeat \\(k\\)-means multiple times. We choose clustering solution with lowest inertia.\n\nChoosing \\(k\\):\n\nIn some applications, \\(k\\) will be given.\nUse validation data.\nThe “elbow” method: plot inertia vs. \\(k\\). Choose \\(k\\) where the curve starts to flatten out.\n\n\n\n\n\n\n\n\nFigure 3: Elbow Method\n\n\n\n\n\n\n\n\n\n\nRemark 1 (The Elbow Plot). If \\(k=N\\), every point forms its own cluster. \\[\\sum_{j=1}^N\\sum_{i\\in C_j}\\norm{\\va x^{(i)}-\\va z^{(j)}}^2=0\\implies \\va z^{(j)}=\\va x^{(i)}.\\]"
  }
]