[
  {
    "objectID": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html",
    "href": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html",
    "title": "1 Linear Classification",
    "section": "",
    "text": "feature vector: \\(\\va x=\\mqty[x_1&x_2&\\cdots&x_d]^\\top\\in\\R^d\\). \\(\\R^d\\) is called the feature space.\nlabel: \\(y\\in\\qty{-1,+1}\\), binary.\ntraining set of labeled examples: \\[D=\\qty{\\va x^{(i)},y^{(i)}}_{i=1}^N\\]\nclassifier: \\[h:\\R^d\\to\\qty{-1,+1}.\\]\n\nGola: select the best \\(h\\) from a set of possible classifiers \\(\\mathcal{H}\\) that would (the ability to generalization).\nWe will solve this goal by a learning algorithm, typically an optimization problem \\(\\wrt D\\)."
  },
  {
    "objectID": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html#machine-learning-terminology",
    "href": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html#machine-learning-terminology",
    "title": "1 Linear Classification",
    "section": "",
    "text": "feature vector: \\(\\va x=\\mqty[x_1&x_2&\\cdots&x_d]^\\top\\in\\R^d\\). \\(\\R^d\\) is called the feature space.\nlabel: \\(y\\in\\qty{-1,+1}\\), binary.\ntraining set of labeled examples: \\[D=\\qty{\\va x^{(i)},y^{(i)}}_{i=1}^N\\]\nclassifier: \\[h:\\R^d\\to\\qty{-1,+1}.\\]\n\nGola: select the best \\(h\\) from a set of possible classifiers \\(\\mathcal{H}\\) that would (the ability to generalization).\nWe will solve this goal by a learning algorithm, typically an optimization problem \\(\\wrt D\\)."
  },
  {
    "objectID": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html#linear-classifer-through-origin",
    "href": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html#linear-classifer-through-origin",
    "title": "1 Linear Classification",
    "section": "Linear Classifer (Through Origin)",
    "text": "Linear Classifer (Through Origin)\n\n\n\n\n\n\n\nDefinition 1 (Thresholded Linear Mapping from Feature Vectors to Labels) \\[\nh(\\va x; \\va\\theta)=\\begin{cases}+1\\quad\\text{if }\\va\\theta\\cdot\\va x&gt;0,\\\\-1\\quad\\text{if }\\va\\theta\\cdot\\va x&lt;0\\end{cases},\n\\] where \\(\\va\\theta\\in\\R^d\\) is the parameter vector, and \\(\\va\\theta=\\mqty[\\theta_1,\\theta_2,\\dots,\\theta_d]^\\top\\).\nOne can also write it using the \\(\\operatorname{sign}\\) function: \\[\nh(\\va x;\\va\\theta)=\\operatorname{sign}(\\va\\theta\\cdot\\va x)=\\begin{cases}\n+1\\quad\\text{if }\\va\\theta\\cdot\\va x&gt;0,\\\\\n0\\quad\\text{if }\\va\\theta\\cdot\\va x=0,\\\\\n-1\\quad\\text{if }\\va\\theta\\cdot\\va x&lt;0.\n\\end{cases}.\n\\]\n\n\n\n\n\nRecall: dot product: \\[\n\\va\\theta\\cdot\\va x=\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_dx_d=\\sum_{j=1}^d\\theta_jx_j,\n\\] a linear combination of input features.\nIn \\(h(\\va x;\\va\\theta)\\), different \\(\\va\\theta\\)’s produce (potentially) different labelings for the same \\(\\va x\\).\n\n\nGraphical Representation\n\n\n\n\n\n\nFigure 1: Decision Boundary\n\n\n\n\nHowever, what happens on this \\(90^\\circ\\) line? We call this line the decision boundary, which separates the two classes. Recall: \\[\n\\va\\theta\\cdot\\va x=\\norm{\\va\\theta}\\cdot\\norm{\\va x}\\cdot\\cos 90^\\circ=0.\n\\]\nView the decision boundary as a hyperplane in \\(\\R^d\\).\n\nDoes the length of \\(\\va\\theta\\) matter? No.\nDoes the direction of \\(\\va\\theta\\) matter? Yes."
  },
  {
    "objectID": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html#linear-classifier-with-offset",
    "href": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html#linear-classifier-with-offset",
    "title": "1 Linear Classification",
    "section": "Linear Classifier with Offset",
    "text": "Linear Classifier with Offset\n\n\n\n\n\n\n\nDefinition 2 (Linear Classifier with Offset) \\[\nh(\\va x;\\va\\theta,b)=\\operatorname{sign}(\\va\\theta\\cdot\\va x+b),\n\\] where \\(\\va x\\in\\R^d\\), \\(\\va\\theta\\in\\R^d\\), and \\(b\\in\\R\\). \\(b\\) is called the offset or intercept.\n\n\n\n\n\nGraphical Representation\n\n\n\n\n\n\nFigure 2: Linear Classifier with Offset\n\n\n\n\nNote that the signed distance from \\(\\va\\theta\\cdot\\va x=0\\) to the hyperplane \\(\\va\\theta\\cdot\\va x+b=0\\) is given by: \\[\n\\dfrac{-b}{\\norm{\\va\\theta}}.\n\\]\n\n\nProof. \n\nPick a point on old decision boundary \\(\\va x^{(1)}\\) satisfies \\[\\va\\theta\\cdot\\va x^{(1)}=0.\\]\nPick a point on the new decision boundary \\(\\va x^{(2)}\\) satisfies \\[\\va\\theta\\cdot\\va x^{(2)}+b=0\\implies\\va\\theta\\va x^{(2)}=-b.\\]\nLet \\(\\va v=\\va x^{(2)}-\\va x^{(1)}.\\)\nNow, project \\(\\va v\\) into direction of \\(\\va\\theta\\): \\[\n\\operatorname{proj}_{\\va\\theta}\\va v=\\qty(\\va v\\cdot\\dfrac{\\va\\theta}{\\norm{\\va\\theta}})\\dfrac{\\va\\theta}{\\norm{\\va\\theta}}.\n\\] Note: \\(\\dfrac{\\va\\theta}{\\norm{\\va\\theta}}\\) is the unit vector in the direction of \\(\\va\\theta\\).\nTherefore, the signed sitance is given by: \\[\n\\va v\\cdot\\dfrac{\\va\\theta}{\\norm{\\va\\theta}}=\\dfrac{\\qty(\\va x^{(2)}-\\va x^{(1)})\\cdot\\va\\theta}{\\norm{\\va\\theta}}=\\dfrac{\\va x^{(2)}\\cdot\\va\\theta-\\va x^{(1)}\\cdot\\va\\theta}{\\norm{\\va\\theta}}=\\dfrac{-b}{\\norm{\\va\\theta}}.\n\\]"
  },
  {
    "objectID": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html#training-error",
    "href": "notes/cs334/01-Linear-Classification/01-Linear-Classification.html#training-error",
    "title": "1 Linear Classification",
    "section": "Training Error",
    "text": "Training Error\n\nIntuition: we want \\(\\va\\theta\\) that works well on training data \\(D\\).\n\n\n\n\n\n\n\n\nRemark 1. We’ve restricted the class of possible clasassifiers to linear classifiers, reducing the chance of overfitting.\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3 (Training Error and Learning Algorithms) The training error (\\(\\epsilon\\)) is the fraction of training examples for which the classifier produces wrong labels: \\[\n\\epsilon_N(\\va \\theta)=\\dfrac{1}{N}\\sum_{i=1}^N\\1\\qty{y^{(i)}\\neq h\\qty(\\va x^{(i)};\\va\\theta)},\n\\] where \\(\\1\\{\\cdot\\}\\) returns \\(1\\) if true and \\(0\\) if false.\nAn equivalent form is: \\[\n\\epsilon_N(\\va \\theta)=\\dfrac{1}{N}\\sum_{i=1}^N\\1\\{{\\color{orange}{\\underbrace{y^{(i)}\\qty(\\va\\theta\\cdot\\va x^{(i)})}_{\\substack{y^(i)\\text{ and }\\va\\theta\\cdot\\va x^{(i)}\\\\\\text{ have opposite signs}}}}}{\\color{green}{\\overbrace{\\leq 0}^{\\substack{\\text{points on the decision boundary}\\\\\\text{are considered misclassified}}}}}\\}\n\\]\n\n\n\n\n\nGoal: Find \\(\\displaystyle\\va\\theta^*=\\argmin_{\\va\\theta}\\epsilon_N\\qty(\\va\\theta)\\).\nHow:\n\nIn general, this is not easy to solve (it’s NP-hard).\nFor now, we will consider a special case: linearly separable data.\n\n\n\n\n\n\n\n\n\nDefinition 4 (Linear Separable) Training examples \\(D=\\qty{\\va x^{(i)}, y^{(i)}}_{i=1}^N\\) are linearly separable through the origin if \\(\\exists\\ \\va\\theta\\) such that \\[\ny^{(i)}\\qty(\\va\\theta\\cdot\\va x^{(i)})&gt;0\\quad\\forall\\ i=1,\\dots, N\n\\]\n\n\n\n\n\n\n\n\n\n\n\nRemark 2. This assumption of linear separability is NOT testable.\n\n\n\n\n\nPerceptron Algorithm\n\nThe perceptron algorithm is a mistaken-driven algorithm. It starts with \\(\\va\\theta=\\va 0\\) (the zero vector), and then tries to update \\(\\va\\theta\\) to correct any mistakes.\n\n\n\n\\begin{algorithm} \\caption{Perceptron (Through Origin)} \\begin{algorithmic} \\Procedure{Perceptron}{$D=\\qty{\\va x^{(i)}, y^{(i)}}_{i=1}^N$} \\State $k=0$; $\\va\\theta^{(0)}=\\va 0$ \\While{not all points are correctly classified} \\For{$i=1,\\dots, N$} \\State \\Comment{$\\color{green}\\text{if mistake}$} \\If{$y^{(i)}\\qty(\\va\\theta^{(k)}\\cdot\\va x^{(i)})\\leq 0$} \\State $\\va\\theta^{(k+1)}=\\va\\theta^{(k)}+y^{(i)}\\va x^{(i)}$ \\State $k++$ \\EndIf \\EndFor \\EndWhile \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\n\nTheorem 1 (Existence of Perceptron Solution) The perceptron algorithm (Algorithm 1) converges after a finite number of mistkaes if the training examples are linearly separable through the origin.\n\n\n\n\n\nHowever, :\n\nSolution is not unique\nMay need to loop through the dataset more than once, or not use some points at all.\n\n\n\n\n\\begin{algorithm} \\caption{Perceptron (With Offset)} \\begin{algorithmic} \\Procedure{Perceptron}{$D=\\qty{\\va x^{(i)}, y^{(i)}}_{i=1}^N$} \\State $k=0$; $\\va\\theta^{(0)}=\\va 0$; $b^{(0)}=0$ \\While{not all points are correctly classified} \\For{$i=1,\\dots, N$} \\State \\Comment{$\\color{green}\\text{if mistake}$} \\If{$y^{(i)}\\qty(\\va\\theta^{(k)}\\cdot\\va x^{(i)}+b^{(k)})\\leq 0$} \\State $\\va\\theta^{(k+1)}=\\va\\theta^{(k)}+y^{(i)}\\va x^{(i)}$ \\State $b^{(k+1)}=b^{(k)}+y^{(i)}$ \\State $k++$ \\EndIf \\EndFor \\EndWhile \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\nProof. Produce augmented vecotrs: \\[\\va x'=\\mqty[1, \\va x]^\\top\\quad\\text{and}\\quad\\va\\theta'=\\mqty[b,\\va\\theta]^\\top.\\] Then, we have implicit offset formula: \\[\\va\\theta'\\cdot\\va x'=b+\\va\\theta\\cdot\\va x.\\] Apply Algorithm 1 to \\(\\va x'\\) and \\(\\va\\theta'\\): \\[\n\\begin{aligned}\n\\va\\theta'^{(k+1)}&=\\va\\theta'^{(k)}+y^{(i)}\\va x'^{(i)}\\\\\n\\mqty[b^{(k+1)},\\va\\theta^{(k+1)}]&=\\mqty[b^{(k)},\\va\\theta^{(k)}]+y^{(i)}\\mqty[1,\\va x^{(i)}]\\\\\n\\implies \\va\\theta^{(k+1)}&=\\va\\theta^{(k)}+y^{(i)}\\va x^{(i)}\\\\\nb^{(k+1)}&=b^{(k)}+y^{(i)}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "notes/cs377.html",
    "href": "notes/cs377.html",
    "title": "CS 377 Database Systems",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n1 The Relational Model\n\n\n\n\n\n\nDatabase\n\n\nRelational Model\n\n\n\nThis lecture discusses the relational model, which is the foundation of modern database systems.\n\n\n\n\n\nAug 28, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n2 Relational Algebra\n\n\n\n\n\n\nDatabase\n\n\nRelational Model\n\n\nRelational Algebra\n\n\n\nThis lecture discusses the relational algebra, which is the foundation of modern database systems. Topics include select, project, and join operators.\n\n\n\n\n\nSep 8, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n3 SQL Introduction\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\nDatabase\n\n\n\nThis note introduces SQL, the Structured Query Language, which is used to interact with databases. We will cover basic queries, the use of *, AS, conditions, and ORDER BY in SQL.\n\n\n\n\n\nSep 10, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n4 SQL Aggregation\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\nDatabase\n\n\nAggregation\n\n\n\nThis lecture discusses SQL Aggregation, including computing on a column, GROUP BY, and HAVING clauses.\n\n\n\n\n\nSep 14, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n5 SQL Set Operations\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\nDatabase\n\n\nSet Operations\n\n\n\nThis lecture discusses SQL set operations, including UNION, INTERSECT, and EXCEPT. It also covers the difference between bag and set semantics in SQL.\n\n\n\n\n\nSep 18, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n6 SQL Join\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\nDatabase\n\n\nJoin\n\n\n\nThis lecture discusses the different types of joins in SQL, including inner, outer, and cross joins. It also covers the dangers of using NATURAL JOIN and the best practices for using joins in SQL.\n\n\n\n\n\nSep 28, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n7 SQL NULL\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\nDatabase\n\n\nJoin\n\n\nNULL\n\n\n\nThis lecture discusses the concept of NULL values in SQL, including how to represent missing information and inapplicable attributes. It also covers how to check for NULL values and the impact of NULL values on arithmetic expressions, comparison operators, and aggregation.\n\n\n\n\n\nOct 8, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n8 SQL Subqueries\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\nDatabase\n\n\nSubqueries\n\n\n\nThis lecture discusses subqueries in SQL, including subqueries in a FROM clause, subqueries in a WHERE clause, and the scope of subqueries. It also covers special cases of subqueries, such as when the subquery returns NULL or multiple values.\n\n\n\n\n\nOct 18, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n9 SQL DDL\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\nDatabase\n\n\nDDL\n\n\n\nThis lecture discusses Database Modification Language in SQL, including Insert, Delete, Update, and Create operations. It also covers SQL Schemas, Types, Keys, and Foreign Keys.\n\n\n\n\n\nOct 28, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n10 JDBC\n\n\n\n\n\n\nCoding\n\n\nSQL\n\n\nDatabase\n\n\nJava\n\n\nJDBC\n\n\n\nThis lecture discusses how to embed SQL in Java using JDBC. It covers the JDBC API, SQL Injection, and Prepared Statements.\n\n\n\n\n\nOct 30, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n11 ER Design\n\n\n\n\n\n\nDatabase\n\n\nDatabase Design\n\n\nER Design\n\n\n\nThis lecture discusses Entity-Relationship (ER) design, which is a technique for designing databases. It covers the ER model, ER diagrams, and the process of converting ER diagrams to relational schemas.\n\n\n\n\n\nNov 10, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n12 Database Design Theory: Normalization\n\n\n\n\n\n\nDatabase\n\n\nDatabase Design\n\n\nDB Design Theory\n\n\nBCNF\n\n\nNormalization\n\n\nFunctional Dependencies\n\n\nClosure Test\n\n\nFD Projection\n\n\n\nThis lecture discusses the concept of normalization in database design theory. The lecture covers functional dependencies, closure test, and FD projection. It finally introduces the concept of BCNF.\n\n\n\n\n\nNov 18, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n13 NOSQL: Not Only SQL\n\n\n\n\n\n\nDatabase\n\n\nNOSQL\n\n\n\nThis lecture introduces the concept of NOSQL databases and their applications.\n\n\n\n\n\nDec 4, 2024\n\n\nJiuru Lyu\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "CV",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes/nonlinearOpt.html",
    "href": "notes/nonlinearOpt.html",
    "title": "Nonlinear Optimization",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes/Calc3.html",
    "href": "notes/Calc3.html",
    "title": "Multivariable Calculus/Calculus 3",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes/NumericalAnalysis2.html",
    "href": "notes/NumericalAnalysis2.html",
    "title": "PhD-Level Numerical Analysis I",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes/ODE.html",
    "href": "notes/ODE.html",
    "title": "Ordinary Differential Equations",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes/cs377/05-sql-set-operations/05-SQL-Set-Operations.html",
    "href": "notes/cs377/05-sql-set-operations/05-SQL-Set-Operations.html",
    "title": "5 SQL Set Operations",
    "section": "",
    "text": "A table can have duplicate tuples, unless this would violate an integrity constraint.\nAnd SELECT-FROM-WHERE (SFW) statements leave duplicates in, unless you say not to!\nWhy?\n\nGetting rid of duplicates is expensive!\nWe may want the duplicates because they tell us how many times something occurred.\n\n\n\n\n\nSQL treats tables as “bags” (or “multisets”) rather than sets.\nBags are just like sets, but duplicates are allowed.\n\n\n\n\n\n\n\nTip 1: Example: Bag Semantics\n\n\n\n\n\n\n\\(\\{6, 2, 7, 1, 9\\}\\) is a set and a bag\n\\(\\{6, 2, 7, 1, 9, 1\\}\\) is not a set but a bag\n\n\n\n\n\nLet sets, order doesn’t matter: \\(\\{6, 2, 7, 1, 9, 1\\}=\\{1, 1, 2, 6, 7, 9\\}\\)\nOperations \\(\\cap\\), \\(\\cup\\), and \\(-\\) with bags:\n\nFor \\(\\cap\\), \\(\\cup\\), and \\(-\\) the number of occurrences of a tuple in the result requires some thought.\nSuppose tuple \\(t\\) occurs:\n\n\\(m\\) times in relation \\(R\\), and\n\\(n\\) times in relation \\(S\\)\n\n\n\n\n\n\nOperation\nNumber of Occurrences of \\(t\\) in tuples\n\n\n\n\n\\(R\\cap S\\)\n\\(\\min(m,n)\\)\n\n\n\\(R\\cup S\\)\n\\(m+n\\)\n\n\n\\(R-S\\)\n\\(\\max(m-n,0)\\)\n\n\n\n\n\\(\\cap\\), \\(\\cup\\), and \\(-\\) in SQL:\n\n(&lt;subquery&gt;) UNION (&lt;subquery&gt;)\n(&lt;subquery&gt;) INTERSECT (&lt;subquery&gt;)\n(&lt;subquery&gt;) EXCEPT (&lt;subquery&gt;)\n\nThe parentheses () are mandatory\nThe operands must be queries; you can’t simply use a relation name.\n\n\n\n\n\n\n\nTip 2: Example: Set Operations in SQL\n\n\n\n\n\n(SELECT sid\n FROM Took\n WHERE grade &gt; 95)\n    UNION\n(SELECT sid\n FROM Took\n WHERE grade &lt; 50);\n\n\n\n\nBag vs. Set Semantics: which is used and when\n\nA SELECT-FROM-WHERE statement uses bag semantics by default.\n\nDuplicates are kept in the result\n\nThe set (INTERSECT/UNION/EXCEPT) operations use set semantics by default\n\nDuplicates are eliminated from the result\n\n\nMotivation: Efficiency\n\nWhen doing projection, it is easier not to eliminate duplicate\n\nJust work one tuple at a time\n\nFor intersection or difference, it is most efficient to sort the relations first.\n\nAt that point you may was well eliminate the duplicates anyway\n\n\nHowever, we can control which semantic is used.\n\nWe can force the result of a SFW query to be a set by using SELECT DISTINCT\nWe can force the result of a set operation to be a bag by using ALL.\n\n\n\n\n\n\n\n\nTip 3: Example: Force to Use Set Operations in SQL\n\n\n\n\n\n(SELECT sid\n FROM Took\n WHERE grade &gt; 95)\n    UNION ALL\n(SELECT sid\n FROM Took\n WHERE grade &lt; 50);\n\n\n\n\n\n\n\n\n\nTip 4: Example: Comparison of Set and Bag Semantics\n\n\n\n\n\n\nA single occurrence of a value for x in B wipes out all occurrences of it from A:\n\n(SELECT x FROM A) EXCEPT (SELECT x FROM B)\n\nWith EXCEPT ALL, we match up the value one by one:\n\n(SELECT x FROM A) EXCEPT (SELECT x FROM B)"
  },
  {
    "objectID": "notes/cs377/05-sql-set-operations/05-SQL-Set-Operations.html#duplicates-in-sql",
    "href": "notes/cs377/05-sql-set-operations/05-SQL-Set-Operations.html#duplicates-in-sql",
    "title": "5 SQL Set Operations",
    "section": "",
    "text": "A table can have duplicate tuples, unless this would violate an integrity constraint.\nAnd SELECT-FROM-WHERE (SFW) statements leave duplicates in, unless you say not to!\nWhy?\n\nGetting rid of duplicates is expensive!\nWe may want the duplicates because they tell us how many times something occurred.\n\n\n\n\n\nSQL treats tables as “bags” (or “multisets”) rather than sets.\nBags are just like sets, but duplicates are allowed.\n\n\n\n\n\n\n\nTip 1: Example: Bag Semantics\n\n\n\n\n\n\n\\(\\{6, 2, 7, 1, 9\\}\\) is a set and a bag\n\\(\\{6, 2, 7, 1, 9, 1\\}\\) is not a set but a bag\n\n\n\n\n\nLet sets, order doesn’t matter: \\(\\{6, 2, 7, 1, 9, 1\\}=\\{1, 1, 2, 6, 7, 9\\}\\)\nOperations \\(\\cap\\), \\(\\cup\\), and \\(-\\) with bags:\n\nFor \\(\\cap\\), \\(\\cup\\), and \\(-\\) the number of occurrences of a tuple in the result requires some thought.\nSuppose tuple \\(t\\) occurs:\n\n\\(m\\) times in relation \\(R\\), and\n\\(n\\) times in relation \\(S\\)\n\n\n\n\n\n\nOperation\nNumber of Occurrences of \\(t\\) in tuples\n\n\n\n\n\\(R\\cap S\\)\n\\(\\min(m,n)\\)\n\n\n\\(R\\cup S\\)\n\\(m+n\\)\n\n\n\\(R-S\\)\n\\(\\max(m-n,0)\\)\n\n\n\n\n\\(\\cap\\), \\(\\cup\\), and \\(-\\) in SQL:\n\n(&lt;subquery&gt;) UNION (&lt;subquery&gt;)\n(&lt;subquery&gt;) INTERSECT (&lt;subquery&gt;)\n(&lt;subquery&gt;) EXCEPT (&lt;subquery&gt;)\n\nThe parentheses () are mandatory\nThe operands must be queries; you can’t simply use a relation name.\n\n\n\n\n\n\n\nTip 2: Example: Set Operations in SQL\n\n\n\n\n\n(SELECT sid\n FROM Took\n WHERE grade &gt; 95)\n    UNION\n(SELECT sid\n FROM Took\n WHERE grade &lt; 50);\n\n\n\n\nBag vs. Set Semantics: which is used and when\n\nA SELECT-FROM-WHERE statement uses bag semantics by default.\n\nDuplicates are kept in the result\n\nThe set (INTERSECT/UNION/EXCEPT) operations use set semantics by default\n\nDuplicates are eliminated from the result\n\n\nMotivation: Efficiency\n\nWhen doing projection, it is easier not to eliminate duplicate\n\nJust work one tuple at a time\n\nFor intersection or difference, it is most efficient to sort the relations first.\n\nAt that point you may was well eliminate the duplicates anyway\n\n\nHowever, we can control which semantic is used.\n\nWe can force the result of a SFW query to be a set by using SELECT DISTINCT\nWe can force the result of a set operation to be a bag by using ALL.\n\n\n\n\n\n\n\n\nTip 3: Example: Force to Use Set Operations in SQL\n\n\n\n\n\n(SELECT sid\n FROM Took\n WHERE grade &gt; 95)\n    UNION ALL\n(SELECT sid\n FROM Took\n WHERE grade &lt; 50);\n\n\n\n\n\n\n\n\n\nTip 4: Example: Comparison of Set and Bag Semantics\n\n\n\n\n\n\nA single occurrence of a value for x in B wipes out all occurrences of it from A:\n\n(SELECT x FROM A) EXCEPT (SELECT x FROM B)\n\nWith EXCEPT ALL, we match up the value one by one:\n\n(SELECT x FROM A) EXCEPT (SELECT x FROM B)"
  },
  {
    "objectID": "notes/cs377/05-sql-set-operations/05-SQL-Set-Operations.html#views",
    "href": "notes/cs377/05-sql-set-operations/05-SQL-Set-Operations.html#views",
    "title": "5 SQL Set Operations",
    "section": "Views",
    "text": "Views\n\nThe idea\n\nA view is a relation defined in terms of stored tables (called base tables) and possibly also other views.\nAccess a view like any base table.\nTwo kinds of view:\n\nVirtual: no tuples are stored; view is just a query for constructing the relation when needed.\nMaterialized: actually constructed and stored. Expensive to maintain.\n\nViews are particularly important when you want to give different access rights (i.e. permissions) to different users viewing data in your tables!\n\n\n\n\n\n\n\nTip 5: Example: Application of Views\n\n\n\n\n\nCanvas student page vs. instructor page\n\n\n\n\n\n\n\n\n\nTip 6: Example: Creating a View\n\n\n\n\n\n\nA view for students who earned an 80 or higher in a CSC course:\n\nCREATE VIEW topresults AS\n    SELECT firstname, surname, cnum\n    FROM Student, Took, Offering\n    WHERE\n        Student.sid = Took.sid AND\n        Took.oid = Offering.oid AND\n        grade &gt;= 80 AND dept = 'CSC';\n\n\n\n\n\nUses of Views\n\nBreak down a large query\nProvide another way of looking at the same data, e.g. for one category of user\nWrap commonly used complex queries"
  },
  {
    "objectID": "notes/cs377/07-sql-null/07-SQL-NULL.html",
    "href": "notes/cs377/07-sql-null/07-SQL-NULL.html",
    "title": "7 SQL NULL",
    "section": "",
    "text": "Missing Information:\n\nMissing value.\n\nE.g., we know a student has some email address, but we don’t know what it is.\n\nInapplicable attribute.\n\nE.g., the value of attribute spouse for a person who is single.\n\n\nRepresenting missing information:\n\nOne possibility: use a special value as a placeholder. E.g.,\n\nIf age unknown, use -1.\nIf StNum unknown, use 999999999.\n\nPros and cons?\n\nBetter solution: use a value not in any domain. We call this a null value.\nTuples in SQL relations can have NULL as a value for one or more components.\n\n\nCheck for NULL values\n\nYou can compare an attribute value to NULL with\n\nIS NULL\nIS NOT NULL\n\n\n\n\n\n\n\n\n\nTip 1: Example: Check for NULL values\n\n\n\n\n\nSELECT *\nFROM Course\nWHERE breadth IS NULL;\n\n\n\n\nNote: do not use WHERE breadth = NULL;\n\n\n\n\nAssume \\(x\\) is NULL\nArithmetic expression: Result is always NULL\n\n\n\n\n\n\n\nTip 2: Example: Arithmetic with NULL\n\n\n\n\n\n\\[x+\\texttt{grade}=\\texttt{NULL}\\] \\[x*0=\\texttt{NULL}\\] \\[x-x=\\texttt{NULL}\\]\n\n\n\n\nComparison operators (\\(&gt;\\), \\(&lt;\\), \\(=\\), \\(\\dots\\)): Result is UNKNOWN (neither TRUE nor FALSE)\n\n\n\n\n\n\n\nTip 3: Example: Comparison with NULL\n\n\n\n\n\n\\[x&lt;32 \\quad\\texttt{ --&gt; UNKNOWN}\\]\n\n\n\n\nThis UNKNOWN is a truth-value\nTruth-values in SQL are: TRUE, FALSE, UNKNOWN (a 3-value truth value system!)\n\nLogic with UNKNOWN: \\[\\begin{aligned}\n\\texttt{UNKNOWN} \\lor\\texttt{FALSE}&\\equiv\\texttt{UNKNOWN}\\\\\n\\texttt{UNKNOWN} \\lor\\texttt{TRUE}&\\equiv\\texttt{TRUE}\\\\\n\\texttt{UNKNOWN} \\land\\texttt{FALSE}&\\equiv\\texttt{FALSE}\\\\\n\\texttt{UNKNOWN} \\land\\texttt{TRUE}&\\equiv\\texttt{UNKNOWN}\\\\\n\\neg\\texttt{UNKNOWN}&\\equiv\\texttt{UNKONWN}\n\\end{aligned}\\]\nA tuple is in a query result \\(\\iff\\) the result of the WHERE clause is TRUE.\n\n\n\n\n\n\n“Aggregation ignores NULL.”\n\nNULL never contributes to a sum, average, or count, and can never be the minimum or maximum of a column (unless every value is NULL).\n\nIf ALL values are NULL in a column, then the result of the aggregation is NULL.\nException: COUNT of an empty set is 0. (think of COUNT(columnName) as a function that counts the non-null values in that column.)\n\n\n\n\n\n\n\nTip 4: Example: Aggregation with NULL\n\n\n\n\n\n\nR&S&T are defined as:\n\n R             |      S             |       T\n   x           |          x         |            x\n -----         |        -----       |          -----\n NULL          |        NULL        |\n 1             |                    |\n\nCOUNT()\n\nCOUNT(R.*)=2 and COUNT(R.x)=1\nCOUNT(S.*)=1 and COUNT(S.x)=0\nCOUNT(T.*)=0 and COUNT(T.x)=0\n\nOther aggregates:\n\nMIN(R.x)=1 and MAX(R.x)=1\nMIN(S.x)=NULL and MAX(S.x)=NULL\nMIN(T.x)=NULL and MAX(T.x)=NULL\n\n\n\n\n\n\n\n\n\nNULL is treated differently by the set operators UNION, EXCEPT, and INTERSECT than it is in search conditions.\nWhen comparing rows, set operators treat NULL values as equal to each other.\nIn contrast, when NULL is compared to NULL in a search condition the result is UNKNOWN (not true)."
  },
  {
    "objectID": "notes/cs377/07-sql-null/07-SQL-NULL.html#null-values-in-sql",
    "href": "notes/cs377/07-sql-null/07-SQL-NULL.html#null-values-in-sql",
    "title": "7 SQL NULL",
    "section": "",
    "text": "Missing Information:\n\nMissing value.\n\nE.g., we know a student has some email address, but we don’t know what it is.\n\nInapplicable attribute.\n\nE.g., the value of attribute spouse for a person who is single.\n\n\nRepresenting missing information:\n\nOne possibility: use a special value as a placeholder. E.g.,\n\nIf age unknown, use -1.\nIf StNum unknown, use 999999999.\n\nPros and cons?\n\nBetter solution: use a value not in any domain. We call this a null value.\nTuples in SQL relations can have NULL as a value for one or more components.\n\n\nCheck for NULL values\n\nYou can compare an attribute value to NULL with\n\nIS NULL\nIS NOT NULL\n\n\n\n\n\n\n\n\n\nTip 1: Example: Check for NULL values\n\n\n\n\n\nSELECT *\nFROM Course\nWHERE breadth IS NULL;\n\n\n\n\nNote: do not use WHERE breadth = NULL;\n\n\n\n\nAssume \\(x\\) is NULL\nArithmetic expression: Result is always NULL\n\n\n\n\n\n\n\nTip 2: Example: Arithmetic with NULL\n\n\n\n\n\n\\[x+\\texttt{grade}=\\texttt{NULL}\\] \\[x*0=\\texttt{NULL}\\] \\[x-x=\\texttt{NULL}\\]\n\n\n\n\nComparison operators (\\(&gt;\\), \\(&lt;\\), \\(=\\), \\(\\dots\\)): Result is UNKNOWN (neither TRUE nor FALSE)\n\n\n\n\n\n\n\nTip 3: Example: Comparison with NULL\n\n\n\n\n\n\\[x&lt;32 \\quad\\texttt{ --&gt; UNKNOWN}\\]\n\n\n\n\nThis UNKNOWN is a truth-value\nTruth-values in SQL are: TRUE, FALSE, UNKNOWN (a 3-value truth value system!)\n\nLogic with UNKNOWN: \\[\\begin{aligned}\n\\texttt{UNKNOWN} \\lor\\texttt{FALSE}&\\equiv\\texttt{UNKNOWN}\\\\\n\\texttt{UNKNOWN} \\lor\\texttt{TRUE}&\\equiv\\texttt{TRUE}\\\\\n\\texttt{UNKNOWN} \\land\\texttt{FALSE}&\\equiv\\texttt{FALSE}\\\\\n\\texttt{UNKNOWN} \\land\\texttt{TRUE}&\\equiv\\texttt{UNKNOWN}\\\\\n\\neg\\texttt{UNKNOWN}&\\equiv\\texttt{UNKONWN}\n\\end{aligned}\\]\nA tuple is in a query result \\(\\iff\\) the result of the WHERE clause is TRUE.\n\n\n\n\n\n\n“Aggregation ignores NULL.”\n\nNULL never contributes to a sum, average, or count, and can never be the minimum or maximum of a column (unless every value is NULL).\n\nIf ALL values are NULL in a column, then the result of the aggregation is NULL.\nException: COUNT of an empty set is 0. (think of COUNT(columnName) as a function that counts the non-null values in that column.)\n\n\n\n\n\n\n\nTip 4: Example: Aggregation with NULL\n\n\n\n\n\n\nR&S&T are defined as:\n\n R             |      S             |       T\n   x           |          x         |            x\n -----         |        -----       |          -----\n NULL          |        NULL        |\n 1             |                    |\n\nCOUNT()\n\nCOUNT(R.*)=2 and COUNT(R.x)=1\nCOUNT(S.*)=1 and COUNT(S.x)=0\nCOUNT(T.*)=0 and COUNT(T.x)=0\n\nOther aggregates:\n\nMIN(R.x)=1 and MAX(R.x)=1\nMIN(S.x)=NULL and MAX(S.x)=NULL\nMIN(T.x)=NULL and MAX(T.x)=NULL\n\n\n\n\n\n\n\n\n\nNULL is treated differently by the set operators UNION, EXCEPT, and INTERSECT than it is in search conditions.\nWhen comparing rows, set operators treat NULL values as equal to each other.\nIn contrast, when NULL is compared to NULL in a search condition the result is UNKNOWN (not true)."
  },
  {
    "objectID": "notes/NumericalAnalysis1.html",
    "href": "notes/NumericalAnalysis1.html",
    "title": "Undergraduate-Level Numerical Analysis",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes/Proofs.html#proof-practice",
    "href": "notes/Proofs.html#proof-practice",
    "title": "IB Math AA HL Notes",
    "section": "Proof Practice",
    "text": "Proof Practice"
  },
  {
    "objectID": "notes/Calc2.html",
    "href": "notes/Calc2.html",
    "title": "Calculus 2",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes/cs334.html",
    "href": "notes/cs334.html",
    "title": "CS 334 Machine Learning",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n1 Linear Classification\n\n\n\n\n\n\nLinear Cassification\n\n\nLoss Function\n\n\nTraining Error\n\n\nPerceptron\n\n\n\nThis lecture discusses the linear classification, a fundamental concept in machine learning. It introduces loss functions, training error, and the perceptron algorithm.\n\n\n\n\n\nJan 23, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n2 Gradient Descent on Classification\n\n\n\n\n\n\nClassification\n\n\nGradient Descent\n\n\nHinge Loss\n\n\n\nThis lecture discusses the gradient descent algorithm and its application to classification problem.It introduces the hinge loss function and the update rule for gradient descent.\n\n\n\n\n\nJan 28, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n3 Linear Regression\n\n\n\n\n\n\nLinear Regreesion\n\n\nGradient Descent\n\n\nSquared Loss\n\n\nLeast Squares\n\n\nNormal Equation\n\n\n\nThis lecture discusses the linear regression, a fundamental concept in machine learning. It introduces the squared loss function and the update rule for gradient descent. It also derived the closed-form solution for linear regression using matrix notation.\n\n\n\n\n\nJan 30, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n4 Regularization\n\n\n\n\n\n\nRegularization\n\n\nBias-Variance Tradeoff\n\n\nRidge Regression\n\n\nLasso Regression\n\n\nElastic Net\n\n\n\nThis lecture starts from the bias-variance tradeoff, and then introduces regularization as a way to control the tradeoff. We will discuss the \\(L_2\\) regularization (Ridge Regression), \\(L_1\\) regularization (Lasso Regression), and Elastic Net.\n\n\n\n\n\nFeb 4, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n5 Logistic Regression\n\n\n\n\n\n\nLogistic Regression\n\n\nClassification\n\n\nSigmoid Function\n\n\n\nThis lecture introduces the logistic regression model, which is used for binary classification. We will discuss the sigmoid function, the likelihood function, and the cost function.\n\n\n\n\n\nFeb 6, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n6 Model Selection and Model Assessment\n\n\n\n\n\n\nModel Selection\n\n\nModel Assessment\n\n\nClassification Metrics\n\n\nRegression Metrics\n\n\nCross Validation\n\n\n\nThis lecture discusses the model assessment and model selection process. We will cover classification performance metrics, regression metrics, model assessment process, and model selection.\n\n\n\n\n\nFeb 11, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n7 Feature Selection and Kernels\n\n\n\n\n\n\nFeature Engineering\n\n\nFeature Selection\n\n\nKernels\n\n\n\nThis lecture discusses the feature selection and kernel methods. We will cover feature engineering and selection methods, kernel methods, and kernel tricks.\n\n\n\n\n\nFeb 18, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\n\n\n\n\n\n\n8 Decision Trees and Random Forest\n\n\n\n\n\n\nDecision Trees\n\n\nEnsemble Methods\n\n\nRandom Forest\n\n\n\nThis lecture discusses the basics od decision trees including how to build a decision tree. It also introduces ensemble methods and discusses the random forest algorithm as an example of ensemble methods.\n\n\n\n\n\nMar 6, 2025\n\n\nJiuru Lyu\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "My Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nPolynomial Representation of Arnoldi\n\n\n\nNumerical Analysis\n\n\nIterative Method\n\n\nLinear Algebra\n\n\nArnoldi\n\n\n\nI’m recently learning about the Arnoldi’s method, and the textbook mentioned that the it can be viewed from a polynomial approximation point of view. However, I think the…\n\n\n\nJiuru Lyu\n\n\nDec 10, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "photo.html",
    "href": "photo.html",
    "title": "Photograph",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "High School Level Math\n\nIB Math AA HL Notes\n\n\n\nCollege Level Math\n\nMath Fundamentals\n\nCalculus II\nMultivariable Calculus/Advanced Calculus/Calculus III\nLinear Algebra\nMathematical Proofs\n\n\n\nApplied Mathematics\n\nOrdinary Differential Equations\nNonlinear Optimization\nUndergraduate-Level Numerical Analysis\nPhD-Level Numerical Analysis I\n\n\n\nPure Mathematics\n\nReal Analysis\n\n\n\nData Science, Statistics, and Causal Inference\n\nIntroduction to Causal Inference\nGoogle Data Analytics Learning Notes\nMathematical Statistics\nCausal Designs and Inference\nMachine Learning\n\n\n\n\nComputer Science\n\nObject-Oriented Programming & Introduction to Data Structures\nIntroduction to Data Structure and Algorithms\nDatabase Systems\n\n\n\nOther Fields\n\nIntroduction to Sociology\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jiuru Lyu",
    "section": "",
    "text": "Hi! My name is Jiuru Lyu, and I am a junior at Emory University studying Applied Mathematics. In my leisure time, I enjoy coffee brewing, traveling, photograph, and commercial aviation.\n\n\n Back to top"
  },
  {
    "objectID": "notes/cs334/02-Gradient-Descent-on-Classification/02-Gradient-Descent-on-Classification.html",
    "href": "notes/cs334/02-Gradient-Descent-on-Classification/02-Gradient-Descent-on-Classification.html",
    "title": "2 Gradient Descent on Classification",
    "section": "",
    "text": "Goal: Learning to classify non-linearly separable data (by considering a different objective function)."
  },
  {
    "objectID": "notes/cs334/02-Gradient-Descent-on-Classification/02-Gradient-Descent-on-Classification.html#objective-function-and-hinge-loss",
    "href": "notes/cs334/02-Gradient-Descent-on-Classification/02-Gradient-Descent-on-Classification.html#objective-function-and-hinge-loss",
    "title": "2 Gradient Descent on Classification",
    "section": "Objective Function and Hinge Loss",
    "text": "Objective Function and Hinge Loss\n\n\n\n\n\n\n\nDefinition 1 (Objective Function for Perceptron Algorithm) \\[\n\\epsilon_{N}(\\va\\theta)=\\sum_{i=1}^N\\1\\qty{-y^{(i)}(\\va\\theta\\cdot\\va x^{(i)})\\leq0},\n\\] for conciseness, we assume the offset parameter is implicity. Note that the empirical risk is \\[\nR_N(\\va\\theta)=\\frac{1}{N}\\sum_{i=1}^N\\operatorname{loss}\\qty(y^{(i)}\\qty(\\va\\theta\\cdot\\va x^{(i)})),\n\\] so, training error is a special case of empirical risk.\n\n\n\n\n\nPreviously, we were using “zero-one loss”. Let \\(z=y(\\va\\theta\\cdot\\va x)\\), then the zero-one loss is \\[\n\\operatorname{loss}_{0-1}(z)=\\1\\qty{z\\leq0}=\\begin{cases}1\\quad&\\text{if }z\\leq 0\\\\0\\quad&\\text{o.w}\\end{cases}.\n\\]\n\nThe graph of the zero-one loss is shown below:\n\n\n\n\n\n\nFigure 1: Zero One Loss\n\n\n\n\nHowever, there are some issues with the zero-one loss:\n\nNot continuous at \\(z=0\\).\nNot differentiable at \\(z=0\\).\nNot convex.\n\nDue to these issues, direct minimization of empirical risk with \\(0-1\\) loss is chanllenging for the general case (NP-hard).\n\n\n\n\n\n\n\nTip 1: Calculus Refresher\n\n\n\n\n\n\nContinuous: A function \\(f\\) is continuous at \\(x_0\\) if \\[\\lim_{x\\to x_0}f(x)=f(x_0)\\].\nDifferentiable: A function \\(f\\) is differentiable at \\(x_0\\) if the following limit exists: \\[\\lim_{h\\to 0}\\frac{f(x_0+h)-f(x_0)}{h}\\] Also, Differentiable \\(\\implies\\) continuous.\nConvex/concave up: \\[f(\\lambda x+(1-\\lambda)y)\\leq\\lambda f(x)+(1-\\lambda)f(y).\\]\n\n\n\n\n\n\n\nFigure 2: Convex Function\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 1 (Motivation of Hinge Loss: Misclassification)  \n\n\n\n\n\n\nFigure 3: Misclassification on Non-Linearly Separable Data\n\n\n\n\nLoss (or cost) associated with the ==misclassification== of these points is identified under \\(0-1\\) loss. Then, loss is \\(1\\) for each misclassified point.\nHowever, these two misclassifications are not equally bad.\nWe need a loss function that treats these mistakes differently.\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 2 \\[\n\\operatorname{loss}_h(z)=\\max\\qty{0,1-z}=\\begin{cases}1-z\\quad&\\text{if }z\\leq 1\\\\0\\quad&\\text{if }z&gt;1\\end{cases}\n\\]\nThe graph of the hinge loss is shown below:\n\n\n\n\n\n\nFigure 4: Hinge Loss\n\n\n\n\n\n\n\n\nProperties of hinge loss:\n\nNon-negative.\nIf \\(z=y\\qty(\\va\\theta\\cdot\\va x)&lt;1\\), we incur a non-zero cost.\nThis forces predictions to be more than just correct, but we need \\[y\\qty(\\va\\theta\\cdot\\va x)\\geq 1.\\] Note, previously, we only imposed \\(y\\qty(\\va\\theta\\cdot\\va x)&gt;0\\).\n\n\n\n\n\n\n\n\n\nRemark 1 (What does \\(z=y\\qty(\\va\\theta\\cdot\\va x)\\) tell us?). \n\nSign: correct (\\(&gt;0\\)) or wrong (\\(&lt;0\\)).\nMagnitude: how wrong (the larger, the more wrong). \\[\n\\abs{y\\qty(\\va\\theta\\cdot\\va x)}=\\abs{y}\\cdot\\abs{\\va\\theta\\cdot\\va x},\n\\] where \\(\\abs{y}=1\\) since \\(y\\in{-1,+1}\\) and \\(\\dfrac{\\abs{\\va\\theta\\cdot\\va x}}{\\norm{\\va\\theta}}\\) is the distance from the point to the decision boundary.\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3 (Empirical Risk with Hinge Loss) \\[\nR_N(\\va\\theta)=\\dfrac{1}{N}\\sum_{i=1}^N\\max\\qty{0, 1-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}}\n\\] Why is it better/easier to minimize? - Continuous - (Sub)differentiable - Convex \\(\\implies\\) We can use a simple algorithm to minimize it: Gradient Descent."
  },
  {
    "objectID": "notes/cs334/02-Gradient-Descent-on-Classification/02-Gradient-Descent-on-Classification.html#gradient-descent",
    "href": "notes/cs334/02-Gradient-Descent-on-Classification/02-Gradient-Descent-on-Classification.html#gradient-descent",
    "title": "2 Gradient Descent on Classification",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nGradient:\n\nUnivariate case: \\(f:\\R\\to\\R\\), \\(y=f(x)\\). Its derivative is \\(f'(x)=\\dsst\\dv{x} f(x)\\). \\(f'(x)\\) represents the rate of change:\n\n\\(f'(x)&gt;0\\): increasing\n\\(f'(x)=0\\): a critical point\n\\(f'(x)&lt;0\\): decreasing.\n\nMultivariate case: \\(f:\\R^d\\to\\R\\), \\(y=f(\\va x)=f(x_1,x_2,\\dots,x_d)\\). Its gradient is \\[\\grad_{\\va x} f(\\va x)=\\mqty[\\dsst\\pdv{x_1} f(\\va x),\\dots,\\dsst\\pdv{x_d} f(\\va x)]^\\top.\\] The gradient points in the direction of the steepest ascent.\n\nGradient Descent (GD):\n\nSuppose we want to minimize some \\(f(\\va\\theta)\\) with respect to \\(\\va\\theta\\). We know gradient \\(\\grad_{\\va\\theta}f(\\va\\theta)\\) points in the direction of steepest increase.\nIdea: start somewhere, and take small steps in the opposite direction as gradient.\n\n\n\n\n\\begin{algorithm} \\caption{Gradient Descent (GD)} \\begin{algorithmic} \\State $k=0$; $\\va\\theta^{(0)}=\\va 0$ \\While{not converged} \\State $\\va\\theta^{(k+1)}=\\va\\theta^{(k)}-\\underbrace{\\eta_k}_{\\substack{\\text{learning}\\\\\\text{rate}}}\\overbrace{\\eval{\\grad_{\\va\\theta} f(\\va\\theta)}_{\\va\\theta=\\va\\theta^{(k)}}}^{\\substack{\\text{evaluate gradient}\\\\\\text{at current }\\va\\theta}}$ \\State $k++$ \\EndWhile \\end{algorithmic} \\end{algorithm}\n\n\n\nImprove GD (Algorithm 1): Stochastic Gradient Descnet (SGD):\n\nIn GD (Algorithm 1), we need to compute the gradient over the entire dataset. This can be expensive for large datasets.\nIn SGD (Algorithm 2), we compute the gradient for each data point and update \\(\\va\\theta\\) accordingly.\n\n\n\n\n\\begin{algorithm} \\caption{Stochastic Gradient Descent (SGD)} \\begin{algorithmic} \\State $k=0$; $\\va\\theta^{(0)}=\\va 0$ \\While{not converged} \\State randomly shuffle points \\For{$i=1,\\dots, N$} \\State $\\va\\theta^{(k+1)}=\\va\\theta^{(k)}-\\eta_k\\eval{\\grad_{\\va\\theta}\\operatorname{loss}(y^{(i)}\\va\\theta\\cdot\\va x^{(i)})}_{\\va\\theta=\\va\\theta^{(k)}}$ \\State $k++$ \\EndFor \\EndWhile \\end{algorithmic} \\end{algorithm}\n\n\n\nGradient for Hinge Loss: \\[\n\\operatorname{loss}_h(z)=\\max\\qty{0,1-y\\va\\theta\\cdot\\va x}\n\\]\n\n\\(\\boxed{\\text{Case I}}\\) If \\(y^{(i)}\\va\\theta\\cdot\\va x^{(i)}\\geq 1\\): loss is zero. Already at minimum. No update.\n\\(\\boxed{\\text{Case II}}\\) If \\(y^{(i)}\\va\\theta\\cdot\\va x^{(i)}&lt;1\\): loss is \\[1-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}\\] Note that \\(\\va\\theta\\cdot\\va x=\\theta_1x_1+\\theta_2x_2+\\cdots+\\theta_dx_d\\), we have \\[\n\\grad_{\\va\\theta}(\\va\\theta\\cdot\\va x)=\\mqty[x_1,x_2,\\dots,x_d]^\\top=\\va x.\n\\] So, the gradient is \\[\n\\begin{aligned}\n\\grad_{\\va\\theta}\\operatorname{loss}_h(y^{(i)}\\va\\theta\\cdot\\va x^{(i)})&=\\grad_{\\va\\theta}(1-y^{(i)}\\va\\theta\\cdot\\va x)\\\\\n&=0-y^{(i)}\\grad_{\\va\\theta}(\\va\\theta\\cdot\\va x)\\\\\n&=-y^{(i)}\\va x^{(i)}.\n\\end{aligned}\n\\]\nSo, in Algorithm 2, the update rule is\n\\[\n\\begin{aligned}\n\\va\\theta^{(k+1)}&=\\va\\theta^{(k)}-\\eta_k\\eval{\\grad_{\\va\\theta}\\operatorname{loss}(y^{(i)}\\va\\theta\\cdot\\va x^{(i)})}_{\\va\\theta=\\va\\theta^{(k)}}\\\\\n&=\\va\\theta^{(k)}+\\eta_ky^{(i)}\\va x^{(i)}.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nRemark 2 (Why shuffle points in SGD?). \n\nRemove the effect of any unintended/aritifical ordering in the dataset.\nFaster convergence\n\n\n\n\n\n\nStopping criteria\n\nCheck if empirical risk stops decreasing.\nCheck if parameter vector stop changing.\nNote: make \\(\\operatorname{loss}=0\\) does not work here since the data is not perfectly linearly separable. \n\nLearning rate/Step size/\\(\\eta\\):\n\nToo small: slow convergence.\nToo large: overshot & never converge\nA hyperparameter that can (and should) be tuned.\nMay set \\(\\eta\\) as a constant or a function of \\(k\\). For example, \\[\\eta_k=\\dfrac{1}{k}.\\]\n\n\n\n\n\n\n\n\n\nTheorem 1 (GD (Algorithm 1) and SGD (Algorithm 2) Convergence Theorem)  \n\nWith appropriate learning rate \\(\\eta\\), if \\(R_N(\\va\\theta)\\) is convex, (S)GD will converge to the global minimum almost surely.\nSGD is a general algorithm that can be applied to non-convex functions as well, in which case it converges to a local minimum.\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 4 (Robbins-Monro Conditions) A good learning rate ensures convergence satisfying the Robbins-Monro conditions: \\[\n\\sum_{k=1}^\\infty\\eta_k=\\infty\\quad\\text{and}\\quad\\sum_{k=1}^\\infty\\eta_k^2&lt;\\infty.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nRemark 3 (Differentiability of Loss Function). \\(R_N(\\va\\theta)\\) with hinge loss is not everywhere differentiable since it si piecewise linear. What do we do? - When differentiable, use gradient. - When sub-differentiable, choose any gradient around the kink."
  },
  {
    "objectID": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html",
    "href": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html",
    "title": "3 Linear Regression",
    "section": "",
    "text": "Goal: Given data \\(\\qty{\\va x^{(i)}, y^{(i)}}_{i=1}^N\\), find a linear model \\[\nf(\\va x;\\va\\theta)=\\va\\theta\\cdot\\va x,\n\\] such that minimizes the empirical loss."
  },
  {
    "objectID": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html#the-loss-function",
    "href": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html#the-loss-function",
    "title": "3 Linear Regression",
    "section": "The Loss Function",
    "text": "The Loss Function\n\n\n\n\n\n\n\nDefinition 1 (Squared Loss Function) The empirical loss for linear regression is defined by \\[\nR_N(\\va\\theta)=\\dfrac{1}{N}\\sum_{i=1}^N\\operatorname{loss}\\qty(y^{(i)}, \\va\\theta\\cdot\\va x^{(i)}),\n\\] where loss is the squared loss: \\[\n\\operatorname{loss}(z)=\\dfrac{z^2}{2}.\n\\]\nThis loss function is continuous, differentiable, and convex.\nHere is a graph of the squared loss function:\n\n\n\n\n\n\nFigure 1: Squared Loss\n\n\n\nIntuition: we permits small discrepancies but penalizes large deviations."
  },
  {
    "objectID": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html#ordinary-least-squares-ols",
    "href": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html#ordinary-least-squares-ols",
    "title": "3 Linear Regression",
    "section": "Ordinary Least Squares (OLS)",
    "text": "Ordinary Least Squares (OLS)\n\nWe will minimize the empirical loss with squared loss, i.e., \\[\nR_N(\\va\\theta)=\\dfrac{1}{N}\\sum_{i=1}^N\\dfrac{\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})^2}{2},\n\\] using SGD algorithm\nRecall the gradient descent update rule: \\[\n\\va\\theta^{(k+1)}=\\va\\theta^{(k)}-\\eta_k\\eval{\\grad_{\\va\\theta}\\operatorname{loss}(y^{(i)}\\va\\theta\\cdot\\va x^{(i)})}_{\\va\\theta=\\va\\theta^{(k)}}\n\\]\n\nIn OLS case, we have \\[\n\\begin{aligned}\n\\grad_{\\va\\theta}\\operatorname{loss}(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})&=\\grad_{\\va\\theta}\\dfrac{\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})^2}{2}\\\\\n&=-\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})\\va x^{(i)}.\n\\end{aligned}\n\\]\nSo, the update rule is \\[\n\\va\\theta^{(k+1)}=\\va\\theta^{(k)}+\\eta_k\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})\\va x^{(i)}.\n\\]\n\n\n\\begin{algorithm} \\caption{SGD for Least Squares Regression} \\begin{algorithmic} \\State $k=0$; $\\va\\theta^{(0)}=\\va 0$ \\While{not converged} \\State randomly shuffle points \\For{$i=1,\\dots, N$} \\State $\\va\\theta^{(k+1)}=\\va\\theta^{(k)}+\\eta_k\\eval{\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})\\va x^{(i)}}_{\\va\\theta=\\va\\theta^{(k)}}$ \\State $k++$ \\EndFor \\EndWhile \\end{algorithmic} \\end{algorithm}\n\n\n\n\n\n\n\n\n\nRemark 1 (Compare with Hinge Loss Classification). \n\nWe make updates on every data points\nStopping criteria, learning rate, shuffling, etc., are the same."
  },
  {
    "objectID": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html#closed-form-solution",
    "href": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html#closed-form-solution",
    "title": "3 Linear Regression",
    "section": "Closed-Form Solution",
    "text": "Closed-Form Solution\n\nSince \\(R_N(\\va\\theta)\\) with squared loss is a covex function and differentiable everywhere, we can try to minimize it analytically by setting the gradient to \\(0\\).\nRewrite empirical risk in matrix notation: \\[\nX=\\mqty[-&\\va x^{(1)\\top}&-\\\\&\\vdots&\\\\-&\\va x^{(N)\\top}&-]_{N\\times d}\\quad\\text{and}\\quad \\va y=\\mqty[y^{(1)}\\\\\\vdots\\\\y^{(N)}]_{N\\times 1}.\n\\] Then, the empirical risk is \\[\n\\begin{aligned}\nR_N(\\va\\theta)=\\dfrac{1}{2}\\dfrac{1}{N}\\sum_{i=1}^N\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})^2&=\\dfrac{1}{2N}\\qty(X\\va\\theta-\\va y)^\\top\\qty(X\\va\\theta-\\va y)\\\\\n&=\\dfrac{1}{2N}\\qty(\\va\\theta^\\top X^\\top X\\va\\theta-2\\va\\theta^\\top (X^\\top\\va y)+\\va y^\\top\\va y).\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nTip 1: Commonly Used Differentiation Rules\n\n\n\n\n\n\n\n\n\\(f(\\va x)\\)\n\\(\\grad_{\\va x}f(\\va x)\\)\n\n\n\n\n\\(\\va v^\\top\\va x\\)\n\\(\\va v\\)\n\n\n\\(\\va x^\\top\\va x\\)\n\\(2\\va x\\)\n\n\n\\(\\va x^\\top A\\va x\\)\n\\((A+A^\\top)\\va x\\) 1\n\n\n\n\n\n\n\nTherefore, the gradient of \\(R_N(\\va\\theta)\\) is\n\n\\[\n\\begin{aligned}\n\\grad_{\\va\\theta}R_N(\\va\\theta)&=\\dfrac{1}{2N}\\qty(2X^\\top X\\va\\theta-2(X^\\top\\va y))\\\\\n&=\\dfrac{1}{N}\\qty(X^\\top X\\va\\theta-X^\\top\\va y).\n\\end{aligned}\n\\]\n\nSet gradient to \\(0\\), we have\n\n\\[\n\\begin{aligned}\n\\grad_{\\va\\theta}R_N(\\va\\theta)&=0\\\\\n\\dfrac{1}{N}\\qty(X^\\top X\\va\\theta-X^\\top\\va y)&=0\\\\\nX^\\top X\\va\\theta-X^\\top\\va y&=0\\\\\nX^\\top X\\va\\theta&=X^\\top\\va y\\\\\n\\va\\theta^*&=\\qty(X^\\top X)^{-1}X^\\top\\va y.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nRemark 2 (Normal Equation and Inveritibility). \n\nThe solutions is called the Normal Equation or the OLS solution.\nOLS solution \\(\\va\\theta^*=\\qty(X^\\top X)^{-1}X^\\top\\va y.\\) exists only when \\(X^\\top X\\) is invertible.\nInvertibility of \\(X^\\top X\\): “Gram matrix”\n\nWhy might it be non-invertible/singular/degenerate? \\[\\rank(X^\\top X)\\leq\\rank(X)\\leq\\min\\qty{N,d}\\]\n\nIf \\(d&gt;N\\), then \\(\\rank(X^\\top X)\\leq N&lt;d\\). Since \\(X^\\top X\\in\\R^{d\\times d}\\), we know \\(X^\\top X\\) is not full rank, and thus non-invertible.\nFor example, if we have duplicated features, say \\(x_1=x_2\\), then if \\(\\mqty[\\theta_1,\\theta_2]\\) is a solution, then \\(\\mqty[\\theta_1-c,\\theta_2+c]\\) is also a solution.\n\nWhat to do if \\(X^\\top X\\) is not invertible?\n\nMonro-Penrose Pseudo-Inverse: \\[\\va\\theta^*=(X^\\top X)^{\\dagger}X^\\top \\va y=X^{\\dagger}\\va y\\]\nRegularization: See next lecture."
  },
  {
    "objectID": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html#footnotes",
    "href": "notes/cs334/03-Linear-Regression/03-Linear-Regression.html#footnotes",
    "title": "3 Linear Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf \\(A\\) is symmetric, then \\(\\grad_{\\va x}f(\\va x)=2A\\va x\\).↩︎"
  },
  {
    "objectID": "notes/cs334/04-Regularization/04-Regulairzation.html",
    "href": "notes/cs334/04-Regularization/04-Regulairzation.html",
    "title": "4 Regularization",
    "section": "",
    "text": "Polynomial regression:\n\nInstead of only using \\(\\phi(x)=[1,x]^\\top\\) , we can add higher dimension terms of feature: \\[\\phi(x)=[1,x,x^2,x^3,\\dots]^\\top\\]\nHowever, if we get too far, we will encounter overfitting.\n\nOverfitting and Underfitting:\n\nIf we optimize \\(R(\\va\\theta, D)\\), \\(R(\\va\\theta, D_\\text{test})\\) can be learge due to:\n\n\n\n\n\nUnderfitting\nOverfitting\n\n\n\n\nBias\nVariance\n\n\nApproximation error\nEstimation error\n\n\nPoor on training set\nGood on training set\n\n\nPoor on test set\nPoor on test set\n\n\nAdding more data will not help\nAdding more data will help\n\n\n\n\n\n\n\n\n\nFigure 1: Model Complexity vs. Error: Bias-Variance Tradeoff\n\n\n\n\nSources of Bias:\n\nModel class too small: unable to represent the underlying relationship\nModels are “too global” (e.g., constant output, single linear separator)\n\nReduing Bias: Use more complex models\n\nInteraction terms\nAdd more features\nKernels\nAlgorithm-specific approaches\n\nSources of Variance:\n\nNoise in labels or features\nModels are too “local” – sensitive to small changes in feature values\nTraining dataset too small.\n\nReducing Variance: Collect more data, or less complex models\n\nDrop interaction terms\nRegualrization\nFeature selection\nDon’t use kernels\nEnsemble\n\nBalancing Bias and Variance: Generalizable Model\n\nBut how to control the tradeoff? Regularization."
  },
  {
    "objectID": "notes/cs334/04-Regularization/04-Regulairzation.html#introduction-bias-variance-tradeoff",
    "href": "notes/cs334/04-Regularization/04-Regulairzation.html#introduction-bias-variance-tradeoff",
    "title": "4 Regularization",
    "section": "",
    "text": "Polynomial regression:\n\nInstead of only using \\(\\phi(x)=[1,x]^\\top\\) , we can add higher dimension terms of feature: \\[\\phi(x)=[1,x,x^2,x^3,\\dots]^\\top\\]\nHowever, if we get too far, we will encounter overfitting.\n\nOverfitting and Underfitting:\n\nIf we optimize \\(R(\\va\\theta, D)\\), \\(R(\\va\\theta, D_\\text{test})\\) can be learge due to:\n\n\n\n\n\nUnderfitting\nOverfitting\n\n\n\n\nBias\nVariance\n\n\nApproximation error\nEstimation error\n\n\nPoor on training set\nGood on training set\n\n\nPoor on test set\nPoor on test set\n\n\nAdding more data will not help\nAdding more data will help\n\n\n\n\n\n\n\n\n\nFigure 1: Model Complexity vs. Error: Bias-Variance Tradeoff\n\n\n\n\nSources of Bias:\n\nModel class too small: unable to represent the underlying relationship\nModels are “too global” (e.g., constant output, single linear separator)\n\nReduing Bias: Use more complex models\n\nInteraction terms\nAdd more features\nKernels\nAlgorithm-specific approaches\n\nSources of Variance:\n\nNoise in labels or features\nModels are too “local” – sensitive to small changes in feature values\nTraining dataset too small.\n\nReducing Variance: Collect more data, or less complex models\n\nDrop interaction terms\nRegualrization\nFeature selection\nDon’t use kernels\nEnsemble\n\nBalancing Bias and Variance: Generalizable Model\n\nBut how to control the tradeoff? Regularization."
  },
  {
    "objectID": "notes/cs334/04-Regularization/04-Regulairzation.html#regularization",
    "href": "notes/cs334/04-Regularization/04-Regulairzation.html#regularization",
    "title": "4 Regularization",
    "section": "Regularization",
    "text": "Regularization\n\n\n\n\n\n\n\nDefinition 1 (Regularization Training Error) A “knob” for controlling model complexity: \\[\nJ(\\va\\theta)=\\underbrace{R_N(\\va\\theta)}_\\text{emiprical risk}+\\underbrace{\\lambda\\Omega(\\va\\theta)}_\\text{regularization term},\n\\] where \\(\\lambda\\) is the regularization strength, \\(\\lambda&gt;0\\), and \\(\\Omega(\\va\\theta)\\) is the regularizer/regularization penalty.\nThe regularization strength \\(\\lambda\\) balances: - How well we fit the data, and - Complexity of model.\nThen, our goal is to \\(\\displaystyle\\min_{\\va\\theta}J(\\va\\theta)\\).\n\n\n\n\n\nCommon Regularizer:\n\n\\(L_2\\): Ridge Regression \\[\\Omega(\\va\\theta)=\\dfrac{\\|\\va\\theta\\|_2^2}{2}=\\dfrac{1}{2}\\sum_{j=1}^d\\theta_j^2\\]\n\\(L_1\\): Lasso Regression \\[\\Omega(\\va\\theta)=\\|\\va\\theta\\|_1=\\sum_{j=1}^d\\abs{\\theta_j}\\]\nElastic net: \\[\\Omega(\\va\\theta)=\\lambda_2\\|\\va\\theta\\|_2^2+\\lambda_1\\|\\va\\theta\\|_1\\]\n\nEffect of Regularization:\n\nWhen minimizing \\(J(\\va\\theta)\\), we are still trying to minimize \\(R_N(\\va\\theta)\\), but at the same time, minimize \\(\\Omega(\\va\\theta)\\)\n\nPush parameters to small values\nResist setting parameters away from default of \\(0\\) unless data strong suggest otherwise.\n\nWhy are small values in \\(\\va\\theta\\) good?\n\nLimit effect of small perturbations in individual factors on putput.\nIf \\(\\theta_j=0\\) exactly, then \\(j\\)-th parameter is effective unused \\(\\implies\\) feature selection.\nOccam’s Razor (\\(13\\)-th century philosopher, William of Ockham): “When you have two competing hypotheses, the simpler one is preferred.”\n\n\nRidge Regression (\\(L_2\\)-Regularized Linear Regression) \\[J(\\va\\theta)=\\sum_{i=1}^N\\dfrac{(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})^2}{2}+\\lambda\\dfrac{\\|\\va\\theta\\|_2^2}{2},\\] where \\(\\dfrac{1}{N}\\) is absorbed into \\(\\lambda\\).\n\nSGD: \\[\\grad_{\\va\\theta}\\qty(\\dfrac{\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})^2}{2}+\\lambda\\dfrac{\\|\\va\\theta\\|_2^2}{2})\\] gradient of squared \\(L_2\\) norm: \\(\\|\\va\\theta\\|_2^2=\\theta_1^2+\\cdots+\\theta_d^2=\\va\\theta\\cdot\\va\\theta\\). So, \\[\\pdv{\\theta_j}\\qty(\\|\\va\\theta\\|_2^2)=2\\theta_j.\\] Then, we have \\[\n\\begin{aligned}\n  \\grad_{\\va\\theta}\\qty(\\|\\va\\theta\\|_2^2)=\\grad_{\\va\\theta}(\\va\\theta\\cdot\\va\\theta)&=\\mqty[\\displaystyle\\pdv{\\theta_1}\\qty(\\|\\va\\theta\\|_2^2),\\dots,\\pdv{\\theta_d}\\qty(\\|\\va\\theta\\|_2^2)]\\\\\n  &=\\mqty[2\\theta_1,2\\theta_2,\\dots,2\\theta_d]\\\\\n  &=2\\va\\theta.\n\\end{aligned}\n\\] Therefore, \\[\n\\grad_{\\va\\theta}\\qty(\\dfrac{\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})^2}{2}+\\lambda\\dfrac{\\|\\va\\theta\\|_2^2}{2})=-\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})\\va x^{(i)}+\\lambda\\va\\theta.\n\\] Hence, the SGD update rule is \\[\n\\begin{aligned}\n\\va\\theta^{(k+1)}=\\va\\theta^{(k)}-\\eta_k\\eval{\\qty[-\\qty(y^{(i)}-\\va\\theta\\cdot\\va x^{(i)})\\va x^{(i)}+\\lambda\\va\\theta]}_{\\va\\theta=\\va\\theta^{(k)}}\\\\\n&=\\va\\theta^{(k)}+\\eta_k\\qty(y^{(i)}-\\va\\theta^{(k)}\\cdot\\va x^{(i)})\\va x^{(i)}-\\eta_k\\lambda\\va\\theta^{(k)}\\\\\n&=\\underbrace{(1-\\eta_k\\lambda)}_{\\text{between }0\\text{ and }1}\\va\\theta^{(k)}+\\underbrace{\\eta_k\\qty(y^{(i)}-\\va\\theta^{(k)}\\cdot\\va x^{(i)})\\va x^{(i)}}_\\text{same as before}.\n\\end{aligned}\n\\] The term \\(1-\\eta_k\\lambda\\) shrinks parameters towards \\(0\\) in each update.\nClosed Form Solution: \\[\n\\begin{aligned}\nJ(\\va\\theta)&=\\dfrac{1}{2}(X\\va\\theta-\\va y)^\\top(X\\va\\theta-\\va y)+\\dfrac{\\lambda}{2}\\va\\theta^\\top\\va\\theta\\\\\n\\grad_{\\va\\theta} J(\\va\\theta)&=(XX)\\va\\theta-X^\\top y+\\lambda\\va\\theta\\\\\n&=(XX+\\lambda I_d)\\va\\theta-X^\\top y=0\\\\\n\\va\\theta^*&=(\\underbrace{XX+\\lambda I_d}_\\text{always invertible})^{-1}X^\\top y.\n\\end{aligned}\n\\]\n\nGeometric Interpretation of Regularization: \\[\n\\va\\theta^*=\\argmin_{\\va\\theta}R_N(\\va\\theta)+\\lambda\\Omega(\\va\\theta) \\iff \\min_{\\va\\theta}R_N(\\va\\theta)\\quad\\text{s.t.}\\quad\\Omega(\\va\\theta)\\leq B.\n\\]\n\nWe can transfer a constrained optimization problem to an unconstrained one by adding a penalty term.\nThey are equivalent by the Lagrange multiplier.\n\n\n\n\n\n\n\n\nFigure 2: Geometric Interpretation of Regularization\n\n\n\n\nWhere is the optimal soltuion of the constrained optimization problem? The solution is the intersection of the level set of \\(R_N(\\va\\theta)\\) and the level set of \\(\\Omega(\\va\\theta)\\).\nMethod of Lagrange Multtiplier:\n\nThe pint where the level curve of function to minimize is tangent to (or just touching) the constraint.\n\nIntuition: two forces play\n\n\\(\\Omega(\\va\\theta)\\leq B\\): shrink \\(\\va\\theta\\) towards winthin the constraint\n\\(\\min R_N(\\va\\theta)\\): move \\(\\va\\theta\\) towards the uncosntraied minimizer.\n\n\\(L_2\\) regularization will make parameters generally small.\n\\(L_1\\) regularization will make parameters sparse, and make some parameters exeactly \\(0\\).\n\n\n\n\n\n\n\n\nRemark 1 (Remarks on Regularization). \n\nIf there is an offset/intercept, this coefficient is usually left unpenalized (depending on the implementation).\n\nIf we write \\(\\va\\theta\\cdot\\va x+b\\) explicity, no penalty on \\(b\\).\nIf we write augmented parameters, then we penalize \\(b\\).\n\nRegularization can be “unfair” if features are on the different scales. So, we need feature pre-procesing (e.g., centering and normalization).\nPratical usage of regression: sklearn\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nsklearn.linear_model.LinearRegression\nclosed form solution (wrapper of scipy.linalg.lstsq that uses SVD)\n\n\nsklearn.linear_model.Ridge\nRidge regressionm, can select solver (SGD, SVD, etc.)\n\n\nsklearn.linear_model.Lasso\nLasso regression, coordinate descent\n\n\nsklearn.linear_model.ElasticNet\nElastic net, coordinate descent\n\n\nsklearn.linear_model.SGDRegressor\ngeneric SGD, mix and match different loss and regularizers\n\n\n\n\nPratical use of linear classification: sklearn\n\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nsklearn.linear_model.LogisticRegression\nlogistic regression, can select solver (SGD, Newton-CG, etc.)\n\n\nsklearn.linear_model.SGDClassifier\ngeneric SGD, mix and match different loss and regularizers\n\n\nsklearn.linear_model.Perceptron\nperceptron, SGD with hinge loss\n\n\nsklearn.svm.LinearSVC\nlinear SVM, hinge loss (support vector machine)\n\n\nsklearn.svm.SVC\nnon-linear SVM, kernel trick"
  },
  {
    "objectID": "notes/cs334/05-Logistic-Regression/05-Logistic-Regression.html",
    "href": "notes/cs334/05-Logistic-Regression/05-Logistic-Regression.html",
    "title": "5 Logistic Regression",
    "section": "",
    "text": "The problem: Given binar labels \\(y\\in\\qty{-1,+1}\\). We want to predict the probability of positive class, \\(\\hat y\\in[0, 1]\\).\nHow to make a linear model output a probability? We already have \\(\\va\\theta\\cdot\\va x\\in(-\\infty,\\infty)\\).\n\nApply a sequashing function: \\(\\sigma(\\va\\theta\\cdot\\va x)\\in[0,1]:\\R\\to[0,1]\\).\nWe will use a sigmoid function*: \\[\\sigma(z)=\\dfrac{1}{1+e^{-z}}\\]\n\nRange of sigmoid:\n\nfor \\(z\\to\\infty\\), \\(\\sigma(z)\\to 1\\)\nfor \\(z\\to-\\infty\\), \\(\\sigma(z)\\to 0\\)\n\\(\\sigma(0)=0.5\\)\n\nUseful properties:\n\n\\(\\sigma(-z)=1-\\sigma(z)\\)\n\\(\\sigma'(z)=\\sigma(z)(1-\\sigma(z))\\)\ncontinuous\ndifferentiable\nnot convex\n\n\n\n\n\n\n\n\n\n\nFigure 1: Sigmoid Function"
  },
  {
    "objectID": "notes/cs334/05-Logistic-Regression/05-Logistic-Regression.html#motivation",
    "href": "notes/cs334/05-Logistic-Regression/05-Logistic-Regression.html#motivation",
    "title": "5 Logistic Regression",
    "section": "",
    "text": "The problem: Given binar labels \\(y\\in\\qty{-1,+1}\\). We want to predict the probability of positive class, \\(\\hat y\\in[0, 1]\\).\nHow to make a linear model output a probability? We already have \\(\\va\\theta\\cdot\\va x\\in(-\\infty,\\infty)\\).\n\nApply a sequashing function: \\(\\sigma(\\va\\theta\\cdot\\va x)\\in[0,1]:\\R\\to[0,1]\\).\nWe will use a sigmoid function*: \\[\\sigma(z)=\\dfrac{1}{1+e^{-z}}\\]\n\nRange of sigmoid:\n\nfor \\(z\\to\\infty\\), \\(\\sigma(z)\\to 1\\)\nfor \\(z\\to-\\infty\\), \\(\\sigma(z)\\to 0\\)\n\\(\\sigma(0)=0.5\\)\n\nUseful properties:\n\n\\(\\sigma(-z)=1-\\sigma(z)\\)\n\\(\\sigma'(z)=\\sigma(z)(1-\\sigma(z))\\)\ncontinuous\ndifferentiable\nnot convex\n\n\n\n\n\n\n\n\n\n\nFigure 1: Sigmoid Function"
  },
  {
    "objectID": "notes/cs334/05-Logistic-Regression/05-Logistic-Regression.html#logistic-regression",
    "href": "notes/cs334/05-Logistic-Regression/05-Logistic-Regression.html#logistic-regression",
    "title": "5 Logistic Regression",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nLogistic Regression: \\[\nh(\\va x;\\va\\theta)=\\sigma(\\va\\theta\\cdot\\va x)=\\dfrac{1}{1+e^{-\\va\\theta\\cdot\\va x}}\n\\]\nHow to train this classifier? What loss function should we use?\n\nWhat we want: \\(\\sigma(\\va\\theta\\cdot\\va x)\\) should be the probability of positive class: \\(\\P[y=+1\\mid\\va x]\\).\nIdea: If \\(\\sigma(\\va\\theta\\cdot\\va x)\\) is truely the probability, then we can use it to wrtie down the likelihood of the training data \\(\\qty{\\va x^{(i)}, y^{(i)}}_{i=1}^N\\).\n\nFor each example \\(\\va x^{(i)}\\), the likelihood of seeing its label to be \\(y^{(i)}\\) is \\[\n\\P[y=y^{(i)}\\mid \\va x^{(i)};\\va\\theta]=\\begin{cases}\\sigma(\\va\\theta\\cdot\\va x^{(i)})&\\quad\\text{if }y^{(i)}=+1\\\\\\underbrace{1-\\sigma(\\va\\theta\\cdot\\va x^{(i)})}_{=\\sigma(-\\va\\theta\\cdot\\va x^{(i)})}&\\quad\\text{if }y^{(i)}=-1\\end{cases}=\\sigma(y^{(i)}\\va\\theta\\cdot\\va x^{(i)})\n\\]\nSince each training example is generated independently, \\[\\P\\qty[\\qty{y^{(i)}}_{i=1}^N\\mid\\qty{\\va x^{(i)}}_{i=1}^N;\\va\\theta]=\\prod_{i=1}^N\\sigma(y^{(i)}\\va\\theta\\cdot\\va x^{(i)})=\\prod_{i=1}^N\\dfrac{1}{1+e^{-y^(i)\\va\\theta\\cdot\\va x}}\\]\n\nGoal: Find \\(\\va\\theta^*\\) such that maximizes the likelihood of the training data: \\[\\va\\theta^*=\\argmax_{\\va\\theta}\\prod_{i=1}^N\\dfrac{1}{1+e^{-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}}}\\]\n\nTake the log: \\(\\prod\\to\\sum\\): \\[\\argmax_{\\va\\theta}\\log\\prod_{i=1}^N\\dfrac{1}{1+e^{-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}}}=\\argmax_{\\va\\theta}\\sum_{i=1}^N\\log\\qty(\\dfrac{1}{1+e^{-y^{(i)\\va\\theta\\cdot\\va x^{(i)}}}})\\]\nTake the negative: \\[\\argmax_{\\va\\theta}\\sum_{i=1}^N\\mathrm{loss}=\\argmin{\\va\\theta}\\qty(\\sum_{i=1}^N-\\log\\qty(\\dfrac{1}{1+e^{-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}}}))=\\argmin_{\\va\\theta}\\sum_{i=1}^N\\log\\qty(1+e^{-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}})\\]\n\n\n\n\n\n\n\n\n\n\nDefinition 1 (Logistic Loss:) \\[\\mathrm{loss}_\\text{log}\\qty(\\va x^{(i)}, y^{(i)};\\va\\theta)=\\log\\qty(1+e^{-y^{(i)}\\va\\theta\\cdot\\va x})\\]\n\n\n\n\n\n\nFigure 2: Logistic Loss\n\n\n\nThis loss is: - continuous - Differentiable, and - Convex\nSo, we can use SGD to minimize it."
  },
  {
    "objectID": "notes/cs334/05-Logistic-Regression/05-Logistic-Regression.html#sgd-update-rule",
    "href": "notes/cs334/05-Logistic-Regression/05-Logistic-Regression.html#sgd-update-rule",
    "title": "5 Logistic Regression",
    "section": "SGD Update Rule",
    "text": "SGD Update Rule\n\\[\n\\begin{aligned}\n\\grad_{\\va\\theta}\\log\\qty(1+e^{-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}})&=\\dfrac{1}{1+e^{-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}}}\\qty(e^{-y^{(i)}\\va\\theta\\cdot\\va x^{(i)}})\\grad_{\\va\\theta}\\qty(-y^{(i)}\\va\\theta\\cdot\\va x^{(i)})\\\\\n&=\\dfrac{1}{e^{y^{(i)}\\va\\theta\\cdot\\va x^{(i)}}+1}\\qty(-y^{(i)}\\va x^{(i)})\\\\\n&=\\sigma\\qty(-y^{(i)}\\va\\theta\\cdot\\va x^{(i)})\\qty(-y^{(i)}\\va x^{(i)})\\\\\n&=-y^{(i)}\\va x^{(i)}\\qty(1-\\sigma(y^{(i)}\\va\\theta\\cdot\\va x^{(i)})).\n\\end{aligned}\n\\]\nSo, the update rule is: \\[\n\\begin{aligned}\n\\va\\theta^{(k+1)}&=\\va\\theta^{(k)}-\\eta_k\\eval{\\qty[-y^{(i)}\\va x^{(i)}\\qty(1-\\sigma(y^{(i)}\\va\\theta\\cdot\\va x^{(i)}))]}_{\\va\\theta=\\va\\theta^{(k)}}\\\\\n&=\\va\\theta^{(k)}+\\eta_ky^{(i)}\\va x^{(i)}\\qty(1-\\sigma(y^{(i)}\\va\\theta^{(k)}\\cdot\\va x^{(i)}))\n\\end{aligned}\n\\]\n\nThere’s no closed-form solution for logistic regression in general case. However, since the empirical risk function is convex, \\(\\exists\\) unique global minimum. When there are linearly dependent (redundant) feature, there are infinitely many equally good local minima.\nIf the data is linear separable, \\(\\|\\va\\theta\\|=\\infty\\) is bad. So, we need to add regularization."
  },
  {
    "objectID": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html",
    "href": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html",
    "title": "6 Model Selection and Model Assessment",
    "section": "",
    "text": "Given \\(\\hat y=h(\\va x;\\va\\theta)\\) and \\(D=\\qty{\\va x^{(i)}, y^{(i)}}_{i=1}^N\\), \\(\\va x^{(i)}\\in\\R^d\\) and \\(y^{(i)}\\in\\qty{-1,+1}\\).\n\nMisclassification error: \\[\\dfrac{1}{N}\\sum_{i=1}^N\\1[\\hat y^{(i)}\\neq y^{(i)}].\\]\nAccuracy: \\[\\dfrac{1}{N}\\sum_{i=1}^N\\1[\\hat y^{(i)}=y^{(i)}].\\]\nWhat’s wrong with accuracy: It can be misleading when we have disproportional groups. E.g., rare disease.\n\nConfusion Matrix and Classification Metrics:\n\n\n\n\n\nPredicted (\\(-\\))\nPredicted (\\(+\\))\n\n\n\n\nActual (\\(-\\))\nTrue Negative (TN)\nFalse Positive (FP) Type I Error\n\n\nActual (\\(+\\))\nFalse Negative (FN) Type II Error\nTrue Positive (TP)\n\n\n\n\nAccuracy: \\(\\dfrac{TP+TN}{N}\\)\nSensitivity / True Positive Rate (TRR) / Recall: \\(\\dfrac{TP}{TP+FN}\\).\nSpecificity / True Negative Rate (TNR): \\(\\dfrac{TN}{TN+FP}\\).\nFalse Positive Rate (FPR): \\(\\dfrac{FP}{TN+FP}=1-\\text{specificity}\\).\nPrecision / Positive Predictive Value (PPV): \\(\\dfrac{TP}{TP+FP}\\).\n\n\n\n\n\n\n\n\nRemark 1. If we predict everything to be positive, \\(TN=FB=0\\). Then, - Accuracy: \\(\\dfrac{TP}{N}\\). - Sensitivity: \\(1\\). - Specificity: \\(0\\). - False Positive Rate: \\(1\\). - Precision: \\(\\dfrac{TP}{N}\\).\n\n\n\n\n\nComposite Metrics: Trade-offs:\n\nBalanced Accuracy: mean of sensitivity and specificity. \\[\\dfrac{1}{2}\\qty(\\dfrac{TP}{TP+FN}+\\dfrac{TN}{TN+FP})\\]\nF1 Score: harmonic mean of precision and recall \\[2\\dfrac{\\cdot\\text{Precision}\\cdot\\text{Recall}}{\\text{Precision}+\\text{Recall}}\\]\n\nDiscrimination Thresholds: \n\nReceiver Operating Characteristic (ROC) Curve: TPR vs. FPR. \n\nEach point on the ROC curve corresponds to a threshold / a decision boundary.\nEach point represents a differenet trade-off between FPR and TPR.\nProperties of ROC:\n\nSlope is always upward.\nTwo non-intersecting curves means one model dominate the other.\nPerfect prediction vs. random prediction \nROC shows the trade-off between sensitivity and specificity.\nbut still not a very good summary metric: it is not a single number.\n\n\nAread under the ROC Curve (AUROC, ROC-AUC):\n\nCalculated using the trapezoid rule: sklearn.metrics.auc(x, y).\nIntuitive meaning: Given two randomly chosen examples, one positive and one negative, the probability of ranking positive example higher than the negative example.\n\\(AUC=1\\) for perfect prediction, \\(0.5\\) for random prediction.\n\\(AUC&gt;0.9\\): excellent prediction, but consider information leakage.\n\\(AUC\\approx0.8\\): good prediction.\n\\(AUC&lt;0.5\\): something is rong.\n\nPrecision-Recall Curve and AUPRC:\n\nA high AUPRC represents both higher recall and precision.\nROC curves should be used when there are rounghly equal numbers of observations for each class.\nPrecision-Recall curves may be used when there is a moderate to large class imbalance.\n\n\nClassifier Probability Calibration:\n\nWhen a model produce a probability of positive class, is that number actually a meaningful probability?\nFor example, if my model predicts 90% for a set of examples, does that mean 90% of those examples ahve label \\(+1\\)?\n\nMultiple classes metrics:\n\nAccuracy: \\[ACC=\\dfrac{TP1+TP2+TP3+\\cdots}{Total}\\]\nMacro-average precision: \\[PRE_\\text{macro}=\\dfrac{PRE_1+PRE_2+\\cdots+PRE_n}{n}\\]\nMicro-average precision (should not use): \\[PRE_\\text{micro}=\\dfrac{TP1+TP2+TP3+\\cdots}{TP1+TP2+TP3+\\cdots+FP1+FP2+FP3+\\cdots}\\]"
  },
  {
    "objectID": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html#classification-performance",
    "href": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html#classification-performance",
    "title": "6 Model Selection and Model Assessment",
    "section": "",
    "text": "Given \\(\\hat y=h(\\va x;\\va\\theta)\\) and \\(D=\\qty{\\va x^{(i)}, y^{(i)}}_{i=1}^N\\), \\(\\va x^{(i)}\\in\\R^d\\) and \\(y^{(i)}\\in\\qty{-1,+1}\\).\n\nMisclassification error: \\[\\dfrac{1}{N}\\sum_{i=1}^N\\1[\\hat y^{(i)}\\neq y^{(i)}].\\]\nAccuracy: \\[\\dfrac{1}{N}\\sum_{i=1}^N\\1[\\hat y^{(i)}=y^{(i)}].\\]\nWhat’s wrong with accuracy: It can be misleading when we have disproportional groups. E.g., rare disease.\n\nConfusion Matrix and Classification Metrics:\n\n\n\n\n\nPredicted (\\(-\\))\nPredicted (\\(+\\))\n\n\n\n\nActual (\\(-\\))\nTrue Negative (TN)\nFalse Positive (FP) Type I Error\n\n\nActual (\\(+\\))\nFalse Negative (FN) Type II Error\nTrue Positive (TP)\n\n\n\n\nAccuracy: \\(\\dfrac{TP+TN}{N}\\)\nSensitivity / True Positive Rate (TRR) / Recall: \\(\\dfrac{TP}{TP+FN}\\).\nSpecificity / True Negative Rate (TNR): \\(\\dfrac{TN}{TN+FP}\\).\nFalse Positive Rate (FPR): \\(\\dfrac{FP}{TN+FP}=1-\\text{specificity}\\).\nPrecision / Positive Predictive Value (PPV): \\(\\dfrac{TP}{TP+FP}\\).\n\n\n\n\n\n\n\n\nRemark 1. If we predict everything to be positive, \\(TN=FB=0\\). Then, - Accuracy: \\(\\dfrac{TP}{N}\\). - Sensitivity: \\(1\\). - Specificity: \\(0\\). - False Positive Rate: \\(1\\). - Precision: \\(\\dfrac{TP}{N}\\).\n\n\n\n\n\nComposite Metrics: Trade-offs:\n\nBalanced Accuracy: mean of sensitivity and specificity. \\[\\dfrac{1}{2}\\qty(\\dfrac{TP}{TP+FN}+\\dfrac{TN}{TN+FP})\\]\nF1 Score: harmonic mean of precision and recall \\[2\\dfrac{\\cdot\\text{Precision}\\cdot\\text{Recall}}{\\text{Precision}+\\text{Recall}}\\]\n\nDiscrimination Thresholds: \n\nReceiver Operating Characteristic (ROC) Curve: TPR vs. FPR. \n\nEach point on the ROC curve corresponds to a threshold / a decision boundary.\nEach point represents a differenet trade-off between FPR and TPR.\nProperties of ROC:\n\nSlope is always upward.\nTwo non-intersecting curves means one model dominate the other.\nPerfect prediction vs. random prediction \nROC shows the trade-off between sensitivity and specificity.\nbut still not a very good summary metric: it is not a single number.\n\n\nAread under the ROC Curve (AUROC, ROC-AUC):\n\nCalculated using the trapezoid rule: sklearn.metrics.auc(x, y).\nIntuitive meaning: Given two randomly chosen examples, one positive and one negative, the probability of ranking positive example higher than the negative example.\n\\(AUC=1\\) for perfect prediction, \\(0.5\\) for random prediction.\n\\(AUC&gt;0.9\\): excellent prediction, but consider information leakage.\n\\(AUC\\approx0.8\\): good prediction.\n\\(AUC&lt;0.5\\): something is rong.\n\nPrecision-Recall Curve and AUPRC:\n\nA high AUPRC represents both higher recall and precision.\nROC curves should be used when there are rounghly equal numbers of observations for each class.\nPrecision-Recall curves may be used when there is a moderate to large class imbalance.\n\n\nClassifier Probability Calibration:\n\nWhen a model produce a probability of positive class, is that number actually a meaningful probability?\nFor example, if my model predicts 90% for a set of examples, does that mean 90% of those examples ahve label \\(+1\\)?\n\nMultiple classes metrics:\n\nAccuracy: \\[ACC=\\dfrac{TP1+TP2+TP3+\\cdots}{Total}\\]\nMacro-average precision: \\[PRE_\\text{macro}=\\dfrac{PRE_1+PRE_2+\\cdots+PRE_n}{n}\\]\nMicro-average precision (should not use): \\[PRE_\\text{micro}=\\dfrac{TP1+TP2+TP3+\\cdots}{TP1+TP2+TP3+\\cdots+FP1+FP2+FP3+\\cdots}\\]"
  },
  {
    "objectID": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html#regression-metrics",
    "href": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html#regression-metrics",
    "title": "6 Model Selection and Model Assessment",
    "section": "Regression Metrics",
    "text": "Regression Metrics\nGiven \\(\\hat y=f(\\va x;\\va\\theta)\\) and \\(D=\\qty{\\va x^{(i)}, y^{(i)}}_{i=1}^N\\), \\(x^{(i)}\\in\\R^d\\) and \\(y^{(i)}\\in\\R\\).\n\nMean Squared Error (MSE): \\[MSE=\\dfrac{1}{N}\\sum_{i=1}^N\\qty(\\hat y^{(i)}-y^{(i)})^2\\]\nRoot Mean Squared Error (RMSE): \\[RMSE=\\sqrt{MSE}=\\sqrt{\\dfrac{1}{N}\\sum_{i=1}^N\\qty(\\hat y^{(i)}-y^{(i)})^2}\\]\nMean Absolute Error (MAE): \\[MAE=\\dfrac{1}{N}\\sum_{i=1}^N\\abs{\\hat y^{(i)}-y^{(i)}}\\]\nMean Bias Error (MBE): \\[MBE=\\dfrac{1}{N}\\sum_{i=1}^N\\qty(y^{(i)}-y^(i))\\]"
  },
  {
    "objectID": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html#model-assessment-process",
    "href": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html#model-assessment-process",
    "title": "6 Model Selection and Model Assessment",
    "section": "Model Assessment Process",
    "text": "Model Assessment Process\n\nHouldout: forming a test set:\n\nHold out some data (i.e., test data) that are not used for training the modedl.\nProxy for “everything you might see.”\nProcedure:\n\nOn training data, we train our model.\nOn test data, we use the model to make predictions.\nWe compare the predictions with the true labels to get the performance.\n\nProblem of holdout:\n\nToo few data for traiing: unable to properly learn from the data\nToo few for testing: bad approximation of the true error\nRule of thumb: enough test samples to form a reasonable estimate.\nCommon split size is \\(70\\%-30\\%\\) or \\(80\\%-20\\%\\).\n\n\nQuestion: what to do if we don’t have enough data? \\(K\\)-Fold Cross Validation (CV):\n\nUse all the data to train / test (but don’t use all data to train at the same time).\nProcesudre:\n\nSplit the data into \\(K\\) parts or “folds.”\nTrain on all but the \\(k\\)-th part and test / validate on the \\(k\\)-th part.\nRepeat for each \\(k=1,2,\\ldots,K\\).\nReport average performance over \\(K\\) experiments.\n\nCommon values of \\(K\\):\n\n\\(K=2\\): two-fold cross validation.\n\\(K=5\\) or \\(K=10\\): 5-fold or 10-fold cross validation – common choices.\n\\(K=N\\): leave-one-out cross validation (LOOCV).\n\nThe choice of \\(K\\) is based on how much data we have.\n\nMonte-Carlo Cross Validation:\n\nAKA repeated random sub-sampling validation.\nProcedure:\n\nRandomly select (without replacement) some fraction of the data to form training set.\nAssign rest to test set.\nRepeat multiple times with different partitions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nHow many possible partitions do we see?\nHow many models do we need to train?\nWhat is the final model?\nHow to get error estimates?\n\n\n\n\nHoldout\n\\(1\\)\n\\(1\\)\nThe model\nBootstrapping\n\n\n\\(K\\)-Fold CV\n\\(K\\)\n\\(K\\)\nAverage of \\(K\\) models\nAverage of \\(K\\) error estimates\n\n\nMonte-Carlo CV\nas many as possible\nas many as possible\nAverage of all models\nAverage of all error estimates\n\n\n\n\nBootstrapping to find Confidence Intervals:\n\nSample with replacement \\(N\\) times.\nCalculate performance metric on each boostrap iterate.\nThe 95% confidence interval is the \\(2.5\\)-th and \\(97.5\\)-th percentile of the boostrap distribution."
  },
  {
    "objectID": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html#model-selection",
    "href": "notes/cs334/06-Model-Assessment-and-Model-Selection/06-Model-Assessment-and-Model-Selection.html#model-selection",
    "title": "6 Model Selection and Model Assessment",
    "section": "Model Selection",
    "text": "Model Selection\nGoal: selecting the proper level of flexibility for a model (e.g., regularization strength in logistic regression)\n\nSelect hyperparameters of the model: “meta-optimization”\n\nRegularization strength\nRegularization type\nLoss function\nPolynomial degree\nKernal type\n\nDifferent from model parameters:\n\nFeature coefficients\n\nSimple, popular solution: \\(K\\)-fold CV for Hyperparameter Selection\nAssessment + Selection Guidelines:\n\nDo not use same samples to choose optimal hyperparameters and to estimate test / generalization error.\nChoice of methodology will depend on your problem and dataset.\n\nThree-way split:\n\nTraining set: to train the model\nValidation set: to select hyperparameters\nTest set: to estimate generalization error\n\nHoldout + \\(K\\)-Fold CV:\n\nHoldout: test dataset to assess the performance.\nTraining set: use \\(K\\)-fold CV to find optimal hyperparameters.\nUse optimal hyperparameters to train on the training data and assess on test.\n\nNested CV:\n\nOuter \\(K\\)-fold loop: assess the performance.\nInner \\(K\\)-fold loop: choose the optimal hyperparameters.\n\n\n\n\n\n\n\n\nTip 1: An Example in sklearn\n\n\n\n\n\nMost commonly used function/classes: train_test_split, KFold, and StratifiedKFold.\n# generate an 80-10-10 train-validation-test three-way split\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\nX_test, X_val, y_test, y_val = train_test_split(X_train, y_train, test_size=1/9)\n\n# generate a 4-fold CV in a for-loop\n# stratified k-foldL maintains label proportions\nfrom sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=5)\nfor train_idx, test_idx in skf.split(X, y):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n\n\n\n\nHyperparameter Search Space:\n\nGrid Search CV: exhaustive search for the best hyperparameter values using cross validation. sklearn.model_selection.GridSearchCV.\nRandomized Search CV: random search over hyperparameters. sklearn.model_selection.RandomizedSearchCV."
  },
  {
    "objectID": "notes/cs334/07-Feature-Selection-and-Kernels/07-Feature-Selection-and-Kernels.html",
    "href": "notes/cs334/07-Feature-Selection-and-Kernels/07-Feature-Selection-and-Kernels.html",
    "title": "7 Feature Selection and Kernels",
    "section": "",
    "text": "Types of Variables:\n\nNumerical: Discrete / Continuous.\nCategorical\nOrdinal: categocial with order. E.g., Temperature: Low, Medium, High.\n\nDiscretization of Numerical Features (Quantization / Bining)\n\nNumerical \\(\\to\\) Categorical\n\nE.g., age: \\([18,25], (25, 45], (45, 65], (65, 85], &gt;85\\) or binary: \\(\\leq 65, &gt;65\\)\n\nWhy? Allow a linear model to learn nonlinear relationships\n\nMissing data:\n\nDepend on how missing they are:\n\nDrop examples with missing values\nDrop features with missing values\n\nImputation approaches:\n\nUnivariate imputation\nMultivariate imputation\nNearest neighbor imputation\nMissing indicators\n\n\nTime Series Data:\n\nSummary statistics:\n\nmean, std\nmin, median, max, Q1, Q3\ncumulative sum\ncount\ncount above/below threshold\n\nTrend:\n\nlinear slopes\npiece-wise slopes\n\nPeriodic trends: fast fourier transform"
  },
  {
    "objectID": "notes/cs334/07-Feature-Selection-and-Kernels/07-Feature-Selection-and-Kernels.html#feature-engineering",
    "href": "notes/cs334/07-Feature-Selection-and-Kernels/07-Feature-Selection-and-Kernels.html#feature-engineering",
    "title": "7 Feature Selection and Kernels",
    "section": "",
    "text": "Types of Variables:\n\nNumerical: Discrete / Continuous.\nCategorical\nOrdinal: categocial with order. E.g., Temperature: Low, Medium, High.\n\nDiscretization of Numerical Features (Quantization / Bining)\n\nNumerical \\(\\to\\) Categorical\n\nE.g., age: \\([18,25], (25, 45], (45, 65], (65, 85], &gt;85\\) or binary: \\(\\leq 65, &gt;65\\)\n\nWhy? Allow a linear model to learn nonlinear relationships\n\nMissing data:\n\nDepend on how missing they are:\n\nDrop examples with missing values\nDrop features with missing values\n\nImputation approaches:\n\nUnivariate imputation\nMultivariate imputation\nNearest neighbor imputation\nMissing indicators\n\n\nTime Series Data:\n\nSummary statistics:\n\nmean, std\nmin, median, max, Q1, Q3\ncumulative sum\ncount\ncount above/below threshold\n\nTrend:\n\nlinear slopes\npiece-wise slopes\n\nPeriodic trends: fast fourier transform"
  },
  {
    "objectID": "notes/cs334/07-Feature-Selection-and-Kernels/07-Feature-Selection-and-Kernels.html#feature-selection",
    "href": "notes/cs334/07-Feature-Selection-and-Kernels/07-Feature-Selection-and-Kernels.html#feature-selection",
    "title": "7 Feature Selection and Kernels",
    "section": "Feature Selection",
    "text": "Feature Selection\n\nMotivation:\n\nWhen having few examples, but a learge number of features (\\(d\\gg N\\)), it becomes very easy to overfit the model.\nWe need to remove uninformative features.\n\nMethods:\n\nFilter method: before model fitting\nWrapper method: after model fitting\nEmbedded method: during model fitting\n\nFilter Method: preprocessing*\n\nRank individual features according to some statistical measure\nFilter out features that fall below a certain threshold\n\nE.g., sample variance, Person’s correlation coerfficient between feature and label, Mutal information, \\(\\chi^2\\)-statistic for categorical/ordinal features.\n\n\nWrapper Method: Search\n\nView feature selection as a model selection problem\nWhich feature to use: hyperparameter\n\nE.g,b Brute-force, exhausitve search: If we have \\(d\\) features, there are \\(2^d-1\\) possible combinations. Computatianlly infeasible.\nGreedy search: Forward selection, backward elimination.\n\nForward selection:\n\n\n\\begin{algorithm} \\caption{Forward Selection} \\begin{algorithmic} \\State initialize $F$ to the set of all features \\State initialize $S$ to the empty set $\\qquad$\\Comment{Start with $0$ features} \\For{$i=1:d-1$} \\For{ each feature $f\\in F$} \\State frain model using $(S, f)$ \\State evaluate model \\EndFor \\State Select best set $\\qty{S, f^*}$. $S=S+F^*$, and $F=F-f^*$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\nThis algorithm has complexity \\(\\sim\\bigO(d^2)\\).\n\nBackward elimination:\n\n\n\\begin{algorithm} \\caption{Forward elimination} \\begin{algorithmic} \\State initialize $F$ to the set of all features \\For{$i=1:d-1$} \\State $S=$ set of all subsets of $F$ of size $d-i$. $\\qquad$\\Comment{Start with $d$ features} \\For{ each subset $s\\in S$} \\State train model using $s$ \\State evaluate model \\EndFor \\State Select best subset $s^*$, and $F=s^*$ \\EndFor \\end{algorithmic} \\end{algorithm}\n\n\n\nThis algorithm has complexity \\(\\sim\\bigO(d^2)\\). However, forward selection is less computationally expensive because we train more smaller modedls.\n\n\nEmbedded Method: Regularization\n\nIncorporate feature selection as part of the model fitting process\nUse \\(L_1\\) (LASSO) regularization\n\n\n\n\n\n\n\n\n\n\n\n\nFilter\nWrapper\nEmbedded\n\n\n\n\nComputational Cost\nFast. Only run once scalable to high dimension\nSlow\nBetween filter and wrapper\n\n\nModel/Algorithm Specific\nGeneric, agnostic to models and algorithms\nSpecific, considers model performance\nOnly applies to specific model / algorithms\n\n\nOverfitting\nUnlikely\nHigher risk of overfitting\nRegualrization controls overfitting\n\n\n\n\nIn practice, we use combinations of these methods."
  },
  {
    "objectID": "notes/cs334/07-Feature-Selection-and-Kernels/07-Feature-Selection-and-Kernels.html#fitting-nonlinear-functions-kernel-methods",
    "href": "notes/cs334/07-Feature-Selection-and-Kernels/07-Feature-Selection-and-Kernels.html#fitting-nonlinear-functions-kernel-methods",
    "title": "7 Feature Selection and Kernels",
    "section": "Fitting Nonlinear Functions – Kernel Methods",
    "text": "Fitting Nonlinear Functions – Kernel Methods\n\nQuestion: How can we use linear models to learn nonlinear trends?\n\nMap to higher order dimensions: e.g., \\(\\mqty[1&x&x^2&\\cdots&x^m]\\)\n\nFor example, classification:\n\n\n\n\n\n\nFigure 1: Higher Dimensional Classification\n\n\n\n\nLet \\(\\va x=\\mqty[x_1&x_2]^\\top\\) and \\(\\phi(\\va x)=\\mqty[1&x_1&x_2&x_1x_2&x_1^2&x_2^2]^\\top\\).\nFind a linear classifier such that \\(\\va\\theta\\cdot\\phi(\\va x)=0\\) that perfectlly separates the two classes.\nEquation of a circle: \\((x-h)^2+(y-k)^2=r^2\\).\nIn this example, \\((x_1-2)+(x_2-2)^2=1\\). So, \\(7-4x_1-4x_2+x_1^2+x_2^2=0\\).\nTherfore, \\(\\phi(\\va x)=\\mqty[1&x_1&x_2&x_1x_2&x_1^2&x_2^2]^\\top\\) and \\(\\va\\theta=\\mqty[7&-4&-4&0&1&1]^\\top\\).\nBut, we don’t want to use this method. Why?\n\nBecause dimension will blow-up. Suppose \\(\\va x\\in\\R^d\\) and \\(\\phi(\\va x)\\in\\R^p\\). Then, \\[p=\\text{number of first and second order terms}=(d+1)+\\dfrac{d(d+1)}{2}\\sim\\bigO(d^2).\\]\nWe have \\(p\\gg d\\). It’s also likely that \\(p\\gg N\\).\nIn perceptron algorithm, our update complexity will then be \\(\\sim\\bigO(p)=\\bigO(d^2)\\), which is inefficient.\n\n\nKey Observation:\n\nIf we initialize \\(\\va\\theta^{(k)}=\\va 0\\), then \\(\\va\\theta^{(k)}\\) is always in the span of feature vectors and can be expressed to linear combination of feature vectors: \\[\\va\\theta^{(k)}=\\sum_{i=1}^N\\alpha_i\\va x^{(i)}\\quad\\text{for some }\\alpha_1,\\alpha_2,\\dots,\\alpha_N.\\]\nThen, we can rewrite the classifier to get \\[\\begin{aligned}h(\\va x;\\va\\theta)&=\\operatorname{sign}(\\va\\theta\\cdot\\va x)\\\\&=\\operatorname{sign}\\qty(\\qty(\\sum_{i=1}^N\\alpha_i\\va x^{(i)})\\cdot\\va x)\\\\&=\\operatorname{sign}\\qty(\\sum_{i=1}^N\\alpha_i\\va x^{(i)}\\cdot\\va x).\\end{aligned}\\]\nEven if we are in higher dimensional feature space: \\[h(\\phi(\\va x);\\va\\theta)=\\operatorname{sign}\\qty(\\sum_{i=1}^N\\alpha_i\\qty(\\phi(\\va x^{(i)})\\cdot\\phi(\\va x))).\\]\nClassifiers now is written in \\(\\alpha_i\\) (\\(N\\)-dimensional) space, not in \\(\\va x\\) (\\(d\\)-dimensional) space.\nThen, the update rule is \\[\\alpha_i^{(k+1)}=\\alpha_i^{(k)}+y^{(i)}.\\]\n\n\n\n\n\\begin{algorithm} \\caption{New Perceptron} \\begin{algorithmic} \\State $\\va\\alpha^{(0)}=\\va 0\\in\\R^N$, $k=0$ \\While{not converged} \\For{$i=1,\\dots,N$} \\If{$y^{(i)}\\qty(\\sum_{j=1}^N\\alpha_j\\phi(\\va x^{(i)})\\cdot\\phi(\\va x^{(j)}))\\leq 0$} \\State $\\alpha_{i}^{(k+1)}=\\alpha_i^{(k)}+y^{(i)}$ \\State $\\alpha_j^{(k+1)}=\\alpha_j^{(k)}$ when $j\\neq i$ \\State $k++$ \\EndIf \\EndFor \\EndWhile \\end{algorithmic} \\end{algorithm}\n\n\n\n\\(\\phi(\\va x^{(i)})\\cdot\\phi(\\va x^{(j)})\\) can be pre-computed. Complexity is \\(\\bigO(N^2p)\\)\nThe update complexity \\(\\bigO(1)\\).\nHow can we speed up dot product \\(\\phi(\\va x)\\cdot\\phi(\\va x')\\)? We can calculate the dot producrt \\(\\phi(\\va x)\\cdot\\phi(\\va x')\\) withtout calculating \\(\\phi(\\va x)\\) and \\(\\phi(\\va x')\\).\n\n\n\n\n\n\n\n\nExample 1 Suppose \\(\\phi(\\va x)=\\mqty[x_1^2&x_2^2&\\sqrt{2}x_1x_2]^\\top\\). Then, \\[\n\\begin{aligned}\n\\phi(\\va u)\\cdot\\phi(\\va v)&=\\mqty[u_1^2&u_2^2&\\sqrt{2}u_1u_2]^\\top\\cdot\\mqty[v_1^2&v_2^2&\\sqrt{2}v_1v_2]^\\top\\\\\n&=u_1^2v_1^2+u_2v_2^2+2u_1u_2v_1v_2\\\\\n&=(u_1v_1+u_2v_2)^2\\\\\n&=\\qty(\\va u\\cdot\\va v)^2.\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1 (Kernel Function) Kernel function is an implicit feature mapping that allows us to compute the dot product in the feature space without explicitly computing the feature vectors. \\[K:\\underbrace{\\R^d\\times\\R^d}_\\text{feature vectors}\\to\\R\\]\n\n\n\n\n\n\n\n\n\n\n\nTheorem 1 (Common Kernels:)  \n\nLinear Kernel: \\[K\\qty(\\va x^{(i)},\\va x^{(j)})=\\va x^{(i)}\\cdot\\va x^{(j)}.\\]\nPolynomial Kernel: \\[K\\qty(\\va x^{(i)},\\va x^{(j)})=\\qty(\\va x^{(i)}\\cdot\\va x^{(j)}+1)^d.\\]\nRadial Basis Function (RBF) Kernel: infinite-dimensional feature space \\[\\begin{aligned}K_\\text{RBF}(\\va u,\\va v)&=\\exp\\qty(-\\gamma\\norm{\\va u-\\va v}^2)\\\\&=C\\sum_{n=0}^\\infty\\dfrac{K_{\\operatorname{poly}(n)}(\\va u,\\va v)}{n!}\\end{aligned}\\]\n\n\n\n\n\n\n\n\\begin{algorithm} \\caption{Kernelized Perceptron} \\begin{algorithmic} \\State $\\alpha_j^{(0)}=0$, $k=0$ \\While{not converged} \\For{$i=1,\\dots,N$} \\If{$y^{(i)}\\qty(\\sum_{j=1}^N\\alpha_j^{(k)}K(\\va x^{(j)}, \\va x^{(i)}))\\leq 0$} \\State $\\alpha_{i}^{(k+1)}=\\alpha_i^{(k)}+y^{(i)}$ \\State $\\alpha_j^{(k+1)}=\\alpha_j^{(k)}$ when $j\\neq i$ \\State $k++$ \\EndIf \\EndFor \\EndWhile \\end{algorithmic} \\end{algorithm}"
  },
  {
    "objectID": "notes/cs334/08-Decision-Trees-and-Random-Forest/08-Decision-Trees-and-Random-Forest.html",
    "href": "notes/cs334/08-Decision-Trees-and-Random-Forest/08-Decision-Trees-and-Random-Forest.html",
    "title": "8 Decision Trees and Random Forest",
    "section": "",
    "text": "Interpretability: Given a linear classifier dinfed by \\(\\va\\theta=[\\theta_1,\\theta_2,\\dots,\\theta_d]\\). How can we interpret the meaning of each parameter/coefficient? \\[\\norm{\\theta_i}\\text{: how this feature contributes to the decision.}\\] To fit nonlinear model, we can use kernels. However, with kernels, the fitted parameter \\(\\va\\theta\\) becomes a blackbox, and we lose interpretability (especially when we use RBF kernels, \\(\\va\\theta\\in\\R^\\infty\\)).\nA different approach: Decision Tree \\[f:\\mathcal{X}\\to\\mathcal{Y}\\]\n\nBoth features (\\(\\mathcal{X}\\)) and labels (\\(\\mathcal{Y}\\)) can be a continuous, descrete, or binary value.\n\n\n\n\n\n\n\n\n\nExample 1 (Forming a Decision Tree) Suppose \\(\\va x\\in\\qty{0,1}^2\\)\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y\\)\n\n\n\n\n1\n1\n0\n\n\n1\n0\n1\n\n\n0\n1\n1\n\n\n0\n0\n0\n\n\n\n\n\n\n\n\n\nFigure 1: Decision Tree\n\n\n\n\n\n\n\n\nWe can use a boolean function of input feature to represent a decision tree.\n\n\n\n\n\n\n\n\nRemark 1. \n\nAs the number of nodes increases, the hypothesis space grows.\nTrivial solution: each examples has its own leaf node. If \\(\\va x\\in\\qty{0,1}^d\\), then we have \\(2^d\\) leaf nodes.\nProblem: overffing, unlikely to generalize to unseen examples.\n\n\n\n\n\n\nGoal: Find the smallest tree that performs well on training data.\n\nHowever, finding the optimal partition of the data is NP-complete (hard).\nInstead, we can use a greedy appraoch:\n\nStart with empty tree.\nFind best feature to split on.\nRecursively build branches into subtree.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2 (How to Find the Best Split) We want to predict whether ot not a flight will get delayed: \\[\\text{training data: }\\begin{cases}29\\text{ positive}\\\\35\\text{ negative}\\end{cases}\\implies[29^+, 35^-].\\] Suppose \\(\\va x=[x_1,x_2]\\) are two binary features:\n\n\n\n\n\n\nFigure 2: First Slipt\n\n\n\nSplitting by snow is better because it produces more certain labels. But, how do we measure uncertainty?\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1 (Shannon’s Entropy) Let \\(D_N\\) be the training data, \\(y\\in\\qty{-1,+1}\\) be the binary outcome/label.\n\n\\(\\P_\\oplus\\): fraction of positive examples\n\\(\\P_\\ominus\\): fraction of negative examples \\[\\text{Entropy of }D_N=-\\qty\\Big(\\P_\\oplus\\log_2\\P_\\oplus+\\P_\\ominus\\log_2\\P_\\ominus)\\]\nThe definition uses \\(\\log_2\\) because entropy is measured in bits.\nIt measures the expected number of bits needed to encode a randomly drawn value of \\(y\\).\n\n\n\n\n\n\n\nFigure 3: Plot of Entropy\n\n\n\n\nMore generally, for categorical outcome \\(y\\in\\qty{y_1,y_2,\\dots,y_k}\\), \\[\n\\begin{aligned}\nH(y)&=-\\qty\\Big(\\P(Y=y_1)\\log_2\\P(Y=y_1)+\\P(Y=y_2)\\log_2\\P(Y=y_2)+\\cdots+\\P(Y=y_k)\\log_2\\P(Y=y_k))\\\\\n&=-\\sum_{i=1}^k\\P(Y=y_i)\\log_2\\P(Y=y_i)\n\\end{aligned}\n\\]\nNote: entropy is usually positive as \\(\\log_2\\P(Y=y_i)\\) is negative.\n\n\n\n\n\n\nEntropy and Peakyness:\n\nImagine rolling a die and plotting the empirical distribution: \n\nOn the left: high entropy, more uncertain about the label, less peaky distribution\nOn the right: low entropy, less uncertain about the label, more peaky distribution\n\n\n\n\n\n\n\n\n\n\nDefinition 2 (Conditional Entropy) \\[H(Y\\mid X=x)=-\\qty(\\sum_{i=1}^k\\P(Y=y_i\\mid X=x)\\log_2\\P(Y=y_i\\mid X=x))\\] \\[H(Y\\mid X)=\\sum_{x\\in X}\\P(X=x)\\cdot H(Y\\mid X=x)\\]\n\n\\(H(Y\\mid X)\\) shows the average surprise of \\(Y\\) when \\(X\\) is known.\n\n\n\n\n\n\n\n\n\n\n\n\nRemark 2. \n\nWhen does \\(H(Y\\mid X)=0\\)? When \\(Y\\) is completely determined by \\(X\\).\nWhen does \\(H(Y\\mid X)=H(Y)\\)? When \\(Y\\independ X\\).\nWe can use conditional entropy to measure the quality of a split:\n\nIdea: if knowing \\(x_1\\) reduces uncertainty more than knowing \\(x_2\\), we should split by \\(x_1\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3 (Information Gain (IG) / Mutual Information) \\[\nIG(X;Y)=H(Y)-H(Y\\mid X),\n\\]\n\nwhere \\(H(Y)\\) is the entropy of parent node, and \\(H(Y\\mid X)\\) is the average entropy of the children note.\n\\(IG\\) measures the amount of information we learn about \\(Y\\) by knowing the value of \\(C\\) (and vice versa \\(\\implies\\) symmetric).\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3 (Back to the Example) We want to predict whether ot not a flight will get delayed: \\[\\text{training data: }\\begin{cases}29\\text{ positive}\\\\35\\text{ negative}\\end{cases}\\implies[29^+, 35^-].\\] Suppose \\(\\va x=[x_1,x_2]\\) are two binary features:\n\n\n\n\n\n\nFigure 4: First Slipt\n\n\n\n\nCalculate entropy of \\(y\\): \\[H(y)=-\\qty(\\dfrac{29}{64}\\log_2\\dfrac{29}{64}+\\dfrac{35}{64}\\log_2\\dfrac{35}{64})\\approx0.9937\\]\nCalcualte the conditional entropy for each feature: \\[\n\\begin{aligned}\nH(y\\mid x_1)&=\\dfrac{26}{64}\\qty(-\\dfrac{21}{26}\\log_2\\dfrac{21}{26}-\\dfrac{5}{26}\\log_2\\dfrac{5}{26})+\\dfrac{38}{64}\\qty(-\\dfrac{8}{38}\\log_2\\dfrac{8}{38}-\\dfrac{30}{38}\\log_2\\dfrac{30}{38})\\\\\n&=0.7278\\\\\\\\\nH(y\\mid x_2)&=\\dfrac{45}{64}\\qty(-\\dfrac{18}{45}\\log_2\\dfrac{18}{45}-\\dfrac{27}{45}\\log_2\\dfrac{27}{45})+\\dfrac{19}{64}\\qty(-\\dfrac{11}{19}\\log_2\\dfrac{11}{19}-\\dfrac{8}{19}\\log_2\\dfrac{8}{19})\\\\\n&=0.9742.\n\\end{aligned}\n\\]\nCalcualte the information gain: \\[\n\\begin{aligned}\nIG(x_1;y)&=0.9937-0.7278=0.2659\\\\\nIG(x_2;y)&=0.9937-0.9742=0.0195.\n\\end{aligned}\n\\] So, \\(IG(x_1;y)&gt;IG(x_2;y)\\), \\(x_1\\) is a better split.\n\n\n\n\n\n\nAnother Measure of Uncertainty: Gini Index and Gini Gain: \\[\n\\begin{aligned}\n\\text{Gini}(Y)&=\\sum_{k=1}^k\\P(Y=y_k)\\qty(1-\\P(Y=y_k))=1-\\sum_{k=1}^k\\P(Y=y_k)^2\\\\\n\\text{GiniGain}(X;Y)&=\\text{Gini}(Y)-\\sum_{x\\in X}\\P(X=x)\\text{Gini}(Y\\mid X=x).\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nRemark 3. In practice, using \\(IG\\) or \\(\\text{GiniGain}\\) may lead to different results, but it is unclear how different it can be.\n\n\n\n\n\n\nFigure 5: Gini Index and Entropy Plot\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\arg\\max_{j=1,\\dots,d}IG(x_j;y)&=\\arg\\max_j H(y)-H(y\\mid x_j)\\\\\n&=\\arg\\min_j H(y\\mid x_j).\n\\end{aligned}\n\\]\n\nWhen to stop growing a tree? (Stopping Criteria)\n\nWhen all record have the same label (assume no noise).\nIf all record have identical features (no further splits possible).\n\n\n\n\n\n\n\n\n\nRemark 4. We should not stop when all stributtes have \\(0\\ IG\\). See Example 1.\n\n\n\n\n\n\n\\begin{algorithm} \\caption{Building Decision Tree} \\begin{algorithmic} \\Procedure{BuildTree}{$DS$} \\IF{$y^{(i)}==y$ for all examples in $DS$} \\Return{$y$} \\ElseIf{$x^{(i)}==x$ for all examples in $DS$} \\Return{majority label} \\Else \\State $x_s=\\argmin_x H(y\\mid x)$ \\For{each value $v$ of $x_s$} \\State $DS_y=\\qty{\\text{examples in }DS\\text{ where }x_s=v}$ \\State BuildTree($DS_y$)$\\qquad$\\Comment{recursive function} \\EndFor \\EndIf \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\nHow do we avoid overfitting? We want simpler trees.\n\nSet a maximum depth\nMeasure performance on validation data. If growing tree results in worse performance, stop.\nPost-prune: grow entire/full tree and then greedily remove nodes that affect validation error the least."
  },
  {
    "objectID": "notes/cs334/08-Decision-Trees-and-Random-Forest/08-Decision-Trees-and-Random-Forest.html#decision-tree",
    "href": "notes/cs334/08-Decision-Trees-and-Random-Forest/08-Decision-Trees-and-Random-Forest.html#decision-tree",
    "title": "8 Decision Trees and Random Forest",
    "section": "",
    "text": "Interpretability: Given a linear classifier dinfed by \\(\\va\\theta=[\\theta_1,\\theta_2,\\dots,\\theta_d]\\). How can we interpret the meaning of each parameter/coefficient? \\[\\norm{\\theta_i}\\text{: how this feature contributes to the decision.}\\] To fit nonlinear model, we can use kernels. However, with kernels, the fitted parameter \\(\\va\\theta\\) becomes a blackbox, and we lose interpretability (especially when we use RBF kernels, \\(\\va\\theta\\in\\R^\\infty\\)).\nA different approach: Decision Tree \\[f:\\mathcal{X}\\to\\mathcal{Y}\\]\n\nBoth features (\\(\\mathcal{X}\\)) and labels (\\(\\mathcal{Y}\\)) can be a continuous, descrete, or binary value.\n\n\n\n\n\n\n\n\n\nExample 1 (Forming a Decision Tree) Suppose \\(\\va x\\in\\qty{0,1}^2\\)\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y\\)\n\n\n\n\n1\n1\n0\n\n\n1\n0\n1\n\n\n0\n1\n1\n\n\n0\n0\n0\n\n\n\n\n\n\n\n\n\nFigure 1: Decision Tree\n\n\n\n\n\n\n\n\nWe can use a boolean function of input feature to represent a decision tree.\n\n\n\n\n\n\n\n\nRemark 1. \n\nAs the number of nodes increases, the hypothesis space grows.\nTrivial solution: each examples has its own leaf node. If \\(\\va x\\in\\qty{0,1}^d\\), then we have \\(2^d\\) leaf nodes.\nProblem: overffing, unlikely to generalize to unseen examples.\n\n\n\n\n\n\nGoal: Find the smallest tree that performs well on training data.\n\nHowever, finding the optimal partition of the data is NP-complete (hard).\nInstead, we can use a greedy appraoch:\n\nStart with empty tree.\nFind best feature to split on.\nRecursively build branches into subtree.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2 (How to Find the Best Split) We want to predict whether ot not a flight will get delayed: \\[\\text{training data: }\\begin{cases}29\\text{ positive}\\\\35\\text{ negative}\\end{cases}\\implies[29^+, 35^-].\\] Suppose \\(\\va x=[x_1,x_2]\\) are two binary features:\n\n\n\n\n\n\nFigure 2: First Slipt\n\n\n\nSplitting by snow is better because it produces more certain labels. But, how do we measure uncertainty?\n\n\n\n\n\n\n\n\n\n\n\nDefinition 1 (Shannon’s Entropy) Let \\(D_N\\) be the training data, \\(y\\in\\qty{-1,+1}\\) be the binary outcome/label.\n\n\\(\\P_\\oplus\\): fraction of positive examples\n\\(\\P_\\ominus\\): fraction of negative examples \\[\\text{Entropy of }D_N=-\\qty\\Big(\\P_\\oplus\\log_2\\P_\\oplus+\\P_\\ominus\\log_2\\P_\\ominus)\\]\nThe definition uses \\(\\log_2\\) because entropy is measured in bits.\nIt measures the expected number of bits needed to encode a randomly drawn value of \\(y\\).\n\n\n\n\n\n\n\nFigure 3: Plot of Entropy\n\n\n\n\nMore generally, for categorical outcome \\(y\\in\\qty{y_1,y_2,\\dots,y_k}\\), \\[\n\\begin{aligned}\nH(y)&=-\\qty\\Big(\\P(Y=y_1)\\log_2\\P(Y=y_1)+\\P(Y=y_2)\\log_2\\P(Y=y_2)+\\cdots+\\P(Y=y_k)\\log_2\\P(Y=y_k))\\\\\n&=-\\sum_{i=1}^k\\P(Y=y_i)\\log_2\\P(Y=y_i)\n\\end{aligned}\n\\]\nNote: entropy is usually positive as \\(\\log_2\\P(Y=y_i)\\) is negative.\n\n\n\n\n\n\nEntropy and Peakyness:\n\nImagine rolling a die and plotting the empirical distribution: \n\nOn the left: high entropy, more uncertain about the label, less peaky distribution\nOn the right: low entropy, less uncertain about the label, more peaky distribution\n\n\n\n\n\n\n\n\n\n\nDefinition 2 (Conditional Entropy) \\[H(Y\\mid X=x)=-\\qty(\\sum_{i=1}^k\\P(Y=y_i\\mid X=x)\\log_2\\P(Y=y_i\\mid X=x))\\] \\[H(Y\\mid X)=\\sum_{x\\in X}\\P(X=x)\\cdot H(Y\\mid X=x)\\]\n\n\\(H(Y\\mid X)\\) shows the average surprise of \\(Y\\) when \\(X\\) is known.\n\n\n\n\n\n\n\n\n\n\n\n\nRemark 2. \n\nWhen does \\(H(Y\\mid X)=0\\)? When \\(Y\\) is completely determined by \\(X\\).\nWhen does \\(H(Y\\mid X)=H(Y)\\)? When \\(Y\\independ X\\).\nWe can use conditional entropy to measure the quality of a split:\n\nIdea: if knowing \\(x_1\\) reduces uncertainty more than knowing \\(x_2\\), we should split by \\(x_1\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3 (Information Gain (IG) / Mutual Information) \\[\nIG(X;Y)=H(Y)-H(Y\\mid X),\n\\]\n\nwhere \\(H(Y)\\) is the entropy of parent node, and \\(H(Y\\mid X)\\) is the average entropy of the children note.\n\\(IG\\) measures the amount of information we learn about \\(Y\\) by knowing the value of \\(C\\) (and vice versa \\(\\implies\\) symmetric).\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3 (Back to the Example) We want to predict whether ot not a flight will get delayed: \\[\\text{training data: }\\begin{cases}29\\text{ positive}\\\\35\\text{ negative}\\end{cases}\\implies[29^+, 35^-].\\] Suppose \\(\\va x=[x_1,x_2]\\) are two binary features:\n\n\n\n\n\n\nFigure 4: First Slipt\n\n\n\n\nCalculate entropy of \\(y\\): \\[H(y)=-\\qty(\\dfrac{29}{64}\\log_2\\dfrac{29}{64}+\\dfrac{35}{64}\\log_2\\dfrac{35}{64})\\approx0.9937\\]\nCalcualte the conditional entropy for each feature: \\[\n\\begin{aligned}\nH(y\\mid x_1)&=\\dfrac{26}{64}\\qty(-\\dfrac{21}{26}\\log_2\\dfrac{21}{26}-\\dfrac{5}{26}\\log_2\\dfrac{5}{26})+\\dfrac{38}{64}\\qty(-\\dfrac{8}{38}\\log_2\\dfrac{8}{38}-\\dfrac{30}{38}\\log_2\\dfrac{30}{38})\\\\\n&=0.7278\\\\\\\\\nH(y\\mid x_2)&=\\dfrac{45}{64}\\qty(-\\dfrac{18}{45}\\log_2\\dfrac{18}{45}-\\dfrac{27}{45}\\log_2\\dfrac{27}{45})+\\dfrac{19}{64}\\qty(-\\dfrac{11}{19}\\log_2\\dfrac{11}{19}-\\dfrac{8}{19}\\log_2\\dfrac{8}{19})\\\\\n&=0.9742.\n\\end{aligned}\n\\]\nCalcualte the information gain: \\[\n\\begin{aligned}\nIG(x_1;y)&=0.9937-0.7278=0.2659\\\\\nIG(x_2;y)&=0.9937-0.9742=0.0195.\n\\end{aligned}\n\\] So, \\(IG(x_1;y)&gt;IG(x_2;y)\\), \\(x_1\\) is a better split.\n\n\n\n\n\n\nAnother Measure of Uncertainty: Gini Index and Gini Gain: \\[\n\\begin{aligned}\n\\text{Gini}(Y)&=\\sum_{k=1}^k\\P(Y=y_k)\\qty(1-\\P(Y=y_k))=1-\\sum_{k=1}^k\\P(Y=y_k)^2\\\\\n\\text{GiniGain}(X;Y)&=\\text{Gini}(Y)-\\sum_{x\\in X}\\P(X=x)\\text{Gini}(Y\\mid X=x).\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\nRemark 3. In practice, using \\(IG\\) or \\(\\text{GiniGain}\\) may lead to different results, but it is unclear how different it can be.\n\n\n\n\n\n\nFigure 5: Gini Index and Entropy Plot\n\n\n\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\arg\\max_{j=1,\\dots,d}IG(x_j;y)&=\\arg\\max_j H(y)-H(y\\mid x_j)\\\\\n&=\\arg\\min_j H(y\\mid x_j).\n\\end{aligned}\n\\]\n\nWhen to stop growing a tree? (Stopping Criteria)\n\nWhen all record have the same label (assume no noise).\nIf all record have identical features (no further splits possible).\n\n\n\n\n\n\n\n\n\nRemark 4. We should not stop when all stributtes have \\(0\\ IG\\). See Example 1.\n\n\n\n\n\n\n\\begin{algorithm} \\caption{Building Decision Tree} \\begin{algorithmic} \\Procedure{BuildTree}{$DS$} \\IF{$y^{(i)}==y$ for all examples in $DS$} \\Return{$y$} \\ElseIf{$x^{(i)}==x$ for all examples in $DS$} \\Return{majority label} \\Else \\State $x_s=\\argmin_x H(y\\mid x)$ \\For{each value $v$ of $x_s$} \\State $DS_y=\\qty{\\text{examples in }DS\\text{ where }x_s=v}$ \\State BuildTree($DS_y$)$\\qquad$\\Comment{recursive function} \\EndFor \\EndIf \\EndProcedure \\end{algorithmic} \\end{algorithm}\n\n\n\nHow do we avoid overfitting? We want simpler trees.\n\nSet a maximum depth\nMeasure performance on validation data. If growing tree results in worse performance, stop.\nPost-prune: grow entire/full tree and then greedily remove nodes that affect validation error the least."
  },
  {
    "objectID": "notes/cs334/08-Decision-Trees-and-Random-Forest/08-Decision-Trees-and-Random-Forest.html#forming-a-decision-tree",
    "href": "notes/cs334/08-Decision-Trees-and-Random-Forest/08-Decision-Trees-and-Random-Forest.html#forming-a-decision-tree",
    "title": "8 Decision Trees and Random Forest",
    "section": "",
    "text": "Suppose \\(\\va x\\in\\qty{0,1}^2\\)\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y\\)\n\n\n\n\n1\n1\n0\n\n\n1\n0\n1\n\n\n0\n1\n1\n\n\n0\n0\n0\n\n\n\n\n\n\n\n\n\nFigure 1: Decision Tree"
  },
  {
    "objectID": "notes/cs334/08-Decision-Trees-and-Random-Forest/08-Decision-Trees-and-Random-Forest.html#ensemble-methods-and-random-forest",
    "href": "notes/cs334/08-Decision-Trees-and-Random-Forest/08-Decision-Trees-and-Random-Forest.html#ensemble-methods-and-random-forest",
    "title": "8 Decision Trees and Random Forest",
    "section": "Ensemble Methods and Random Forest",
    "text": "Ensemble Methods and Random Forest\n\nGoal: Descreae variance without increasing bias (recall bias-variance trade-off)\nIdea: Average across multiple models to reduce estimation error. But we only have one single training set, how can we learn multiple models? BAGGING\n\n\nBootstrap Aggregatting (BAGGING)\n\nGeneral procedure:\n\nCreate \\(B\\) bootstrap samples: \\(D_N^{(1)},\\dots,D_N^{(B)}\\)\nTrain decision tree on each \\(D_N^{(b)}\\)\nClassify new examples by majority vote (i.e., mode)\n\n\n\n\n\n\n\n\nFigure 6: BAGGING\n\n\n\n\nWhy does bagging work? (Assume \\(y\\in\\qty{-1,+1}\\))\n\nSuppose we have \\(B\\) independent classifers: \\[\\hat f^{(b)}:\\R^d\\to\\qty{-1,+1},\\] and each \\(\\hat f^{(b)}\\) has a misclassification rate of \\(0.4\\).\n\nThat is, if \\(y^{(i)}=+1\\), then \\(\\P\\qty(\\hat f^{(b)}(\\va x^{(i)})=-1)=0.4\\quad\\forall b=1,\\dots,B\\) and \\(\\forall\\ i,\\dots,d\\)\n\nNow, applay baaged classifier: \\[\\hat f^{(\\text{bag})}(\\va x)=\\arg\\max_{y\\in\\qty{-1,+1}}\\sum_{b=1}^B\\1\\qty(\\hat f^{(b)}(\\va x)=\\hat y)=\\arg\\max\\qty{B_{-1},B_{+1}},\\] where \\(B_{-1}\\) is the number of votes for \\(-1\\), and \\(B_{+1}\\) is the number of votes for \\(+1\\). Then, \\[B_{-1}\\sim\\text{Binomial}(B,0.4).\\] Recall: if \\(X\\sim\\text{Binomial}(n,p)\\), then the probability of getting exactkly \\(k\\) successes in \\(n\\) trails is given by \\[P(X=k)=\\binom{n}{k}p^k(1-p)^{n-k}.\\] Thus, misclassification rate of bagged classifer is \\[\\begin{aligned}\\P\\qty(\\hat f^{(\\text{bag})}(\\va x^{(i)})=-1)&=\\P\\qty(B_{-1}\\geq\\dfrac{B}{2})\\\\&=1-\\P\\qty(B_{-1}&lt;\\dfrac{B}{2})\\\\&=1-\\sum_{k=1}^{\\lfloor B/2\\rfloor}\\binom{B}{k}(0.4)^k(1-0.4)^{B-k}.\\end{aligned}\\] Note: \\[\\lim_{B\\to+\\infty}\\sum_{k=1}^{\\lfloor B/2\\rfloor}\\binom{B}{k}p^k(1-p)^{B-k}=1\\] as long as misclassification rate \\(p&lt;0.5\\). So, as \\(B\\to\\infty\\), misclassification \\(\\to0\\).\n\nReality: Predcition error rarely goes to \\(0\\).\n\nBagging only reduces estimation error (varaince).\nWe don’t ahve independence assumption: classifiers trained on bootstrapped dataset are NOT independent.\n\n\n\n\n\n\n\n\n\nRemark 5. \n\nOn average, each bootstrap contain \\(63.2\\%\\) of original data.\nHow similar are boostrap samples?\n\nProbability of example \\((i)\\) is not selected once: \\(1-\\dfrac{1}{n}\\).\nProbability of example \\((i)\\) is not selected at all: \\(\\qty(1-\\dfrac{1}{n})^n\\).\n\nThen, \\[\\lim_{n\\to\\infty}\\qty(1-\\dfrac{1}{n})^n=\\dfrac{1}{e}\\approx36.8\\%.\\] So, when \\(n\\to\\infty\\), the probability of example \\((i)\\) is not selected at all is \\(36.8\\%\\).\n\n\n\n\n\n\n\nFurther Decorrelate Trees: Random Forest\n\nRandom forest is an ensemble method designed specifically for trees (bagging applies more boardly).\nTwo sources of randomness:\n\nBagging\nRandom feature subsets:at erach node, best split chosen from subset of \\(m\\) features instead of all \\(d\\) features.\n\n\n\n\n\\begin{algorithm} \\caption{Random Forest} \\begin{algorithmic} \\State{\\# Bagging} \\For{$b=1,\\dots, B$} \\State draw bootstrap sample $D_N^{(b)}$ of size $N$ from $D_N$ \\State grow decision tree $DT^{(b)}$$\\qquad$\\Comment{See below code for growing $DT^{(b)}$} \\EndFor \\Return{ensemble $\\qty{DT^{(1)}, \\dots, DT^{(B)}}$} \\State{\\# Subprocedure for growing $DT^{(b)}$; random feature subset} \\While{stopping criteria not met} \\State Recursively repeat following steps for each node of tree: \\State 1. select $m$ features at random from $d$ features \\State 2. pick best feature to split on (using $IG$ or $\\text{Gini}$) \\State 3. split node into childre. \\EndWhile \\State{\\# Another option to do Steps 1 and 2: } \\State{1. compute $IG$ for all $d$ features} \\State{2. randomly pick from top $m$} \\end{algorithmic} \\end{algorithm}"
  }
]